{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew/Desktop/Project/model_explore/.env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data.dataset import random_split\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/markdown"
   },
   "source": [
    "# Helper Clases / Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_Data(num):\n",
    "    if (num == -1): # All data\n",
    "        X_all = []\n",
    "        y_all = []\n",
    "        for i in range(8):\n",
    "            file_path = './../project_datasets/A0' + str(i+1) + 'T_slice.mat'\n",
    "            data = h5py.File(file_path, 'r')\n",
    "            X = np.copy(data['image'])\n",
    "            y = np.copy(data['type'])\n",
    "            X = X[:, 0:23, :]\n",
    "            X_all.append(X)\n",
    "            y = y[0,0:X.shape[0]:1]\n",
    "            y_all.append(y)\n",
    "        A, N, E, T = np.shape(X_all)\n",
    "        X_all = np.reshape(X_all, (A*N, E, T))\n",
    "        y_all = np.reshape(y_all, (-1))\n",
    "        y_all = y_all - 769\n",
    "        ## Remove NAN\n",
    "        index_Nan = []\n",
    "        for i in range(A*N):\n",
    "            for j in range(E):\n",
    "                if (any(np.isnan(X_all[i,j])) == True):\n",
    "                    index_Nan.append(i)\n",
    "        index_Nan = list(set(index_Nan))\n",
    "        X_all = np.delete(X_all, index_Nan, axis=0)\n",
    "        y_all = np.delete(y_all, index_Nan)\n",
    "        return (X_all, y_all)\n",
    "    else:\n",
    "        file_path = './../project_datasets/A0' + str(num) + 'T_slice.mat'\n",
    "        data = h5py.File(file_path, 'r')\n",
    "        X = np.copy(data['image'])\n",
    "        y = np.copy(data['type'])\n",
    "        X = X[:, 0:23, :]\n",
    "        y = y[0,0:X.shape[0]:1]\n",
    "        y = y - 769\n",
    "         ## Remove NAN\n",
    "        N, E, T = np.shape(X)\n",
    "        index_Nan = []\n",
    "        for i in range(N):\n",
    "            for j in range(E):\n",
    "                if (any(np.isnan(X[i,j])) == True):\n",
    "                    index_Nan.append(i)\n",
    "        index_Nan = list(set(index_Nan))\n",
    "        X = np.delete(X, index_Nan, axis=0)\n",
    "        y = np.delete(y, index_Nan)\n",
    "        return (X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2280, 23, 1000)\n"
     ]
    }
   ],
   "source": [
    "X, y = Load_Data(-1) # -1 to load all datas\n",
    "N, E, T = np.shape(X)\n",
    "print (np.shape(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_train = 200\n",
    "bs_val = 100\n",
    "bs_test = 100\n",
    "data = data_utils.TensorDataset(torch.Tensor(X), torch.Tensor(y))\n",
    "dset = {}\n",
    "dataloaders = {}\n",
    "dset['train'], dset['val'], dset['test'] = random_split(data, [N-bs_val-bs_test, bs_val, bs_test])\n",
    "dataloaders['train'] = data_utils.DataLoader(dset['train'], batch_size=bs_train, shuffle=True, num_workers=1)\n",
    "dataloaders['val'] = data_utils.DataLoader(dset['val'], batch_size=bs_val, shuffle=True, num_workers=1)\n",
    "dataloaders['test'] = data_utils.DataLoader(dset['test'], batch_size=bs_test, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myGRU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layer, num_class):\n",
    "        super(myGRU, self).__init__()\n",
    "        self.num_layer = num_layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # First Inception layer\n",
    "        self.conv11 = nn.Conv1d(23, 32, 2, stride=2)\n",
    "        self.conv12 = nn.Conv1d(23, 32, 4, stride=2, padding=1)\n",
    "        self.conv13 = nn.Conv1d(23, 32, 8, stride=2, padding=3)\n",
    "        # Second Inception layer\n",
    "        self.conv21 = nn.Conv1d(96, 32, 2, stride=2)\n",
    "        self.conv22 = nn.Conv1d(96, 32, 4, stride=2, padding=1)\n",
    "        self.conv23 = nn.Conv1d(96, 32, 8, stride=2, padding=3)\n",
    "        # Third Inception layer\n",
    "        self.conv31 = nn.Conv1d(96, 32, 2, stride=2)\n",
    "        self.conv32 = nn.Conv1d(96, 32, 4, stride=2, padding=1)\n",
    "        self.conv33 = nn.Conv1d(96, 32, 8, stride=2, padding=3)\n",
    "        #self.conv_13 = nn.Conv2d()\n",
    "        self.conv_elec = nn.Conv3d(1,23,tuple([40, 23, 1]))\n",
    "        self.gru1 = nn.GRU(32*3, hidden_dim, num_layer)\n",
    "        self.gru2 = nn.GRU(hidden_dim, hidden_dim, num_layer)\n",
    "        self.gru3 = nn.GRU(hidden_dim*2, hidden_dim, num_layer)\n",
    "        self.gru4 = nn.GRU(hidden_dim*3, hidden_dim, num_layer)\n",
    "        self.classifier = nn.Linear(hidden_dim, num_class)\n",
    "    def forward(self, x):\n",
    "        out_conv11 = self.conv11(x)\n",
    "        out_conv12 = self.conv12(x)\n",
    "        out_conv13 = self.conv13(x)\n",
    "        out_conv1 = torch.cat((out_conv11, out_conv12, out_conv13), 1)\n",
    "        out_conv21 = self.conv21(out_conv1)\n",
    "        out_conv22 = self.conv22(out_conv1)\n",
    "        out_conv23 = self.conv23(out_conv1)\n",
    "        out_conv2 = torch.cat((out_conv21, out_conv22, out_conv23), 1)\n",
    "        out_conv31 = self.conv31(out_conv2)\n",
    "        out_conv32 = self.conv32(out_conv2)\n",
    "        out_conv33 = self.conv33(out_conv2)\n",
    "        out_conv3 = torch.cat((out_conv31, out_conv32, out_conv33), 1)\n",
    "        # N, C, L --> L, N, C\n",
    "        out_conv3 = out_conv3.permute(2,0,1)\n",
    "        out_gru1, _ = self.gru1(out_conv3)\n",
    "        out_gru2, _ = self.gru2(out_gru1)\n",
    "        out_gru12 = torch.cat((out_gru1, out_gru2), 2)\n",
    "        out_gru3, _ = self.gru3(out_gru12)\n",
    "        out_gru321 = torch.cat((out_gru1, out_gru2, out_gru3), 2)\n",
    "        out_gru4, _ = self.gru4(out_gru321)\n",
    "        out_gru4 = out_gru4[-1, :, :] # taking the last time seq\n",
    "        out = self.classifier(out_gru4)\n",
    "        return out\n",
    "    def check_accuracy(self, dataloader):\n",
    "        total_correct = 0\n",
    "        total_label = 0\n",
    "        for i_batch, sample_batched in enumerate(dataloader):\n",
    "            X_sample, y_sample = sample_batched\n",
    "            X_sample, y_sample = Variable(X_sample), Variable(y_sample)\n",
    "            out = self.forward(X_sample.cuda())\n",
    "            _, pred = torch.max(out, 1)\n",
    "            num_correct = np.sum(pred.data.cpu().numpy() == y_sample.data.cpu().numpy())\n",
    "            total_correct += num_correct\n",
    "            total_label += len(pred)\n",
    "        return  total_correct / total_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.cuda.FloatTensor\n",
    "hidden_dim = 20\n",
    "num_classes = 4\n",
    "num_epoches = 100\n",
    "model = myGRU(E, hidden_dim, 1, num_classes)\n",
    "model.type(dtype)\n",
    "loss_fn = nn.CrossEntropyLoss().type(dtype)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0 batch) loss: 1.396448\n",
      "(1 batch) loss: 1.410255\n",
      "(2 batch) loss: 1.389469\n",
      "(3 batch) loss: 1.395272\n",
      "(4 batch) loss: 1.403893\n",
      "(5 batch) loss: 1.384132\n",
      "(6 batch) loss: 1.409157\n",
      "(7 batch) loss: 1.403443\n",
      "(8 batch) loss: 1.390879\n",
      "(9 batch) loss: 1.381419\n",
      "(10 batch) loss: 1.390914\n",
      "(Epoch 1 / 100) train_acc: 0.298077; val_acc: 0.180000\n",
      "(0 batch) loss: 1.373098\n",
      "(1 batch) loss: 1.354397\n",
      "(2 batch) loss: 1.365756\n",
      "(3 batch) loss: 1.359802\n",
      "(4 batch) loss: 1.368188\n",
      "(5 batch) loss: 1.364464\n",
      "(6 batch) loss: 1.370817\n",
      "(7 batch) loss: 1.364024\n",
      "(8 batch) loss: 1.370324\n",
      "(9 batch) loss: 1.368630\n",
      "(10 batch) loss: 1.398853\n",
      "(Epoch 2 / 100) train_acc: 0.340865; val_acc: 0.330000\n",
      "(0 batch) loss: 1.354602\n",
      "(1 batch) loss: 1.353235\n",
      "(2 batch) loss: 1.379616\n",
      "(3 batch) loss: 1.342688\n",
      "(4 batch) loss: 1.351555\n",
      "(5 batch) loss: 1.355071\n",
      "(6 batch) loss: 1.343851\n",
      "(7 batch) loss: 1.334753\n",
      "(8 batch) loss: 1.353116\n",
      "(9 batch) loss: 1.362871\n",
      "(10 batch) loss: 1.345742\n",
      "(Epoch 3 / 100) train_acc: 0.394712; val_acc: 0.240000\n",
      "(0 batch) loss: 1.327274\n",
      "(1 batch) loss: 1.333207\n",
      "(2 batch) loss: 1.337301\n",
      "(3 batch) loss: 1.352870\n",
      "(4 batch) loss: 1.344920\n",
      "(5 batch) loss: 1.337597\n",
      "(6 batch) loss: 1.349268\n",
      "(7 batch) loss: 1.330413\n",
      "(8 batch) loss: 1.339320\n",
      "(9 batch) loss: 1.331024\n",
      "(10 batch) loss: 1.340648\n",
      "(Epoch 4 / 100) train_acc: 0.405769; val_acc: 0.310000\n",
      "(0 batch) loss: 1.306357\n",
      "(1 batch) loss: 1.308960\n",
      "(2 batch) loss: 1.305720\n",
      "(3 batch) loss: 1.319505\n",
      "(4 batch) loss: 1.319949\n",
      "(5 batch) loss: 1.318804\n",
      "(6 batch) loss: 1.331418\n",
      "(7 batch) loss: 1.328612\n",
      "(8 batch) loss: 1.340666\n",
      "(9 batch) loss: 1.349727\n",
      "(10 batch) loss: 1.305880\n",
      "(Epoch 5 / 100) train_acc: 0.434615; val_acc: 0.240000\n",
      "(0 batch) loss: 1.301998\n",
      "(1 batch) loss: 1.291336\n",
      "(2 batch) loss: 1.290140\n",
      "(3 batch) loss: 1.291916\n",
      "(4 batch) loss: 1.285115\n",
      "(5 batch) loss: 1.308185\n",
      "(6 batch) loss: 1.323089\n",
      "(7 batch) loss: 1.316725\n",
      "(8 batch) loss: 1.314738\n",
      "(9 batch) loss: 1.288768\n",
      "(10 batch) loss: 1.323013\n",
      "(Epoch 6 / 100) train_acc: 0.441827; val_acc: 0.210000\n",
      "(0 batch) loss: 1.255019\n",
      "(1 batch) loss: 1.253616\n",
      "(2 batch) loss: 1.306278\n",
      "(3 batch) loss: 1.275649\n",
      "(4 batch) loss: 1.286439\n",
      "(5 batch) loss: 1.319554\n",
      "(6 batch) loss: 1.314562\n",
      "(7 batch) loss: 1.311296\n",
      "(8 batch) loss: 1.275693\n",
      "(9 batch) loss: 1.283621\n",
      "(10 batch) loss: 1.239577\n",
      "(Epoch 7 / 100) train_acc: 0.465385; val_acc: 0.230000\n",
      "(0 batch) loss: 1.260008\n",
      "(1 batch) loss: 1.262756\n",
      "(2 batch) loss: 1.256459\n",
      "(3 batch) loss: 1.277320\n",
      "(4 batch) loss: 1.269623\n",
      "(5 batch) loss: 1.231869\n",
      "(6 batch) loss: 1.298053\n",
      "(7 batch) loss: 1.285376\n",
      "(8 batch) loss: 1.267331\n",
      "(9 batch) loss: 1.313890\n",
      "(10 batch) loss: 1.273734\n",
      "(Epoch 8 / 100) train_acc: 0.448558; val_acc: 0.280000\n",
      "(0 batch) loss: 1.218171\n",
      "(1 batch) loss: 1.236598\n",
      "(2 batch) loss: 1.210316\n",
      "(3 batch) loss: 1.245414\n",
      "(4 batch) loss: 1.253681\n",
      "(5 batch) loss: 1.235703\n",
      "(6 batch) loss: 1.260924\n",
      "(7 batch) loss: 1.230377\n",
      "(8 batch) loss: 1.243254\n",
      "(9 batch) loss: 1.291958\n",
      "(10 batch) loss: 1.339639\n",
      "(Epoch 9 / 100) train_acc: 0.446635; val_acc: 0.220000\n",
      "(0 batch) loss: 1.252680\n",
      "(1 batch) loss: 1.232385\n",
      "(2 batch) loss: 1.241768\n",
      "(3 batch) loss: 1.235015\n",
      "(4 batch) loss: 1.251155\n",
      "(5 batch) loss: 1.282790\n",
      "(6 batch) loss: 1.249745\n",
      "(7 batch) loss: 1.241141\n",
      "(8 batch) loss: 1.214285\n",
      "(9 batch) loss: 1.250160\n",
      "(10 batch) loss: 1.264938\n",
      "(Epoch 10 / 100) train_acc: 0.496635; val_acc: 0.230000\n",
      "(0 batch) loss: 1.177643\n",
      "(1 batch) loss: 1.210716\n",
      "(2 batch) loss: 1.214280\n",
      "(3 batch) loss: 1.162671\n",
      "(4 batch) loss: 1.244053\n",
      "(5 batch) loss: 1.230702\n",
      "(6 batch) loss: 1.211326\n",
      "(7 batch) loss: 1.263175\n",
      "(8 batch) loss: 1.256511\n",
      "(9 batch) loss: 1.252414\n",
      "(10 batch) loss: 1.322259\n",
      "(Epoch 11 / 100) train_acc: 0.488462; val_acc: 0.210000\n",
      "(0 batch) loss: 1.206026\n",
      "(1 batch) loss: 1.199733\n",
      "(2 batch) loss: 1.163953\n",
      "(3 batch) loss: 1.189570\n",
      "(4 batch) loss: 1.170928\n",
      "(5 batch) loss: 1.189911\n",
      "(6 batch) loss: 1.201663\n",
      "(7 batch) loss: 1.217151\n",
      "(8 batch) loss: 1.247280\n",
      "(9 batch) loss: 1.205905\n",
      "(10 batch) loss: 1.234836\n",
      "(Epoch 12 / 100) train_acc: 0.500000; val_acc: 0.240000\n",
      "(0 batch) loss: 1.153115\n",
      "(1 batch) loss: 1.181583\n",
      "(2 batch) loss: 1.121315\n",
      "(3 batch) loss: 1.182971\n",
      "(4 batch) loss: 1.178625\n",
      "(5 batch) loss: 1.219298\n",
      "(6 batch) loss: 1.227043\n",
      "(7 batch) loss: 1.215976\n",
      "(8 batch) loss: 1.198972\n",
      "(9 batch) loss: 1.211287\n",
      "(10 batch) loss: 1.274766\n",
      "(Epoch 13 / 100) train_acc: 0.525481; val_acc: 0.290000\n",
      "(0 batch) loss: 1.137745\n",
      "(1 batch) loss: 1.109345\n",
      "(2 batch) loss: 1.100155\n",
      "(3 batch) loss: 1.142531\n",
      "(4 batch) loss: 1.142192\n",
      "(5 batch) loss: 1.233958\n",
      "(6 batch) loss: 1.202858\n",
      "(7 batch) loss: 1.186752\n",
      "(8 batch) loss: 1.151889\n",
      "(9 batch) loss: 1.180264\n",
      "(10 batch) loss: 1.197273\n",
      "(Epoch 14 / 100) train_acc: 0.525962; val_acc: 0.260000\n",
      "(0 batch) loss: 1.231314\n",
      "(1 batch) loss: 1.137979\n",
      "(2 batch) loss: 1.134089\n",
      "(3 batch) loss: 1.160994\n",
      "(4 batch) loss: 1.098294\n",
      "(5 batch) loss: 1.149512\n",
      "(6 batch) loss: 1.211129\n",
      "(7 batch) loss: 1.198423\n",
      "(8 batch) loss: 1.209055\n",
      "(9 batch) loss: 1.216003\n",
      "(10 batch) loss: 1.184102\n",
      "(Epoch 15 / 100) train_acc: 0.514423; val_acc: 0.270000\n",
      "(0 batch) loss: 1.184790\n",
      "(1 batch) loss: 1.167189\n",
      "(2 batch) loss: 1.171298\n",
      "(3 batch) loss: 1.241052\n",
      "(4 batch) loss: 1.180395\n",
      "(5 batch) loss: 1.161570\n",
      "(6 batch) loss: 1.124620\n",
      "(7 batch) loss: 1.140101\n",
      "(8 batch) loss: 1.162796\n",
      "(9 batch) loss: 1.160955\n",
      "(10 batch) loss: 1.230518\n",
      "(Epoch 16 / 100) train_acc: 0.507692; val_acc: 0.260000\n",
      "(0 batch) loss: 1.130311\n",
      "(1 batch) loss: 1.152928\n",
      "(2 batch) loss: 1.108741\n",
      "(3 batch) loss: 1.117957\n",
      "(4 batch) loss: 1.120351\n",
      "(5 batch) loss: 1.240503\n",
      "(6 batch) loss: 1.161526\n",
      "(7 batch) loss: 1.150780\n",
      "(8 batch) loss: 1.169843\n",
      "(9 batch) loss: 1.236236\n",
      "(10 batch) loss: 1.102412\n",
      "(Epoch 17 / 100) train_acc: 0.510096; val_acc: 0.250000\n",
      "(0 batch) loss: 1.206831\n",
      "(1 batch) loss: 1.130456\n",
      "(2 batch) loss: 1.161207\n",
      "(3 batch) loss: 1.130716\n",
      "(4 batch) loss: 1.204213\n",
      "(5 batch) loss: 1.170512\n",
      "(6 batch) loss: 1.137729\n",
      "(7 batch) loss: 1.143354\n",
      "(8 batch) loss: 1.212626\n",
      "(9 batch) loss: 1.190817\n",
      "(10 batch) loss: 1.123657\n",
      "(Epoch 18 / 100) train_acc: 0.537981; val_acc: 0.300000\n",
      "(0 batch) loss: 1.073878\n",
      "(1 batch) loss: 1.020734\n",
      "(2 batch) loss: 1.205268\n",
      "(3 batch) loss: 1.210398\n",
      "(4 batch) loss: 1.224699\n",
      "(5 batch) loss: 1.058014\n",
      "(6 batch) loss: 1.073104\n",
      "(7 batch) loss: 1.112977\n",
      "(8 batch) loss: 1.169283\n",
      "(9 batch) loss: 1.114848\n",
      "(10 batch) loss: 1.181977\n",
      "(Epoch 19 / 100) train_acc: 0.545192; val_acc: 0.250000\n",
      "(0 batch) loss: 1.099351\n",
      "(1 batch) loss: 1.062300\n",
      "(2 batch) loss: 1.109026\n",
      "(3 batch) loss: 1.134587\n",
      "(4 batch) loss: 1.169957\n",
      "(5 batch) loss: 1.097745\n",
      "(6 batch) loss: 1.131502\n",
      "(7 batch) loss: 1.137819\n",
      "(8 batch) loss: 1.221285\n",
      "(9 batch) loss: 1.128551\n",
      "(10 batch) loss: 1.057243\n",
      "(Epoch 20 / 100) train_acc: 0.545673; val_acc: 0.240000\n",
      "(0 batch) loss: 1.077361\n",
      "(1 batch) loss: 1.091052\n",
      "(2 batch) loss: 1.096962\n",
      "(3 batch) loss: 1.042360\n",
      "(4 batch) loss: 1.044516\n",
      "(5 batch) loss: 1.140992\n",
      "(6 batch) loss: 1.114026\n",
      "(7 batch) loss: 1.169819\n",
      "(8 batch) loss: 1.106600\n",
      "(9 batch) loss: 1.170757\n",
      "(10 batch) loss: 1.025831\n",
      "(Epoch 21 / 100) train_acc: 0.543269; val_acc: 0.250000\n",
      "(0 batch) loss: 1.084630\n",
      "(1 batch) loss: 1.130092\n",
      "(2 batch) loss: 1.147944\n",
      "(3 batch) loss: 1.156214\n",
      "(4 batch) loss: 1.045011\n",
      "(5 batch) loss: 1.010085\n",
      "(6 batch) loss: 1.018702\n",
      "(7 batch) loss: 1.090287\n",
      "(8 batch) loss: 1.027661\n",
      "(9 batch) loss: 1.107306\n",
      "(10 batch) loss: 0.983234\n",
      "(Epoch 22 / 100) train_acc: 0.578365; val_acc: 0.330000\n",
      "(0 batch) loss: 0.960717\n",
      "(1 batch) loss: 1.081001\n",
      "(2 batch) loss: 1.032440\n",
      "(3 batch) loss: 1.028632\n",
      "(4 batch) loss: 1.107806\n",
      "(5 batch) loss: 1.031031\n",
      "(6 batch) loss: 1.096432\n",
      "(7 batch) loss: 1.143878\n",
      "(8 batch) loss: 1.064433\n",
      "(9 batch) loss: 1.110514\n",
      "(10 batch) loss: 1.140128\n",
      "(Epoch 23 / 100) train_acc: 0.589904; val_acc: 0.310000\n",
      "(0 batch) loss: 1.011567\n",
      "(1 batch) loss: 1.061374\n",
      "(2 batch) loss: 1.029666\n",
      "(3 batch) loss: 1.020782\n",
      "(4 batch) loss: 1.063422\n",
      "(5 batch) loss: 0.998229\n",
      "(6 batch) loss: 1.050715\n",
      "(7 batch) loss: 1.084912\n",
      "(8 batch) loss: 1.114493\n",
      "(9 batch) loss: 1.089879\n",
      "(10 batch) loss: 1.103704\n",
      "(Epoch 24 / 100) train_acc: 0.580769; val_acc: 0.330000\n",
      "(0 batch) loss: 0.972467\n",
      "(1 batch) loss: 1.083334\n",
      "(2 batch) loss: 1.008047\n",
      "(3 batch) loss: 1.047431\n",
      "(4 batch) loss: 1.030643\n",
      "(5 batch) loss: 0.959219\n",
      "(6 batch) loss: 1.028690\n",
      "(7 batch) loss: 1.120932\n",
      "(8 batch) loss: 1.129007\n",
      "(9 batch) loss: 1.092744\n",
      "(10 batch) loss: 1.051099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 25 / 100) train_acc: 0.551923; val_acc: 0.230000\n",
      "(0 batch) loss: 1.027461\n",
      "(1 batch) loss: 1.049334\n",
      "(2 batch) loss: 1.036801\n",
      "(3 batch) loss: 1.063697\n",
      "(4 batch) loss: 1.049629\n",
      "(5 batch) loss: 0.948785\n",
      "(6 batch) loss: 1.052818\n",
      "(7 batch) loss: 1.067448\n",
      "(8 batch) loss: 1.126886\n",
      "(9 batch) loss: 1.089031\n",
      "(10 batch) loss: 1.066816\n",
      "(Epoch 26 / 100) train_acc: 0.575962; val_acc: 0.280000\n",
      "(0 batch) loss: 0.995159\n",
      "(1 batch) loss: 1.044919\n",
      "(2 batch) loss: 1.149133\n",
      "(3 batch) loss: 1.082493\n",
      "(4 batch) loss: 1.076463\n",
      "(5 batch) loss: 1.011518\n",
      "(6 batch) loss: 1.105398\n",
      "(7 batch) loss: 1.022923\n",
      "(8 batch) loss: 1.005407\n",
      "(9 batch) loss: 1.038026\n",
      "(10 batch) loss: 1.013032\n",
      "(Epoch 27 / 100) train_acc: 0.593269; val_acc: 0.260000\n",
      "(0 batch) loss: 0.988773\n",
      "(1 batch) loss: 0.932209\n",
      "(2 batch) loss: 1.047366\n",
      "(3 batch) loss: 1.097889\n",
      "(4 batch) loss: 1.042784\n",
      "(5 batch) loss: 1.016025\n",
      "(6 batch) loss: 1.146329\n",
      "(7 batch) loss: 1.013946\n",
      "(8 batch) loss: 1.095721\n",
      "(9 batch) loss: 1.126082\n",
      "(10 batch) loss: 0.918469\n",
      "(Epoch 28 / 100) train_acc: 0.587019; val_acc: 0.310000\n",
      "(0 batch) loss: 1.005673\n",
      "(1 batch) loss: 0.998332\n",
      "(2 batch) loss: 1.016483\n",
      "(3 batch) loss: 1.089695\n",
      "(4 batch) loss: 0.975811\n",
      "(5 batch) loss: 1.014231\n",
      "(6 batch) loss: 1.066062\n",
      "(7 batch) loss: 1.126228\n",
      "(8 batch) loss: 1.146785\n",
      "(9 batch) loss: 1.075952\n",
      "(10 batch) loss: 1.074852\n",
      "(Epoch 29 / 100) train_acc: 0.591827; val_acc: 0.260000\n",
      "(0 batch) loss: 0.948201\n",
      "(1 batch) loss: 0.975513\n",
      "(2 batch) loss: 0.961758\n",
      "(3 batch) loss: 1.030972\n",
      "(4 batch) loss: 1.012839\n",
      "(5 batch) loss: 1.042400\n",
      "(6 batch) loss: 1.012795\n",
      "(7 batch) loss: 1.068937\n",
      "(8 batch) loss: 1.045987\n",
      "(9 batch) loss: 1.068246\n",
      "(10 batch) loss: 0.998154\n",
      "(Epoch 30 / 100) train_acc: 0.596635; val_acc: 0.210000\n",
      "(0 batch) loss: 0.921383\n",
      "(1 batch) loss: 1.064238\n",
      "(2 batch) loss: 1.049956\n",
      "(3 batch) loss: 1.105648\n",
      "(4 batch) loss: 0.955681\n",
      "(5 batch) loss: 0.979982\n",
      "(6 batch) loss: 1.003061\n",
      "(7 batch) loss: 1.027415\n",
      "(8 batch) loss: 1.048542\n",
      "(9 batch) loss: 1.064024\n",
      "(10 batch) loss: 0.952884\n",
      "(Epoch 31 / 100) train_acc: 0.606250; val_acc: 0.220000\n",
      "(0 batch) loss: 1.019663\n",
      "(1 batch) loss: 0.978282\n",
      "(2 batch) loss: 0.992841\n",
      "(3 batch) loss: 1.019212\n",
      "(4 batch) loss: 1.108858\n",
      "(5 batch) loss: 1.006553\n",
      "(6 batch) loss: 1.050054\n",
      "(7 batch) loss: 1.059840\n",
      "(8 batch) loss: 0.970956\n",
      "(9 batch) loss: 1.066194\n",
      "(10 batch) loss: 1.255009\n",
      "(Epoch 32 / 100) train_acc: 0.561058; val_acc: 0.250000\n",
      "(0 batch) loss: 1.068032\n",
      "(1 batch) loss: 1.031147\n",
      "(2 batch) loss: 1.024194\n",
      "(3 batch) loss: 1.057224\n",
      "(4 batch) loss: 1.112381\n",
      "(5 batch) loss: 0.977084\n",
      "(6 batch) loss: 1.068873\n",
      "(7 batch) loss: 1.083169\n",
      "(8 batch) loss: 1.131321\n",
      "(9 batch) loss: 1.084664\n",
      "(10 batch) loss: 1.113592\n",
      "(Epoch 33 / 100) train_acc: 0.580769; val_acc: 0.260000\n",
      "(0 batch) loss: 1.042656\n",
      "(1 batch) loss: 1.092985\n",
      "(2 batch) loss: 0.929699\n",
      "(3 batch) loss: 0.975173\n",
      "(4 batch) loss: 1.055049\n",
      "(5 batch) loss: 0.939627\n",
      "(6 batch) loss: 0.973908\n",
      "(7 batch) loss: 1.036583\n",
      "(8 batch) loss: 1.019474\n",
      "(9 batch) loss: 1.056264\n",
      "(10 batch) loss: 1.058445\n",
      "(Epoch 34 / 100) train_acc: 0.607212; val_acc: 0.260000\n",
      "(0 batch) loss: 0.953564\n",
      "(1 batch) loss: 1.019859\n",
      "(2 batch) loss: 0.987504\n",
      "(3 batch) loss: 1.006764\n",
      "(4 batch) loss: 1.079794\n",
      "(5 batch) loss: 1.009776\n",
      "(6 batch) loss: 1.009919\n",
      "(7 batch) loss: 1.067704\n",
      "(8 batch) loss: 1.050244\n",
      "(9 batch) loss: 0.963781\n",
      "(10 batch) loss: 1.142064\n",
      "(Epoch 35 / 100) train_acc: 0.599519; val_acc: 0.260000\n",
      "(0 batch) loss: 0.922356\n",
      "(1 batch) loss: 0.987050\n",
      "(2 batch) loss: 1.002607\n",
      "(3 batch) loss: 0.961703\n",
      "(4 batch) loss: 0.911609\n",
      "(5 batch) loss: 0.993753\n",
      "(6 batch) loss: 1.095800\n",
      "(7 batch) loss: 1.021706\n",
      "(8 batch) loss: 1.032842\n",
      "(9 batch) loss: 0.986316\n",
      "(10 batch) loss: 0.928112\n",
      "(Epoch 36 / 100) train_acc: 0.612019; val_acc: 0.220000\n",
      "(0 batch) loss: 1.009479\n",
      "(1 batch) loss: 0.914872\n",
      "(2 batch) loss: 0.938289\n",
      "(3 batch) loss: 0.973894\n",
      "(4 batch) loss: 0.989202\n",
      "(5 batch) loss: 1.033072\n",
      "(6 batch) loss: 0.975225\n",
      "(7 batch) loss: 1.053227\n",
      "(8 batch) loss: 1.052145\n",
      "(9 batch) loss: 1.033973\n",
      "(10 batch) loss: 1.091098\n",
      "(Epoch 37 / 100) train_acc: 0.591827; val_acc: 0.320000\n",
      "(0 batch) loss: 1.045654\n",
      "(1 batch) loss: 1.032534\n",
      "(2 batch) loss: 0.993104\n",
      "(3 batch) loss: 1.131761\n",
      "(4 batch) loss: 0.968019\n",
      "(5 batch) loss: 1.053391\n",
      "(6 batch) loss: 1.034085\n",
      "(7 batch) loss: 1.054432\n",
      "(8 batch) loss: 1.005702\n",
      "(9 batch) loss: 0.959747\n",
      "(10 batch) loss: 0.792178\n",
      "(Epoch 38 / 100) train_acc: 0.601923; val_acc: 0.350000\n",
      "(0 batch) loss: 0.900939\n",
      "(1 batch) loss: 1.010407\n",
      "(2 batch) loss: 0.950434\n",
      "(3 batch) loss: 0.990765\n",
      "(4 batch) loss: 0.985693\n",
      "(5 batch) loss: 0.982015\n",
      "(6 batch) loss: 0.990970\n",
      "(7 batch) loss: 1.005252\n",
      "(8 batch) loss: 0.975560\n",
      "(9 batch) loss: 0.978767\n",
      "(10 batch) loss: 1.087467\n",
      "(Epoch 39 / 100) train_acc: 0.619231; val_acc: 0.300000\n",
      "(0 batch) loss: 0.927595\n",
      "(1 batch) loss: 0.856793\n",
      "(2 batch) loss: 0.966221\n",
      "(3 batch) loss: 0.950667\n",
      "(4 batch) loss: 0.882240\n",
      "(5 batch) loss: 1.010274\n",
      "(6 batch) loss: 0.932992\n",
      "(7 batch) loss: 0.977846\n",
      "(8 batch) loss: 0.926321\n",
      "(9 batch) loss: 1.035332\n",
      "(10 batch) loss: 0.963790\n",
      "(Epoch 40 / 100) train_acc: 0.658173; val_acc: 0.260000\n",
      "(0 batch) loss: 0.838317\n",
      "(1 batch) loss: 0.898463\n",
      "(2 batch) loss: 0.853813\n",
      "(3 batch) loss: 0.850547\n",
      "(4 batch) loss: 0.850657\n",
      "(5 batch) loss: 0.907731\n",
      "(6 batch) loss: 0.895581\n",
      "(7 batch) loss: 1.006107\n",
      "(8 batch) loss: 0.905642\n",
      "(9 batch) loss: 0.910504\n",
      "(10 batch) loss: 0.973576\n",
      "(Epoch 41 / 100) train_acc: 0.656731; val_acc: 0.260000\n",
      "(0 batch) loss: 1.005664\n",
      "(1 batch) loss: 0.929154\n",
      "(2 batch) loss: 0.807261\n",
      "(3 batch) loss: 0.929745\n",
      "(4 batch) loss: 0.920004\n",
      "(5 batch) loss: 0.898945\n",
      "(6 batch) loss: 0.944845\n",
      "(7 batch) loss: 0.899397\n",
      "(8 batch) loss: 0.817945\n",
      "(9 batch) loss: 1.015878\n",
      "(10 batch) loss: 1.037734\n",
      "(Epoch 42 / 100) train_acc: 0.648558; val_acc: 0.270000\n",
      "(0 batch) loss: 0.876715\n",
      "(1 batch) loss: 0.908797\n",
      "(2 batch) loss: 0.918481\n",
      "(3 batch) loss: 0.913724\n",
      "(4 batch) loss: 0.928695\n",
      "(5 batch) loss: 0.899057\n",
      "(6 batch) loss: 0.905938\n",
      "(7 batch) loss: 0.948259\n",
      "(8 batch) loss: 0.882214\n",
      "(9 batch) loss: 0.982313\n",
      "(10 batch) loss: 0.950771\n",
      "(Epoch 43 / 100) train_acc: 0.657692; val_acc: 0.280000\n",
      "(0 batch) loss: 0.789664\n",
      "(1 batch) loss: 0.878226\n",
      "(2 batch) loss: 0.975543\n",
      "(3 batch) loss: 0.903672\n",
      "(4 batch) loss: 0.888657\n",
      "(5 batch) loss: 0.876300\n",
      "(6 batch) loss: 0.911040\n",
      "(7 batch) loss: 0.951454\n",
      "(8 batch) loss: 0.983001\n",
      "(9 batch) loss: 0.965114\n",
      "(10 batch) loss: 0.935055\n",
      "(Epoch 44 / 100) train_acc: 0.657212; val_acc: 0.270000\n",
      "(0 batch) loss: 0.890591\n",
      "(1 batch) loss: 0.848479\n",
      "(2 batch) loss: 0.891338\n",
      "(3 batch) loss: 0.839304\n",
      "(4 batch) loss: 0.889220\n",
      "(5 batch) loss: 0.900382\n",
      "(6 batch) loss: 0.859403\n",
      "(7 batch) loss: 0.858840\n",
      "(8 batch) loss: 0.987783\n",
      "(9 batch) loss: 1.025856\n",
      "(10 batch) loss: 0.853646\n",
      "(Epoch 45 / 100) train_acc: 0.659615; val_acc: 0.240000\n",
      "(0 batch) loss: 0.919895\n",
      "(1 batch) loss: 0.846065\n",
      "(2 batch) loss: 0.824501\n",
      "(3 batch) loss: 0.926164\n",
      "(4 batch) loss: 0.787155\n",
      "(5 batch) loss: 0.936057\n",
      "(6 batch) loss: 0.866611\n",
      "(7 batch) loss: 0.874841\n",
      "(8 batch) loss: 0.874392\n",
      "(9 batch) loss: 0.937799\n",
      "(10 batch) loss: 0.809699\n",
      "(Epoch 46 / 100) train_acc: 0.685577; val_acc: 0.280000\n",
      "(0 batch) loss: 0.726006\n",
      "(1 batch) loss: 0.800443\n",
      "(2 batch) loss: 0.782954\n",
      "(3 batch) loss: 0.961540\n",
      "(4 batch) loss: 0.905689\n",
      "(5 batch) loss: 0.841464\n",
      "(6 batch) loss: 0.871347\n",
      "(7 batch) loss: 0.897599\n",
      "(8 batch) loss: 0.945349\n",
      "(9 batch) loss: 0.913317\n",
      "(10 batch) loss: 0.834379\n",
      "(Epoch 47 / 100) train_acc: 0.666346; val_acc: 0.240000\n",
      "(0 batch) loss: 0.876383\n",
      "(1 batch) loss: 0.882840\n",
      "(2 batch) loss: 0.881399\n",
      "(3 batch) loss: 0.853798\n",
      "(4 batch) loss: 0.976312\n",
      "(5 batch) loss: 0.852801\n",
      "(6 batch) loss: 0.977937\n",
      "(7 batch) loss: 0.913257\n",
      "(8 batch) loss: 0.964116\n",
      "(9 batch) loss: 0.937293\n",
      "(10 batch) loss: 0.934021\n",
      "(Epoch 48 / 100) train_acc: 0.632692; val_acc: 0.300000\n",
      "(0 batch) loss: 0.815709\n",
      "(1 batch) loss: 0.946608\n",
      "(2 batch) loss: 0.922194\n",
      "(3 batch) loss: 0.914538\n",
      "(4 batch) loss: 0.909159\n",
      "(5 batch) loss: 1.102542\n",
      "(6 batch) loss: 0.993549\n",
      "(7 batch) loss: 0.959966\n",
      "(8 batch) loss: 0.972440\n",
      "(9 batch) loss: 0.972428\n",
      "(10 batch) loss: 1.016481\n",
      "(Epoch 49 / 100) train_acc: 0.630288; val_acc: 0.250000\n",
      "(0 batch) loss: 0.886823\n",
      "(1 batch) loss: 0.909431\n",
      "(2 batch) loss: 0.945341\n",
      "(3 batch) loss: 0.947749\n",
      "(4 batch) loss: 0.946084\n",
      "(5 batch) loss: 0.938643\n",
      "(6 batch) loss: 0.892034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7 batch) loss: 0.909022\n",
      "(8 batch) loss: 0.950350\n",
      "(9 batch) loss: 0.987903\n",
      "(10 batch) loss: 0.933110\n",
      "(Epoch 50 / 100) train_acc: 0.640865; val_acc: 0.270000\n",
      "(0 batch) loss: 0.904073\n",
      "(1 batch) loss: 0.946685\n",
      "(2 batch) loss: 0.995041\n",
      "(3 batch) loss: 0.868285\n",
      "(4 batch) loss: 0.893939\n",
      "(5 batch) loss: 0.932583\n",
      "(6 batch) loss: 0.958168\n",
      "(7 batch) loss: 0.961319\n",
      "(8 batch) loss: 0.839650\n",
      "(9 batch) loss: 0.903221\n",
      "(10 batch) loss: 1.083911\n",
      "(Epoch 51 / 100) train_acc: 0.657212; val_acc: 0.280000\n",
      "(0 batch) loss: 0.779220\n",
      "(1 batch) loss: 0.882979\n",
      "(2 batch) loss: 0.836780\n",
      "(3 batch) loss: 0.900309\n",
      "(4 batch) loss: 0.981026\n",
      "(5 batch) loss: 0.832211\n",
      "(6 batch) loss: 0.897605\n",
      "(7 batch) loss: 0.886158\n",
      "(8 batch) loss: 0.979630\n",
      "(9 batch) loss: 0.931783\n",
      "(10 batch) loss: 0.888221\n",
      "(Epoch 52 / 100) train_acc: 0.679327; val_acc: 0.320000\n",
      "(0 batch) loss: 0.881051\n",
      "(1 batch) loss: 0.794617\n",
      "(2 batch) loss: 0.845092\n",
      "(3 batch) loss: 0.821384\n",
      "(4 batch) loss: 0.828497\n",
      "(5 batch) loss: 0.905685\n",
      "(6 batch) loss: 0.895118\n",
      "(7 batch) loss: 0.822144\n",
      "(8 batch) loss: 0.905318\n",
      "(9 batch) loss: 0.813780\n",
      "(10 batch) loss: 0.813144\n",
      "(Epoch 53 / 100) train_acc: 0.680769; val_acc: 0.250000\n",
      "(0 batch) loss: 0.701125\n",
      "(1 batch) loss: 0.892155\n",
      "(2 batch) loss: 0.850516\n",
      "(3 batch) loss: 0.839242\n",
      "(4 batch) loss: 0.738874\n",
      "(5 batch) loss: 0.878856\n",
      "(6 batch) loss: 0.864668\n",
      "(7 batch) loss: 0.900026\n",
      "(8 batch) loss: 0.920995\n",
      "(9 batch) loss: 0.822431\n",
      "(10 batch) loss: 0.873490\n",
      "(Epoch 54 / 100) train_acc: 0.685096; val_acc: 0.270000\n",
      "(0 batch) loss: 0.754755\n",
      "(1 batch) loss: 0.812593\n",
      "(2 batch) loss: 0.851392\n",
      "(3 batch) loss: 0.874440\n",
      "(4 batch) loss: 0.787893\n",
      "(5 batch) loss: 0.985486\n",
      "(6 batch) loss: 0.902429\n",
      "(7 batch) loss: 0.859007\n",
      "(8 batch) loss: 0.881549\n",
      "(9 batch) loss: 0.933536\n",
      "(10 batch) loss: 0.948336\n",
      "(Epoch 55 / 100) train_acc: 0.636538; val_acc: 0.280000\n",
      "(0 batch) loss: 0.837359\n",
      "(1 batch) loss: 0.930230\n",
      "(2 batch) loss: 0.906030\n",
      "(3 batch) loss: 0.834758\n",
      "(4 batch) loss: 0.859189\n",
      "(5 batch) loss: 0.852834\n",
      "(6 batch) loss: 0.918111\n",
      "(7 batch) loss: 0.905590\n",
      "(8 batch) loss: 0.952690\n",
      "(9 batch) loss: 1.038302\n",
      "(10 batch) loss: 0.998132\n",
      "(Epoch 56 / 100) train_acc: 0.638462; val_acc: 0.190000\n",
      "(0 batch) loss: 0.865721\n",
      "(1 batch) loss: 0.795339\n",
      "(2 batch) loss: 0.996408\n",
      "(3 batch) loss: 0.925530\n",
      "(4 batch) loss: 0.968796\n",
      "(5 batch) loss: 1.008987\n",
      "(6 batch) loss: 0.969058\n",
      "(7 batch) loss: 1.002388\n",
      "(8 batch) loss: 1.076307\n",
      "(9 batch) loss: 1.041465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-256:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/andrew/Desktop/Project/model_explore/.env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-859a4f2b1171>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;31m# print (out)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Project/model_explore/.env/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mCudaTransfer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Project/model_explore/.env/lib/python3.5/site-packages/torch/autograd/_functions/tensor.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, i, device, async)\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Project/model_explore/.env/lib/python3.5/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epoches):\n",
    "    for i, data in enumerate(dataloaders['train'], 0):\n",
    "        X_train, y_train = data\n",
    "        # Wrap them in Variable\n",
    "        X_train, y_train = Variable(X_train), Variable(y_train)\n",
    "        # forward + backward + optimize\n",
    "        out = model(X_train.cuda())\n",
    "        # print (out)\n",
    "        loss = loss_fn(out, y_train.long().cuda())\n",
    "        print('(%d batch) loss: %f' % (i, loss))\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_acc = model.check_accuracy(dataloaders['train'])\n",
    "    val_acc = model.check_accuracy(dataloaders['val'])\n",
    "    print('(Epoch %d / %d) train_acc: %f; val_acc: %f' % (epoch+1, num_epoches, train_acc, val_acc))\n",
    "    if (val_acc > best_acc):\n",
    "        best_acc = val_acc\n",
    "        torch.save(model, 'best_CHRONET.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = torch.load('best_CHRONET.pt')\n",
    "print (best_model.check_accuracy(dataloaders['val']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
