{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew/Desktop/Project/model_explore/.env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data.dataset import random_split\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/markdown"
   },
   "source": [
    "# Helper Clases / Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_Data(num):\n",
    "    if (num == -1): # All data\n",
    "        X_all = []\n",
    "        y_all = []\n",
    "        for i in range(8):\n",
    "            file_path = './../project_datasets/A0' + str(i+1) + 'T_slice.mat'\n",
    "            data = h5py.File(file_path, 'r')\n",
    "            X = np.copy(data['image'])\n",
    "            y = np.copy(data['type'])\n",
    "            X = X[:, 0:23, :]\n",
    "            X_all.append(X)\n",
    "            y = y[0,0:X.shape[0]:1]\n",
    "            y_all.append(y)\n",
    "        A, N, E, T = np.shape(X_all)\n",
    "        X_all = np.reshape(X_all, (A*N, E, T))\n",
    "        y_all = np.reshape(y_all, (-1))\n",
    "        y_all = y_all - 769\n",
    "        ## Remove NAN\n",
    "        index_Nan = []\n",
    "        for i in range(A*N):\n",
    "            for j in range(E):\n",
    "                if (any(np.isnan(X_all[i,j])) == True):\n",
    "                    index_Nan.append(i)\n",
    "        index_Nan = list(set(index_Nan))\n",
    "        X_all = np.delete(X_all, index_Nan, axis=0)\n",
    "        y_all = np.delete(y_all, index_Nan)\n",
    "        return (X_all, y_all)\n",
    "    else:\n",
    "        file_path = './../project_datasets/A0' + str(num) + 'T_slice.mat'\n",
    "        data = h5py.File(file_path, 'r')\n",
    "        X = np.copy(data['image'])\n",
    "        y = np.copy(data['type'])\n",
    "        X = X[:, 0:23, :]\n",
    "        y = y[0,0:X.shape[0]:1]\n",
    "        y = y - 769\n",
    "         ## Remove NAN\n",
    "        N, E, T = np.shape(X)\n",
    "        index_Nan = []\n",
    "        for i in range(N):\n",
    "            for j in range(E):\n",
    "                if (any(np.isnan(X[i,j])) == True):\n",
    "                    index_Nan.append(i)\n",
    "        index_Nan = list(set(index_Nan))\n",
    "        X = np.delete(X, index_Nan, axis=0)\n",
    "        y = np.delete(y, index_Nan)\n",
    "        return (X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2280, 23, 1000)\n"
     ]
    }
   ],
   "source": [
    "X, y = Load_Data(-1) # -1 to load all datas\n",
    "N, E, T = np.shape(X)\n",
    "print (np.shape(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_train = 200\n",
    "bs_val = 100\n",
    "bs_test = 100\n",
    "data = data_utils.TensorDataset(torch.Tensor(X), torch.Tensor(y))\n",
    "dset = {}\n",
    "dataloaders = {}\n",
    "dset['train'], dset['val'], dset['test'] = random_split(data, [N-bs_val-bs_test, bs_val, bs_test])\n",
    "dataloaders['train'] = data_utils.DataLoader(dset['train'], batch_size=bs_train, shuffle=True, num_workers=1)\n",
    "dataloaders['val'] = data_utils.DataLoader(dset['val'], batch_size=bs_val, shuffle=True, num_workers=1)\n",
    "dataloaders['test'] = data_utils.DataLoader(dset['test'], batch_size=bs_test, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myChronoNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layer, num_class):\n",
    "        super(myChronoNet, self).__init__()\n",
    "        self.num_layer = num_layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # First Inception layer\n",
    "        self.conv11 = nn.Conv1d(23, 32, 2, stride=2)\n",
    "        self.conv12 = nn.Conv1d(23, 32, 4, stride=2, padding=1)\n",
    "        self.conv13 = nn.Conv1d(23, 32, 8, stride=2, padding=3)\n",
    "        # Second Inception layer\n",
    "        self.conv21 = nn.Conv1d(96, 32, 2, stride=2)\n",
    "        self.conv22 = nn.Conv1d(96, 32, 4, stride=2, padding=1)\n",
    "        self.conv23 = nn.Conv1d(96, 32, 8, stride=2, padding=3)\n",
    "        # Third Inception layer\n",
    "        self.conv31 = nn.Conv1d(96, 32, 2, stride=2)\n",
    "        self.conv32 = nn.Conv1d(96, 32, 4, stride=2, padding=1)\n",
    "        self.conv33 = nn.Conv1d(96, 32, 8, stride=2, padding=3)\n",
    "        #self.conv_13 = nn.Conv2d()\n",
    "        self.conv_elec = nn.Conv3d(1,23,tuple([40, 23, 1]))\n",
    "        self.gru1 = nn.GRU(32*3, hidden_dim, num_layer, dropout=0.5)\n",
    "        self.gru2 = nn.GRU(hidden_dim, hidden_dim, num_layer, dropout=0.5)\n",
    "        self.gru3 = nn.GRU(hidden_dim*2, hidden_dim, num_layer, dropout=0.5)\n",
    "        self.gru4 = nn.GRU(hidden_dim*3, hidden_dim, num_layer, dropout=0.5)\n",
    "        self.classifier = nn.Linear(hidden_dim, num_class)\n",
    "    def forward(self, x):\n",
    "        out_conv11 = self.conv11(x)\n",
    "        out_conv12 = self.conv12(x)\n",
    "        out_conv13 = self.conv13(x)\n",
    "        out_conv1 = torch.cat((out_conv11, out_conv12, out_conv13), 1)\n",
    "        out_conv21 = self.conv21(out_conv1)\n",
    "        out_conv22 = self.conv22(out_conv1)\n",
    "        out_conv23 = self.conv23(out_conv1)\n",
    "        out_conv2 = torch.cat((out_conv21, out_conv22, out_conv23), 1)\n",
    "        out_conv31 = self.conv31(out_conv2)\n",
    "        out_conv32 = self.conv32(out_conv2)\n",
    "        out_conv33 = self.conv33(out_conv2)\n",
    "        out_conv3 = torch.cat((out_conv31, out_conv32, out_conv33), 1)\n",
    "        # N, C, L --> L, N, C\n",
    "        out_conv3 = out_conv3.permute(2,0,1)\n",
    "        out_gru1, _ = self.gru1(out_conv3)\n",
    "        out_gru2, _ = self.gru2(out_gru1)\n",
    "        out_gru12 = torch.cat((out_gru1, out_gru2), 2)\n",
    "        out_gru3, _ = self.gru3(out_gru12)\n",
    "        out_gru321 = torch.cat((out_gru1, out_gru2, out_gru3), 2)\n",
    "        out_gru4, _ = self.gru4(out_gru321)\n",
    "        out_gru4 = out_gru4[-1, :, :] # taking the last time seq\n",
    "        out = self.classifier(out_gru4)\n",
    "        return out\n",
    "    def check_accuracy(self, dataloader):\n",
    "        total_correct = 0\n",
    "        total_label = 0\n",
    "        for i_batch, sample_batched in enumerate(dataloader):\n",
    "            X_sample, y_sample = sample_batched\n",
    "            X_sample, y_sample = Variable(X_sample), Variable(y_sample)\n",
    "            out = self.forward(X_sample.cuda())\n",
    "            _, pred = torch.max(out, 1)\n",
    "            num_correct = np.sum(pred.data.cpu().numpy() == y_sample.data.cpu().numpy())\n",
    "            total_correct += num_correct\n",
    "            total_label += len(pred)\n",
    "        return  total_correct / total_label\n",
    "    \n",
    "class myGRU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layer, num_class):\n",
    "        super(myGRU, self).__init__()\n",
    "        self.num_layer = num_layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.conv_temp = nn.Conv2d(1,40,tuple([1,25]))\n",
    "        self.conv_elec = nn.Conv3d(1,23,tuple([40, 23, 1]))\n",
    "        self.gru1 = nn.GRU(input_dim, hidden_dim, num_layer)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.gru2 = nn.GRU(hidden_dim, hidden_dim, num_layer)\n",
    "        self.classifier = nn.Linear(hidden_dim, num_class)\n",
    "    def forward(self, x):\n",
    "        x = x.permute(2,0,1)\n",
    "        out_gru1, _ = self.gru1(x)\n",
    "        out_act1 = self.act1(out_gru1)\n",
    "        out_gru2, _ = self.gru2(out_act1)\n",
    "        out_gru2 = out_gru2[-1, :, :] # taking the last time seq\n",
    "        out = self.classifier(out_gru2)\n",
    "        return out\n",
    "    def check_accuracy(self, dataloader):\n",
    "        total_correct = 0\n",
    "        total_label = 0\n",
    "        for i_batch, sample_batched in enumerate(dataloader):\n",
    "            X_sample, y_sample = sample_batched\n",
    "            X_sample, y_sample = Variable(X_sample), Variable(y_sample)\n",
    "            out = self.forward(X_sample.cuda())\n",
    "            _, pred = torch.max(out, 1)\n",
    "            num_correct = np.sum(pred.data.cpu().numpy() == y_sample.data.cpu().numpy())\n",
    "            total_correct += num_correct\n",
    "            total_label += len(pred)\n",
    "        return  total_correct / total_label\n",
    "class myLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layer, num_class):\n",
    "        super(myLSTM, self).__init__()\n",
    "        self.num_layer = num_layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm1 = nn.LSTM(input_dim, hidden_dim, num_layer, batch_first=True)\n",
    "        self.classifier1 = nn.Linear(hidden_dim, 32)\n",
    "        self.act = nn.ReLU()\n",
    "        self.classifier2 = nn.Linear(32, num_class)\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1,2)\n",
    "        out_lstm1, _ = self.lstm1(x)\n",
    "        out_lstm1 = out_lstm1[:,-1,:]\n",
    "        out_lin1 = self.classifier1(out_lstm1)\n",
    "        out_act = self.act(out_lin1)\n",
    "        out = self.classifier2(out_act)\n",
    "        return out\n",
    "    def check_accuracy(self, dataloader):\n",
    "        total_correct = 0\n",
    "        total_label = 0\n",
    "        for i_batch, sample_batched in enumerate(dataloader):\n",
    "            X_sample, y_sample = sample_batched\n",
    "            # Flip axis first\n",
    "            X_sample, y_sample = Variable(X_sample), Variable(y_sample)\n",
    "            out = self.forward(X_sample.cuda())\n",
    "            _, pred = torch.max(out, 1)\n",
    "            num_correct = np.sum(pred.data.cpu().numpy() == y_sample.data.cpu().numpy())\n",
    "            total_correct += num_correct\n",
    "            total_label += len(pred)\n",
    "        return  total_correct / total_label\n",
    "    \n",
    "class myLSTMDO(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layer, num_class):\n",
    "        super(myLSTMDO, self).__init__()\n",
    "        self.num_layer = num_layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm1 = nn.LSTM(input_dim, hidden_dim, num_layer, batch_first=True, dropout=0.5)\n",
    "        self.classifier1 = nn.Linear(hidden_dim, 32)\n",
    "        self.act = nn.ReLU()\n",
    "        self.classifier2 = nn.Linear(32, num_class)\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1,2)\n",
    "        out_lstm1, _ = self.lstm1(x)\n",
    "        out_lstm1 = out_lstm1[:,-1,:]\n",
    "        out_lin1 = self.classifier1(out_lstm1)\n",
    "        out_act = self.act(out_lin1)\n",
    "        out = self.classifier2(out_act)\n",
    "        return out\n",
    "    def check_accuracy(self, dataloader):\n",
    "        total_correct = 0\n",
    "        total_label = 0\n",
    "        for i_batch, sample_batched in enumerate(dataloader):\n",
    "            X_sample, y_sample = sample_batched\n",
    "            # Flip axis first\n",
    "            X_sample, y_sample = Variable(X_sample), Variable(y_sample)\n",
    "            out = self.forward(X_sample.cuda())\n",
    "            _, pred = torch.max(out, 1)\n",
    "            num_correct = np.sum(pred.data.cpu().numpy() == y_sample.data.cpu().numpy())\n",
    "            total_correct += num_correct\n",
    "            total_label += len(pred)\n",
    "        return  total_correct / total_label\n",
    "\n",
    "class myCONVLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layer, num_class):\n",
    "        super(myCONVLSTM, self).__init__()\n",
    "        self.num_layer = num_layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.conv_temp = nn.Conv2d(1,40,tuple([1,25]))\n",
    "        self.conv_elec = nn.Conv3d(1,23,tuple([40, 23, 1]))\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layer, dropout=0.5)\n",
    "        self.classifier = nn.Linear(hidden_dim, num_class)\n",
    "    def forward(self, x):\n",
    "        x.unsqueeze_(1)\n",
    "        out_conv_temp = self.conv_temp(x)\n",
    "        out_conv_temp = out_conv_temp.unsqueeze_(1)\n",
    "        out_conv_elec = self.conv_elec(out_conv_temp)\n",
    "        out_conv_elec_sque= torch.squeeze(out_conv_elec)\n",
    "        out_swap = out_conv_elec_sque.permute(2,0,1)\n",
    "        out_lstm, _ = self.lstm(out_swap)\n",
    "        out_lstm = out_lstm[-1, :, :]\n",
    "        out = self.classifier(out_lstm)\n",
    "        return out\n",
    "    def check_accuracy(self, dataloader):\n",
    "        total_correct = 0\n",
    "        total_label = 0\n",
    "        for i_batch, sample_batched in enumerate(dataloader):\n",
    "            X_sample, y_sample = sample_batched\n",
    "            X_sample, y_sample = Variable(X_sample), Variable(y_sample)\n",
    "            out = self.forward(X_sample.cuda())\n",
    "            _, pred = torch.max(out, 1)\n",
    "            num_correct = np.sum(pred.data.cpu().numpy() == y_sample.data.cpu().numpy())\n",
    "            total_correct += num_correct\n",
    "            total_label += len(pred)\n",
    "        return  total_correct / total_label\n",
    "\n",
    "class mySHALLOWCONV(nn.Module):\n",
    "    def __init__(self, num_class):\n",
    "        super(mySHALLOWCONV, self).__init__()\n",
    "        self.conv_temp = nn.Conv2d(1,40,tuple([1,25]))\n",
    "        self.conv_elec = nn.Conv3d(1,40,tuple([40, 23, 1]))\n",
    "        self.pool = nn.AvgPool2d(tuple([1,47]))\n",
    "        self.classifier = nn.Linear(40*20, num_class)\n",
    "    def forward(self, x):\n",
    "        N, H, W = x.size()\n",
    "        x.unsqueeze_(1)\n",
    "        out_conv_temp = self.conv_temp(x)\n",
    "        out_conv_temp = out_conv_temp.unsqueeze_(1)\n",
    "        out_conv_elec = self.conv_elec(out_conv_temp)\n",
    "        out_conv_elec = torch.squeeze(out_conv_elec) # shape: [N, 40, 976]\n",
    "        out_conv_elec.unsqueeze_(1)\n",
    "        out_pool = self.pool(out_conv_elec) \n",
    "        out_pool = torch.squeeze(out_pool) # shape: [N, 40, 20]\n",
    "        out_pool = out_pool.view(N, -1) # shape: [N, 800]\n",
    "        out = self.classifier(out_pool)\n",
    "        return out\n",
    "    def check_accuracy(self, dataloader):\n",
    "        total_correct = 0\n",
    "        total_label = 0\n",
    "        for i_batch, sample_batched in enumerate(dataloader):\n",
    "            X_sample, y_sample = sample_batched\n",
    "            X_sample, y_sample = Variable(X_sample), Variable(y_sample)\n",
    "            out = self.forward(X_sample.cuda())\n",
    "            _, pred = torch.max(out, 1)\n",
    "            num_correct = np.sum(pred.data.cpu().numpy() == y_sample.data.cpu().numpy())\n",
    "            total_correct += num_correct\n",
    "            total_label += len(pred)\n",
    "        return  total_correct / total_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.cuda.FloatTensor\n",
    "hidden_dim = 32\n",
    "num_classes = 4\n",
    "num_epoches = 10\n",
    "model = myLSTM(E, hidden_dim, 1, num_classes)\n",
    "#model = myLSTMDO(E, hidden_dim, 1, num_classes)\n",
    "#model = myCONVLSTM(E, hidden_dim, 1, num_classes)\n",
    "#model = myGRU(E, hidden_dim, 1, num_classes)\n",
    "#model = myChronoNet(E, hidden_dim, 1, num_classes)\n",
    "#model = mySHALLOWCONV(4)\n",
    "model.type(dtype)\n",
    "loss_fn = nn.CrossEntropyLoss().type(dtype)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "(0 batch) loss: 1.396874\n",
      "(1 batch) loss: 1.389156\n",
      "(2 batch) loss: 1.382336\n",
      "(3 batch) loss: 1.390406\n",
      "(4 batch) loss: 1.389439\n",
      "(5 batch) loss: 1.391434\n",
      "(6 batch) loss: 1.385312\n",
      "(7 batch) loss: 1.381225\n",
      "(8 batch) loss: 1.373210\n",
      "(9 batch) loss: 1.367425\n",
      "(10 batch) loss: 1.383956\n",
      "(Epoch 1 / 10) train_acc: 0.329327; val_acc: 0.390000\n",
      "(0 batch) loss: 1.381437\n",
      "(1 batch) loss: 1.359275\n",
      "(2 batch) loss: 1.352779\n",
      "(3 batch) loss: 1.359288\n",
      "(4 batch) loss: 1.360952\n",
      "(5 batch) loss: 1.364799\n",
      "(6 batch) loss: 1.346996\n",
      "(7 batch) loss: 1.345513\n",
      "(8 batch) loss: 1.354444\n",
      "(9 batch) loss: 1.352472\n",
      "(10 batch) loss: 1.356031\n",
      "(Epoch 2 / 10) train_acc: 0.384615; val_acc: 0.390000\n",
      "(0 batch) loss: 1.342272\n",
      "(1 batch) loss: 1.336842\n",
      "(2 batch) loss: 1.340346\n",
      "(3 batch) loss: 1.346978\n",
      "(4 batch) loss: 1.324788\n",
      "(5 batch) loss: 1.328349\n",
      "(6 batch) loss: 1.324183\n",
      "(7 batch) loss: 1.327637\n",
      "(8 batch) loss: 1.335470\n",
      "(9 batch) loss: 1.312291\n",
      "(10 batch) loss: 1.311313\n",
      "(Epoch 3 / 10) train_acc: 0.411538; val_acc: 0.380000\n",
      "(0 batch) loss: 1.305750\n",
      "(1 batch) loss: 1.318011\n",
      "(2 batch) loss: 1.307529\n",
      "(3 batch) loss: 1.291499\n",
      "(4 batch) loss: 1.289419\n",
      "(5 batch) loss: 1.311333\n",
      "(6 batch) loss: 1.276818\n",
      "(7 batch) loss: 1.293043\n",
      "(8 batch) loss: 1.300731\n",
      "(9 batch) loss: 1.276397\n",
      "(10 batch) loss: 1.281458\n",
      "(Epoch 4 / 10) train_acc: 0.413942; val_acc: 0.430000\n",
      "(0 batch) loss: 1.281128\n",
      "(1 batch) loss: 1.235904\n",
      "(2 batch) loss: 1.276937\n",
      "(3 batch) loss: 1.244946\n",
      "(4 batch) loss: 1.306179\n",
      "(5 batch) loss: 1.239853\n",
      "(6 batch) loss: 1.259687\n",
      "(7 batch) loss: 1.284447\n",
      "(8 batch) loss: 1.250220\n",
      "(9 batch) loss: 1.242325\n",
      "(10 batch) loss: 1.241619\n",
      "(Epoch 5 / 10) train_acc: 0.419712; val_acc: 0.420000\n",
      "(0 batch) loss: 1.256126\n",
      "(1 batch) loss: 1.220114\n",
      "(2 batch) loss: 1.197443\n",
      "(3 batch) loss: 1.234477\n",
      "(4 batch) loss: 1.210021\n",
      "(5 batch) loss: 1.197128\n",
      "(6 batch) loss: 1.270862\n",
      "(7 batch) loss: 1.276030\n",
      "(8 batch) loss: 1.239281\n",
      "(9 batch) loss: 1.258431\n",
      "(10 batch) loss: 1.270966\n",
      "(Epoch 6 / 10) train_acc: 0.436058; val_acc: 0.460000\n",
      "(0 batch) loss: 1.255667\n",
      "(1 batch) loss: 1.219124\n",
      "(2 batch) loss: 1.217455\n",
      "(3 batch) loss: 1.236957\n",
      "(4 batch) loss: 1.194915\n",
      "(5 batch) loss: 1.216928\n",
      "(6 batch) loss: 1.200037\n",
      "(7 batch) loss: 1.178872\n",
      "(8 batch) loss: 1.182147\n",
      "(9 batch) loss: 1.155658\n",
      "(10 batch) loss: 1.191235\n",
      "(Epoch 7 / 10) train_acc: 0.455288; val_acc: 0.400000\n",
      "(0 batch) loss: 1.222690\n",
      "(1 batch) loss: 1.192019\n",
      "(2 batch) loss: 1.188036\n",
      "(3 batch) loss: 1.154077\n",
      "(4 batch) loss: 1.229277\n",
      "(5 batch) loss: 1.193926\n",
      "(6 batch) loss: 1.175885\n",
      "(7 batch) loss: 1.153640\n",
      "(8 batch) loss: 1.185384\n",
      "(9 batch) loss: 1.162216\n",
      "(10 batch) loss: 1.206771\n",
      "(Epoch 8 / 10) train_acc: 0.470192; val_acc: 0.430000\n",
      "(0 batch) loss: 1.130841\n",
      "(1 batch) loss: 1.165021\n",
      "(2 batch) loss: 1.121292\n",
      "(3 batch) loss: 1.190973\n",
      "(4 batch) loss: 1.187221\n",
      "(5 batch) loss: 1.122148\n",
      "(6 batch) loss: 1.167823\n",
      "(7 batch) loss: 1.141652\n",
      "(8 batch) loss: 1.161702\n",
      "(9 batch) loss: 1.218493\n",
      "(10 batch) loss: 1.269459\n",
      "(Epoch 9 / 10) train_acc: 0.488462; val_acc: 0.430000\n",
      "(0 batch) loss: 1.127735\n",
      "(1 batch) loss: 1.158916\n",
      "(2 batch) loss: 1.195294\n",
      "(3 batch) loss: 1.101508\n",
      "(4 batch) loss: 1.129293\n",
      "(5 batch) loss: 1.157778\n",
      "(6 batch) loss: 1.152704\n",
      "(7 batch) loss: 1.109556\n",
      "(8 batch) loss: 1.178104\n",
      "(9 batch) loss: 1.193423\n",
      "(10 batch) loss: 1.073123\n",
      "(Epoch 10 / 10) train_acc: 0.508173; val_acc: 0.400000\n",
      "200\n",
      "(0 batch) loss: 1.526452\n",
      "(1 batch) loss: 1.522558\n",
      "(2 batch) loss: 1.505123\n",
      "(3 batch) loss: 1.499279\n",
      "(4 batch) loss: 1.504901\n",
      "(5 batch) loss: 1.447547\n",
      "(6 batch) loss: 1.470176\n",
      "(7 batch) loss: 1.447872\n",
      "(8 batch) loss: 1.489533\n",
      "(9 batch) loss: 1.438855\n",
      "(10 batch) loss: 1.469396\n",
      "(Epoch 1 / 10) train_acc: 0.254327; val_acc: 0.220000\n",
      "(0 batch) loss: 1.399799\n",
      "(1 batch) loss: 1.444920\n",
      "(2 batch) loss: 1.406443\n",
      "(3 batch) loss: 1.424922\n",
      "(4 batch) loss: 1.388691\n",
      "(5 batch) loss: 1.400797\n",
      "(6 batch) loss: 1.377463\n",
      "(7 batch) loss: 1.380265\n",
      "(8 batch) loss: 1.375954\n",
      "(9 batch) loss: 1.393107\n",
      "(10 batch) loss: 1.359959\n",
      "(Epoch 2 / 10) train_acc: 0.270673; val_acc: 0.310000\n",
      "(0 batch) loss: 1.377442\n",
      "(1 batch) loss: 1.379468\n",
      "(2 batch) loss: 1.359656\n",
      "(3 batch) loss: 1.368114\n",
      "(4 batch) loss: 1.359592\n",
      "(5 batch) loss: 1.377562\n",
      "(6 batch) loss: 1.383730\n",
      "(7 batch) loss: 1.384431\n",
      "(8 batch) loss: 1.360204\n",
      "(9 batch) loss: 1.370647\n",
      "(10 batch) loss: 1.367597\n",
      "(Epoch 3 / 10) train_acc: 0.327885; val_acc: 0.320000\n",
      "(0 batch) loss: 1.363613\n",
      "(1 batch) loss: 1.368264\n",
      "(2 batch) loss: 1.359502\n",
      "(3 batch) loss: 1.366582\n",
      "(4 batch) loss: 1.343792\n",
      "(5 batch) loss: 1.369945\n",
      "(6 batch) loss: 1.348135\n",
      "(7 batch) loss: 1.328697\n",
      "(8 batch) loss: 1.360333\n",
      "(9 batch) loss: 1.357615\n",
      "(10 batch) loss: 1.376751\n",
      "(Epoch 4 / 10) train_acc: 0.354808; val_acc: 0.310000\n",
      "(0 batch) loss: 1.356022\n",
      "(1 batch) loss: 1.341535\n",
      "(2 batch) loss: 1.336860\n",
      "(3 batch) loss: 1.338570\n",
      "(4 batch) loss: 1.363129\n",
      "(5 batch) loss: 1.345348\n",
      "(6 batch) loss: 1.333780\n",
      "(7 batch) loss: 1.353262\n",
      "(8 batch) loss: 1.336679\n",
      "(9 batch) loss: 1.340754\n",
      "(10 batch) loss: 1.336948\n",
      "(Epoch 5 / 10) train_acc: 0.386538; val_acc: 0.290000\n",
      "(0 batch) loss: 1.330668\n",
      "(1 batch) loss: 1.343901\n",
      "(2 batch) loss: 1.303252\n",
      "(3 batch) loss: 1.335766\n",
      "(4 batch) loss: 1.340165\n",
      "(5 batch) loss: 1.330713\n",
      "(6 batch) loss: 1.335014\n",
      "(7 batch) loss: 1.319990\n",
      "(8 batch) loss: 1.320670\n",
      "(9 batch) loss: 1.358318\n",
      "(10 batch) loss: 1.296443\n",
      "(Epoch 6 / 10) train_acc: 0.397596; val_acc: 0.270000\n",
      "(0 batch) loss: 1.323622\n",
      "(1 batch) loss: 1.313677\n",
      "(2 batch) loss: 1.297582\n",
      "(3 batch) loss: 1.328797\n",
      "(4 batch) loss: 1.340730\n",
      "(5 batch) loss: 1.305260\n",
      "(6 batch) loss: 1.318276\n",
      "(7 batch) loss: 1.325829\n",
      "(8 batch) loss: 1.294083\n",
      "(9 batch) loss: 1.308866\n",
      "(10 batch) loss: 1.317793\n",
      "(Epoch 7 / 10) train_acc: 0.417308; val_acc: 0.370000\n",
      "(0 batch) loss: 1.323817\n",
      "(1 batch) loss: 1.313108\n",
      "(2 batch) loss: 1.301468\n",
      "(3 batch) loss: 1.299770\n",
      "(4 batch) loss: 1.299484\n",
      "(5 batch) loss: 1.307825\n",
      "(6 batch) loss: 1.300750\n",
      "(7 batch) loss: 1.279370\n",
      "(8 batch) loss: 1.284658\n",
      "(9 batch) loss: 1.277257\n",
      "(10 batch) loss: 1.275549\n",
      "(Epoch 8 / 10) train_acc: 0.424519; val_acc: 0.380000\n",
      "(0 batch) loss: 1.270303\n",
      "(1 batch) loss: 1.283405\n",
      "(2 batch) loss: 1.268555\n",
      "(3 batch) loss: 1.268848\n",
      "(4 batch) loss: 1.319305\n",
      "(5 batch) loss: 1.282327\n",
      "(6 batch) loss: 1.268543\n",
      "(7 batch) loss: 1.292543\n",
      "(8 batch) loss: 1.265111\n",
      "(9 batch) loss: 1.276186\n",
      "(10 batch) loss: 1.301044\n",
      "(Epoch 9 / 10) train_acc: 0.436058; val_acc: 0.380000\n",
      "(0 batch) loss: 1.292076\n",
      "(1 batch) loss: 1.245603\n",
      "(2 batch) loss: 1.241660\n",
      "(3 batch) loss: 1.261876\n",
      "(4 batch) loss: 1.268114\n",
      "(5 batch) loss: 1.236242\n",
      "(6 batch) loss: 1.288381\n",
      "(7 batch) loss: 1.241153\n",
      "(8 batch) loss: 1.252318\n",
      "(9 batch) loss: 1.237769\n",
      "(10 batch) loss: 1.311291\n",
      "(Epoch 10 / 10) train_acc: 0.452885; val_acc: 0.330000\n",
      "300\n",
      "(0 batch) loss: 1.432715\n",
      "(1 batch) loss: 1.425264\n",
      "(2 batch) loss: 1.429708\n",
      "(3 batch) loss: 1.478098\n",
      "(4 batch) loss: 1.425594\n",
      "(5 batch) loss: 1.387405\n",
      "(6 batch) loss: 1.385646\n",
      "(7 batch) loss: 1.399763\n",
      "(8 batch) loss: 1.410472\n",
      "(9 batch) loss: 1.398044\n",
      "(10 batch) loss: 1.391842\n",
      "(Epoch 1 / 10) train_acc: 0.319231; val_acc: 0.300000\n",
      "(0 batch) loss: 1.359077\n",
      "(1 batch) loss: 1.379145\n",
      "(2 batch) loss: 1.334358\n",
      "(3 batch) loss: 1.371316\n",
      "(4 batch) loss: 1.371324\n",
      "(5 batch) loss: 1.333235\n",
      "(6 batch) loss: 1.394835\n",
      "(7 batch) loss: 1.363755\n",
      "(8 batch) loss: 1.324994\n",
      "(9 batch) loss: 1.370360\n",
      "(10 batch) loss: 1.367984\n",
      "(Epoch 2 / 10) train_acc: 0.363942; val_acc: 0.350000\n",
      "(0 batch) loss: 1.328471\n",
      "(1 batch) loss: 1.327391\n",
      "(2 batch) loss: 1.315383\n",
      "(3 batch) loss: 1.316600\n",
      "(4 batch) loss: 1.361213\n",
      "(5 batch) loss: 1.324314\n",
      "(6 batch) loss: 1.286557\n",
      "(7 batch) loss: 1.300730\n",
      "(8 batch) loss: 1.335781\n",
      "(9 batch) loss: 1.328543\n",
      "(10 batch) loss: 1.375897\n",
      "(Epoch 3 / 10) train_acc: 0.391827; val_acc: 0.390000\n",
      "(0 batch) loss: 1.332528\n",
      "(1 batch) loss: 1.301211\n",
      "(2 batch) loss: 1.260991\n",
      "(3 batch) loss: 1.300726\n",
      "(4 batch) loss: 1.320718\n",
      "(5 batch) loss: 1.305578\n",
      "(6 batch) loss: 1.326062\n",
      "(7 batch) loss: 1.267788\n",
      "(8 batch) loss: 1.313396\n",
      "(9 batch) loss: 1.286960\n",
      "(10 batch) loss: 1.326361\n",
      "(Epoch 4 / 10) train_acc: 0.409615; val_acc: 0.400000\n",
      "(0 batch) loss: 1.291926\n",
      "(1 batch) loss: 1.299605\n",
      "(2 batch) loss: 1.254987\n",
      "(3 batch) loss: 1.292122\n",
      "(4 batch) loss: 1.279497\n",
      "(5 batch) loss: 1.284778\n",
      "(6 batch) loss: 1.248435\n",
      "(7 batch) loss: 1.267556\n",
      "(8 batch) loss: 1.274691\n",
      "(9 batch) loss: 1.296979\n",
      "(10 batch) loss: 1.294399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5 / 10) train_acc: 0.423558; val_acc: 0.390000\n",
      "(0 batch) loss: 1.232226\n",
      "(1 batch) loss: 1.267311\n",
      "(2 batch) loss: 1.196959\n",
      "(3 batch) loss: 1.261885\n",
      "(4 batch) loss: 1.319560\n",
      "(5 batch) loss: 1.283842\n",
      "(6 batch) loss: 1.265229\n",
      "(7 batch) loss: 1.303769\n",
      "(8 batch) loss: 1.207104\n",
      "(9 batch) loss: 1.266951\n",
      "(10 batch) loss: 1.288167\n",
      "(Epoch 6 / 10) train_acc: 0.437981; val_acc: 0.450000\n",
      "(0 batch) loss: 1.225047\n",
      "(1 batch) loss: 1.251447\n",
      "(2 batch) loss: 1.249230\n",
      "(3 batch) loss: 1.262975\n",
      "(4 batch) loss: 1.258242\n",
      "(5 batch) loss: 1.242198\n",
      "(6 batch) loss: 1.214908\n",
      "(7 batch) loss: 1.224271\n",
      "(8 batch) loss: 1.223294\n",
      "(9 batch) loss: 1.225596\n",
      "(10 batch) loss: 1.262219\n",
      "(Epoch 7 / 10) train_acc: 0.449038; val_acc: 0.440000\n",
      "(0 batch) loss: 1.199998\n",
      "(1 batch) loss: 1.184673\n",
      "(2 batch) loss: 1.248032\n",
      "(3 batch) loss: 1.203829\n",
      "(4 batch) loss: 1.238664\n",
      "(5 batch) loss: 1.199477\n",
      "(6 batch) loss: 1.229819\n",
      "(7 batch) loss: 1.235517\n",
      "(8 batch) loss: 1.258967\n",
      "(9 batch) loss: 1.196258\n",
      "(10 batch) loss: 1.191972\n",
      "(Epoch 8 / 10) train_acc: 0.456250; val_acc: 0.430000\n",
      "(0 batch) loss: 1.137163\n",
      "(1 batch) loss: 1.208682\n",
      "(2 batch) loss: 1.263600\n",
      "(3 batch) loss: 1.172642\n",
      "(4 batch) loss: 1.228637\n",
      "(5 batch) loss: 1.238493\n",
      "(6 batch) loss: 1.205551\n",
      "(7 batch) loss: 1.210264\n",
      "(8 batch) loss: 1.168781\n",
      "(9 batch) loss: 1.188933\n",
      "(10 batch) loss: 1.189878\n",
      "(Epoch 9 / 10) train_acc: 0.480288; val_acc: 0.430000\n",
      "(0 batch) loss: 1.280300\n",
      "(1 batch) loss: 1.155472\n",
      "(2 batch) loss: 1.141834\n",
      "(3 batch) loss: 1.173970\n",
      "(4 batch) loss: 1.174548\n",
      "(5 batch) loss: 1.143767\n",
      "(6 batch) loss: 1.203019\n",
      "(7 batch) loss: 1.208460\n",
      "(8 batch) loss: 1.185262\n",
      "(9 batch) loss: 1.154986\n",
      "(10 batch) loss: 1.173368\n",
      "(Epoch 10 / 10) train_acc: 0.493750; val_acc: 0.440000\n",
      "400\n",
      "(0 batch) loss: 1.379537\n",
      "(1 batch) loss: 1.351232\n",
      "(2 batch) loss: 1.325982\n",
      "(3 batch) loss: 1.391545\n",
      "(4 batch) loss: 1.349335\n",
      "(5 batch) loss: 1.451661\n",
      "(6 batch) loss: 1.458871\n",
      "(7 batch) loss: 1.384566\n",
      "(8 batch) loss: 1.396767\n",
      "(9 batch) loss: 1.339889\n",
      "(10 batch) loss: 1.375719\n",
      "(Epoch 1 / 10) train_acc: 0.355769; val_acc: 0.330000\n",
      "(0 batch) loss: 1.314689\n",
      "(1 batch) loss: 1.322407\n",
      "(2 batch) loss: 1.293537\n",
      "(3 batch) loss: 1.317036\n",
      "(4 batch) loss: 1.338353\n",
      "(5 batch) loss: 1.327347\n",
      "(6 batch) loss: 1.304687\n",
      "(7 batch) loss: 1.340031\n",
      "(8 batch) loss: 1.320503\n",
      "(9 batch) loss: 1.307883\n",
      "(10 batch) loss: 1.319443\n",
      "(Epoch 2 / 10) train_acc: 0.404327; val_acc: 0.370000\n",
      "(0 batch) loss: 1.252443\n",
      "(1 batch) loss: 1.263741\n",
      "(2 batch) loss: 1.346844\n",
      "(3 batch) loss: 1.246297\n",
      "(4 batch) loss: 1.304235\n",
      "(5 batch) loss: 1.284339\n",
      "(6 batch) loss: 1.276672\n",
      "(7 batch) loss: 1.293452\n",
      "(8 batch) loss: 1.307533\n",
      "(9 batch) loss: 1.339272\n",
      "(10 batch) loss: 1.246189\n",
      "(Epoch 3 / 10) train_acc: 0.427404; val_acc: 0.340000\n",
      "(0 batch) loss: 1.244029\n",
      "(1 batch) loss: 1.269807\n",
      "(2 batch) loss: 1.276645\n",
      "(3 batch) loss: 1.221953\n",
      "(4 batch) loss: 1.291453\n",
      "(5 batch) loss: 1.284762\n",
      "(6 batch) loss: 1.256417\n",
      "(7 batch) loss: 1.300752\n",
      "(8 batch) loss: 1.218503\n",
      "(9 batch) loss: 1.270108\n",
      "(10 batch) loss: 1.236660\n",
      "(Epoch 4 / 10) train_acc: 0.454327; val_acc: 0.340000\n",
      "(0 batch) loss: 1.215501\n",
      "(1 batch) loss: 1.223124\n",
      "(2 batch) loss: 1.236869\n",
      "(3 batch) loss: 1.209907\n",
      "(4 batch) loss: 1.223755\n",
      "(5 batch) loss: 1.224872\n",
      "(6 batch) loss: 1.254499\n",
      "(7 batch) loss: 1.265523\n",
      "(8 batch) loss: 1.310871\n",
      "(9 batch) loss: 1.217101\n",
      "(10 batch) loss: 1.248363\n",
      "(Epoch 5 / 10) train_acc: 0.477885; val_acc: 0.330000\n",
      "(0 batch) loss: 1.232434\n",
      "(1 batch) loss: 1.217605\n",
      "(2 batch) loss: 1.214479\n",
      "(3 batch) loss: 1.219624\n",
      "(4 batch) loss: 1.208469\n",
      "(5 batch) loss: 1.179908\n",
      "(6 batch) loss: 1.254048\n",
      "(7 batch) loss: 1.265531\n",
      "(8 batch) loss: 1.171574\n",
      "(9 batch) loss: 1.167030\n",
      "(10 batch) loss: 1.321830\n",
      "(Epoch 6 / 10) train_acc: 0.479808; val_acc: 0.340000\n",
      "(0 batch) loss: 1.197881\n",
      "(1 batch) loss: 1.261510\n",
      "(2 batch) loss: 1.166220\n",
      "(3 batch) loss: 1.206253\n",
      "(4 batch) loss: 1.204524\n",
      "(5 batch) loss: 1.220128\n",
      "(6 batch) loss: 1.184001\n",
      "(7 batch) loss: 1.156265\n",
      "(8 batch) loss: 1.180396\n",
      "(9 batch) loss: 1.210978\n",
      "(10 batch) loss: 1.268300\n",
      "(Epoch 7 / 10) train_acc: 0.502885; val_acc: 0.370000\n",
      "(0 batch) loss: 1.164522\n",
      "(1 batch) loss: 1.239708\n",
      "(2 batch) loss: 1.149259\n",
      "(3 batch) loss: 1.231591\n",
      "(4 batch) loss: 1.163271\n",
      "(5 batch) loss: 1.201455\n",
      "(6 batch) loss: 1.098481\n",
      "(7 batch) loss: 1.184648\n",
      "(8 batch) loss: 1.161140\n",
      "(9 batch) loss: 1.155851\n",
      "(10 batch) loss: 1.125654\n",
      "(Epoch 8 / 10) train_acc: 0.512019; val_acc: 0.350000\n",
      "(0 batch) loss: 1.138160\n",
      "(1 batch) loss: 1.117267\n",
      "(2 batch) loss: 1.130196\n",
      "(3 batch) loss: 1.082169\n",
      "(4 batch) loss: 1.142845\n",
      "(5 batch) loss: 1.176752\n",
      "(6 batch) loss: 1.192685\n",
      "(7 batch) loss: 1.201671\n",
      "(8 batch) loss: 1.158985\n",
      "(9 batch) loss: 1.213303\n",
      "(10 batch) loss: 1.251558\n",
      "(Epoch 9 / 10) train_acc: 0.527404; val_acc: 0.280000\n",
      "(0 batch) loss: 1.146203\n",
      "(1 batch) loss: 1.133582\n",
      "(2 batch) loss: 1.147041\n",
      "(3 batch) loss: 1.091056\n",
      "(4 batch) loss: 1.154734\n",
      "(5 batch) loss: 1.208324\n",
      "(6 batch) loss: 1.084935\n",
      "(7 batch) loss: 1.107879\n",
      "(8 batch) loss: 1.106385\n",
      "(9 batch) loss: 1.176664\n",
      "(10 batch) loss: 1.186524\n",
      "(Epoch 10 / 10) train_acc: 0.542308; val_acc: 0.350000\n",
      "500\n",
      "(0 batch) loss: 1.469356\n",
      "(1 batch) loss: 1.536301\n",
      "(2 batch) loss: 1.473829\n",
      "(3 batch) loss: 1.519439\n",
      "(4 batch) loss: 1.495438\n",
      "(5 batch) loss: 1.511959\n",
      "(6 batch) loss: 1.421433\n",
      "(7 batch) loss: 1.434063\n",
      "(8 batch) loss: 1.496377\n",
      "(9 batch) loss: 1.497090\n",
      "(10 batch) loss: 1.433486\n",
      "(Epoch 1 / 10) train_acc: 0.308654; val_acc: 0.270000\n",
      "(0 batch) loss: 1.396357\n",
      "(1 batch) loss: 1.429142\n",
      "(2 batch) loss: 1.374452\n",
      "(3 batch) loss: 1.489283\n",
      "(4 batch) loss: 1.368562\n",
      "(5 batch) loss: 1.398049\n",
      "(6 batch) loss: 1.385139\n",
      "(7 batch) loss: 1.401977\n",
      "(8 batch) loss: 1.349815\n",
      "(9 batch) loss: 1.399502\n",
      "(10 batch) loss: 1.425894\n",
      "(Epoch 2 / 10) train_acc: 0.342308; val_acc: 0.260000\n",
      "(0 batch) loss: 1.327486\n",
      "(1 batch) loss: 1.339402\n",
      "(2 batch) loss: 1.374102\n",
      "(3 batch) loss: 1.338159\n",
      "(4 batch) loss: 1.331429\n",
      "(5 batch) loss: 1.324128\n",
      "(6 batch) loss: 1.367222\n",
      "(7 batch) loss: 1.386979\n",
      "(8 batch) loss: 1.368386\n",
      "(9 batch) loss: 1.322163\n",
      "(10 batch) loss: 1.374100\n",
      "(Epoch 3 / 10) train_acc: 0.364423; val_acc: 0.240000\n",
      "(0 batch) loss: 1.329190\n",
      "(1 batch) loss: 1.339442\n",
      "(2 batch) loss: 1.329560\n",
      "(3 batch) loss: 1.322227\n",
      "(4 batch) loss: 1.361740\n",
      "(5 batch) loss: 1.318182\n",
      "(6 batch) loss: 1.334376\n",
      "(7 batch) loss: 1.314495\n",
      "(8 batch) loss: 1.299371\n",
      "(9 batch) loss: 1.278236\n",
      "(10 batch) loss: 1.315709\n",
      "(Epoch 4 / 10) train_acc: 0.392788; val_acc: 0.230000\n",
      "(0 batch) loss: 1.305823\n",
      "(1 batch) loss: 1.294283\n",
      "(2 batch) loss: 1.268936\n",
      "(3 batch) loss: 1.313480\n",
      "(4 batch) loss: 1.267820\n",
      "(5 batch) loss: 1.310346\n",
      "(6 batch) loss: 1.313392\n",
      "(7 batch) loss: 1.311048\n",
      "(8 batch) loss: 1.299805\n",
      "(9 batch) loss: 1.324672\n",
      "(10 batch) loss: 1.313987\n",
      "(Epoch 5 / 10) train_acc: 0.410577; val_acc: 0.220000\n",
      "(0 batch) loss: 1.300186\n",
      "(1 batch) loss: 1.328032\n",
      "(2 batch) loss: 1.256229\n",
      "(3 batch) loss: 1.260508\n",
      "(4 batch) loss: 1.268792\n",
      "(5 batch) loss: 1.289597\n",
      "(6 batch) loss: 1.266419\n",
      "(7 batch) loss: 1.281756\n",
      "(8 batch) loss: 1.303285\n",
      "(9 batch) loss: 1.273278\n",
      "(10 batch) loss: 1.335222\n",
      "(Epoch 6 / 10) train_acc: 0.425000; val_acc: 0.220000\n",
      "(0 batch) loss: 1.303733\n",
      "(1 batch) loss: 1.202159\n",
      "(2 batch) loss: 1.243567\n",
      "(3 batch) loss: 1.292945\n",
      "(4 batch) loss: 1.268516\n",
      "(5 batch) loss: 1.313848\n",
      "(6 batch) loss: 1.262638\n",
      "(7 batch) loss: 1.256130\n",
      "(8 batch) loss: 1.260449\n",
      "(9 batch) loss: 1.254070\n",
      "(10 batch) loss: 1.286900\n",
      "(Epoch 7 / 10) train_acc: 0.453365; val_acc: 0.210000\n",
      "(0 batch) loss: 1.241515\n",
      "(1 batch) loss: 1.256624\n",
      "(2 batch) loss: 1.221869\n",
      "(3 batch) loss: 1.255078\n",
      "(4 batch) loss: 1.245474\n",
      "(5 batch) loss: 1.282798\n",
      "(6 batch) loss: 1.254702\n",
      "(7 batch) loss: 1.224407\n",
      "(8 batch) loss: 1.273733\n",
      "(9 batch) loss: 1.231806\n",
      "(10 batch) loss: 1.229745\n",
      "(Epoch 8 / 10) train_acc: 0.463462; val_acc: 0.240000\n",
      "(0 batch) loss: 1.223216\n",
      "(1 batch) loss: 1.227381\n",
      "(2 batch) loss: 1.228320\n",
      "(3 batch) loss: 1.226729\n",
      "(4 batch) loss: 1.197142\n",
      "(5 batch) loss: 1.272670\n",
      "(6 batch) loss: 1.220419\n",
      "(7 batch) loss: 1.208797\n",
      "(8 batch) loss: 1.226673\n",
      "(9 batch) loss: 1.248395\n",
      "(10 batch) loss: 1.197985\n",
      "(Epoch 9 / 10) train_acc: 0.480288; val_acc: 0.270000\n",
      "(0 batch) loss: 1.149140\n",
      "(1 batch) loss: 1.186176\n",
      "(2 batch) loss: 1.192879\n",
      "(3 batch) loss: 1.209528\n",
      "(4 batch) loss: 1.168124\n",
      "(5 batch) loss: 1.214890\n",
      "(6 batch) loss: 1.158304\n",
      "(7 batch) loss: 1.220527\n",
      "(8 batch) loss: 1.273593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9 batch) loss: 1.248325\n",
      "(10 batch) loss: 1.238480\n",
      "(Epoch 10 / 10) train_acc: 0.491346; val_acc: 0.220000\n",
      "600\n",
      "(0 batch) loss: 1.483455\n",
      "(1 batch) loss: 1.495191\n",
      "(2 batch) loss: 1.396878\n",
      "(3 batch) loss: 1.417285\n",
      "(4 batch) loss: 1.431269\n",
      "(5 batch) loss: 1.449084\n",
      "(6 batch) loss: 1.494978\n",
      "(7 batch) loss: 1.417976\n",
      "(8 batch) loss: 1.441932\n",
      "(9 batch) loss: 1.457273\n",
      "(10 batch) loss: 1.455358\n",
      "(Epoch 1 / 10) train_acc: 0.304327; val_acc: 0.290000\n",
      "(0 batch) loss: 1.382115\n",
      "(1 batch) loss: 1.400924\n",
      "(2 batch) loss: 1.407330\n",
      "(3 batch) loss: 1.420653\n",
      "(4 batch) loss: 1.383246\n",
      "(5 batch) loss: 1.366730\n",
      "(6 batch) loss: 1.383226\n",
      "(7 batch) loss: 1.384938\n",
      "(8 batch) loss: 1.378254\n",
      "(9 batch) loss: 1.374792\n",
      "(10 batch) loss: 1.390704\n",
      "(Epoch 2 / 10) train_acc: 0.334615; val_acc: 0.250000\n",
      "(0 batch) loss: 1.372271\n",
      "(1 batch) loss: 1.353117\n",
      "(2 batch) loss: 1.353190\n",
      "(3 batch) loss: 1.350581\n",
      "(4 batch) loss: 1.318723\n",
      "(5 batch) loss: 1.343371\n",
      "(6 batch) loss: 1.339883\n",
      "(7 batch) loss: 1.341238\n",
      "(8 batch) loss: 1.353984\n",
      "(9 batch) loss: 1.345272\n",
      "(10 batch) loss: 1.403251\n",
      "(Epoch 3 / 10) train_acc: 0.361538; val_acc: 0.240000\n",
      "(0 batch) loss: 1.307958\n",
      "(1 batch) loss: 1.339006\n",
      "(2 batch) loss: 1.336568\n",
      "(3 batch) loss: 1.308781\n",
      "(4 batch) loss: 1.369311\n",
      "(5 batch) loss: 1.350070\n",
      "(6 batch) loss: 1.293774\n",
      "(7 batch) loss: 1.344504\n",
      "(8 batch) loss: 1.302900\n",
      "(9 batch) loss: 1.323992\n",
      "(10 batch) loss: 1.257928\n",
      "(Epoch 4 / 10) train_acc: 0.398558; val_acc: 0.240000\n",
      "(0 batch) loss: 1.301210\n",
      "(1 batch) loss: 1.310931\n",
      "(2 batch) loss: 1.323467\n",
      "(3 batch) loss: 1.325282\n",
      "(4 batch) loss: 1.293200\n",
      "(5 batch) loss: 1.287498\n",
      "(6 batch) loss: 1.285616\n",
      "(7 batch) loss: 1.297195\n",
      "(8 batch) loss: 1.341990\n",
      "(9 batch) loss: 1.291818\n",
      "(10 batch) loss: 1.295864\n",
      "(Epoch 5 / 10) train_acc: 0.409135; val_acc: 0.300000\n",
      "(0 batch) loss: 1.285474\n",
      "(1 batch) loss: 1.312100\n",
      "(2 batch) loss: 1.294906\n",
      "(3 batch) loss: 1.247388\n",
      "(4 batch) loss: 1.289460\n",
      "(5 batch) loss: 1.312206\n",
      "(6 batch) loss: 1.293661\n",
      "(7 batch) loss: 1.274449\n",
      "(8 batch) loss: 1.295899\n",
      "(9 batch) loss: 1.286690\n",
      "(10 batch) loss: 1.252389\n",
      "(Epoch 6 / 10) train_acc: 0.434615; val_acc: 0.230000\n",
      "(0 batch) loss: 1.276431\n",
      "(1 batch) loss: 1.287422\n",
      "(2 batch) loss: 1.238229\n",
      "(3 batch) loss: 1.260409\n",
      "(4 batch) loss: 1.239069\n",
      "(5 batch) loss: 1.266471\n",
      "(6 batch) loss: 1.277470\n",
      "(7 batch) loss: 1.284047\n",
      "(8 batch) loss: 1.268934\n",
      "(9 batch) loss: 1.261114\n",
      "(10 batch) loss: 1.316010\n",
      "(Epoch 7 / 10) train_acc: 0.445192; val_acc: 0.230000\n",
      "(0 batch) loss: 1.261296\n",
      "(1 batch) loss: 1.267652\n",
      "(2 batch) loss: 1.215920\n",
      "(3 batch) loss: 1.252560\n",
      "(4 batch) loss: 1.269080\n",
      "(5 batch) loss: 1.252815\n",
      "(6 batch) loss: 1.232129\n",
      "(7 batch) loss: 1.253987\n",
      "(8 batch) loss: 1.289727\n",
      "(9 batch) loss: 1.235188\n",
      "(10 batch) loss: 1.253765\n",
      "(Epoch 8 / 10) train_acc: 0.459615; val_acc: 0.280000\n",
      "(0 batch) loss: 1.225463\n",
      "(1 batch) loss: 1.213187\n",
      "(2 batch) loss: 1.203629\n",
      "(3 batch) loss: 1.283278\n",
      "(4 batch) loss: 1.225394\n",
      "(5 batch) loss: 1.244561\n",
      "(6 batch) loss: 1.241962\n",
      "(7 batch) loss: 1.210910\n",
      "(8 batch) loss: 1.249035\n",
      "(9 batch) loss: 1.310703\n",
      "(10 batch) loss: 1.253910\n",
      "(Epoch 9 / 10) train_acc: 0.472115; val_acc: 0.270000\n",
      "(0 batch) loss: 1.193103\n",
      "(1 batch) loss: 1.261295\n",
      "(2 batch) loss: 1.202098\n",
      "(3 batch) loss: 1.184394\n",
      "(4 batch) loss: 1.192390\n",
      "(5 batch) loss: 1.254424\n",
      "(6 batch) loss: 1.211314\n",
      "(7 batch) loss: 1.238298\n",
      "(8 batch) loss: 1.220277\n",
      "(9 batch) loss: 1.225828\n",
      "(10 batch) loss: 1.140724\n",
      "(Epoch 10 / 10) train_acc: 0.486058; val_acc: 0.290000\n",
      "700\n",
      "(0 batch) loss: 1.424201\n",
      "(1 batch) loss: 1.418108\n",
      "(2 batch) loss: 1.426461\n",
      "(3 batch) loss: 1.489726\n",
      "(4 batch) loss: 1.443493\n",
      "(5 batch) loss: 1.466131\n",
      "(6 batch) loss: 1.470941\n",
      "(7 batch) loss: 1.449052\n",
      "(8 batch) loss: 1.463025\n",
      "(9 batch) loss: 1.461562\n",
      "(10 batch) loss: 1.406352\n",
      "(Epoch 1 / 10) train_acc: 0.295673; val_acc: 0.330000\n",
      "(0 batch) loss: 1.401320\n",
      "(1 batch) loss: 1.397323\n",
      "(2 batch) loss: 1.358122\n",
      "(3 batch) loss: 1.415792\n",
      "(4 batch) loss: 1.372382\n",
      "(5 batch) loss: 1.432582\n",
      "(6 batch) loss: 1.411068\n",
      "(7 batch) loss: 1.427021\n",
      "(8 batch) loss: 1.353846\n",
      "(9 batch) loss: 1.394767\n",
      "(10 batch) loss: 1.438312\n",
      "(Epoch 2 / 10) train_acc: 0.320673; val_acc: 0.330000\n",
      "(0 batch) loss: 1.372368\n",
      "(1 batch) loss: 1.364865\n",
      "(2 batch) loss: 1.375706\n",
      "(3 batch) loss: 1.350603\n",
      "(4 batch) loss: 1.328529\n",
      "(5 batch) loss: 1.340337\n",
      "(6 batch) loss: 1.357961\n",
      "(7 batch) loss: 1.353310\n",
      "(8 batch) loss: 1.316977\n",
      "(9 batch) loss: 1.394359\n",
      "(10 batch) loss: 1.343600\n",
      "(Epoch 3 / 10) train_acc: 0.351442; val_acc: 0.330000\n",
      "(0 batch) loss: 1.335085\n",
      "(1 batch) loss: 1.325212\n",
      "(2 batch) loss: 1.344144\n",
      "(3 batch) loss: 1.311999\n",
      "(4 batch) loss: 1.325463\n",
      "(5 batch) loss: 1.317350\n",
      "(6 batch) loss: 1.367956\n",
      "(7 batch) loss: 1.328915\n",
      "(8 batch) loss: 1.336784\n",
      "(9 batch) loss: 1.328798\n",
      "(10 batch) loss: 1.283342\n",
      "(Epoch 4 / 10) train_acc: 0.398077; val_acc: 0.310000\n",
      "(0 batch) loss: 1.319564\n",
      "(1 batch) loss: 1.278100\n",
      "(2 batch) loss: 1.332472\n",
      "(3 batch) loss: 1.304838\n",
      "(4 batch) loss: 1.311709\n",
      "(5 batch) loss: 1.301163\n",
      "(6 batch) loss: 1.324178\n",
      "(7 batch) loss: 1.302919\n",
      "(8 batch) loss: 1.319790\n",
      "(9 batch) loss: 1.294598\n",
      "(10 batch) loss: 1.318325\n",
      "(Epoch 5 / 10) train_acc: 0.413462; val_acc: 0.350000\n",
      "(0 batch) loss: 1.289237\n",
      "(1 batch) loss: 1.308041\n",
      "(2 batch) loss: 1.315939\n",
      "(3 batch) loss: 1.290625\n",
      "(4 batch) loss: 1.271285\n",
      "(5 batch) loss: 1.280840\n",
      "(6 batch) loss: 1.240466\n",
      "(7 batch) loss: 1.302217\n",
      "(8 batch) loss: 1.326918\n",
      "(9 batch) loss: 1.303061\n",
      "(10 batch) loss: 1.262217\n",
      "(Epoch 6 / 10) train_acc: 0.421154; val_acc: 0.310000\n",
      "(0 batch) loss: 1.254446\n",
      "(1 batch) loss: 1.252459\n",
      "(2 batch) loss: 1.274527\n",
      "(3 batch) loss: 1.309233\n",
      "(4 batch) loss: 1.283791\n",
      "(5 batch) loss: 1.277468\n",
      "(6 batch) loss: 1.277550\n",
      "(7 batch) loss: 1.258161\n",
      "(8 batch) loss: 1.271572\n",
      "(9 batch) loss: 1.295439\n",
      "(10 batch) loss: 1.315768\n",
      "(Epoch 7 / 10) train_acc: 0.437500; val_acc: 0.360000\n",
      "(0 batch) loss: 1.235500\n",
      "(1 batch) loss: 1.234306\n",
      "(2 batch) loss: 1.287572\n",
      "(3 batch) loss: 1.253011\n",
      "(4 batch) loss: 1.281216\n",
      "(5 batch) loss: 1.257621\n",
      "(6 batch) loss: 1.278162\n",
      "(7 batch) loss: 1.218934\n",
      "(8 batch) loss: 1.231294\n",
      "(9 batch) loss: 1.279424\n",
      "(10 batch) loss: 1.239887\n",
      "(Epoch 8 / 10) train_acc: 0.446154; val_acc: 0.350000\n",
      "(0 batch) loss: 1.242530\n",
      "(1 batch) loss: 1.192744\n",
      "(2 batch) loss: 1.233476\n",
      "(3 batch) loss: 1.229506\n",
      "(4 batch) loss: 1.238462\n",
      "(5 batch) loss: 1.233225\n",
      "(6 batch) loss: 1.243045\n",
      "(7 batch) loss: 1.244996\n",
      "(8 batch) loss: 1.268230\n",
      "(9 batch) loss: 1.241444\n",
      "(10 batch) loss: 1.186701\n",
      "(Epoch 9 / 10) train_acc: 0.480769; val_acc: 0.380000\n",
      "(0 batch) loss: 1.202044\n",
      "(1 batch) loss: 1.229953\n",
      "(2 batch) loss: 1.223375\n",
      "(3 batch) loss: 1.211506\n",
      "(4 batch) loss: 1.233238\n",
      "(5 batch) loss: 1.237704\n",
      "(6 batch) loss: 1.200317\n",
      "(7 batch) loss: 1.238939\n",
      "(8 batch) loss: 1.201897\n",
      "(9 batch) loss: 1.217702\n",
      "(10 batch) loss: 1.208863\n",
      "(Epoch 10 / 10) train_acc: 0.484615; val_acc: 0.360000\n",
      "800\n",
      "(0 batch) loss: 1.444094\n",
      "(1 batch) loss: 1.464584\n",
      "(2 batch) loss: 1.450481\n",
      "(3 batch) loss: 1.447715\n",
      "(4 batch) loss: 1.433163\n",
      "(5 batch) loss: 1.459704\n",
      "(6 batch) loss: 1.398300\n",
      "(7 batch) loss: 1.463099\n",
      "(8 batch) loss: 1.438896\n",
      "(9 batch) loss: 1.470074\n",
      "(10 batch) loss: 1.421247\n",
      "(Epoch 1 / 10) train_acc: 0.309135; val_acc: 0.230000\n",
      "(0 batch) loss: 1.395727\n",
      "(1 batch) loss: 1.419751\n",
      "(2 batch) loss: 1.346553\n",
      "(3 batch) loss: 1.360974\n",
      "(4 batch) loss: 1.366380\n",
      "(5 batch) loss: 1.386019\n",
      "(6 batch) loss: 1.367855\n",
      "(7 batch) loss: 1.400723\n",
      "(8 batch) loss: 1.414248\n",
      "(9 batch) loss: 1.429288\n",
      "(10 batch) loss: 1.345501\n",
      "(Epoch 2 / 10) train_acc: 0.337019; val_acc: 0.260000\n",
      "(0 batch) loss: 1.341648\n",
      "(1 batch) loss: 1.317951\n",
      "(2 batch) loss: 1.413509\n",
      "(3 batch) loss: 1.317404\n",
      "(4 batch) loss: 1.322463\n",
      "(5 batch) loss: 1.349827\n",
      "(6 batch) loss: 1.354873\n",
      "(7 batch) loss: 1.314911\n",
      "(8 batch) loss: 1.370896\n",
      "(9 batch) loss: 1.351111\n",
      "(10 batch) loss: 1.293274\n",
      "(Epoch 3 / 10) train_acc: 0.366827; val_acc: 0.280000\n",
      "(0 batch) loss: 1.338958\n",
      "(1 batch) loss: 1.286914\n",
      "(2 batch) loss: 1.302521\n",
      "(3 batch) loss: 1.346571\n",
      "(4 batch) loss: 1.308466\n",
      "(5 batch) loss: 1.326099\n",
      "(6 batch) loss: 1.326511\n",
      "(7 batch) loss: 1.279149\n",
      "(8 batch) loss: 1.301227\n",
      "(9 batch) loss: 1.315202\n",
      "(10 batch) loss: 1.353299\n",
      "(Epoch 4 / 10) train_acc: 0.397115; val_acc: 0.340000\n",
      "(0 batch) loss: 1.322900\n",
      "(1 batch) loss: 1.288726\n",
      "(2 batch) loss: 1.272041\n",
      "(3 batch) loss: 1.280777\n",
      "(4 batch) loss: 1.264817\n",
      "(5 batch) loss: 1.314742\n",
      "(6 batch) loss: 1.319632\n",
      "(7 batch) loss: 1.315473\n",
      "(8 batch) loss: 1.213278\n",
      "(9 batch) loss: 1.308897\n",
      "(10 batch) loss: 1.303832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5 / 10) train_acc: 0.427404; val_acc: 0.250000\n",
      "(0 batch) loss: 1.306969\n",
      "(1 batch) loss: 1.299913\n",
      "(2 batch) loss: 1.275871\n",
      "(3 batch) loss: 1.270405\n",
      "(4 batch) loss: 1.273315\n",
      "(5 batch) loss: 1.238706\n",
      "(6 batch) loss: 1.221727\n",
      "(7 batch) loss: 1.264942\n",
      "(8 batch) loss: 1.273756\n",
      "(9 batch) loss: 1.265769\n",
      "(10 batch) loss: 1.245594\n",
      "(Epoch 6 / 10) train_acc: 0.440865; val_acc: 0.300000\n",
      "(0 batch) loss: 1.267889\n",
      "(1 batch) loss: 1.235580\n",
      "(2 batch) loss: 1.255670\n",
      "(3 batch) loss: 1.266433\n",
      "(4 batch) loss: 1.236531\n",
      "(5 batch) loss: 1.290018\n",
      "(6 batch) loss: 1.278331\n",
      "(7 batch) loss: 1.206742\n",
      "(8 batch) loss: 1.238549\n",
      "(9 batch) loss: 1.209570\n",
      "(10 batch) loss: 1.229704\n",
      "(Epoch 7 / 10) train_acc: 0.462500; val_acc: 0.260000\n",
      "(0 batch) loss: 1.257148\n",
      "(1 batch) loss: 1.245164\n",
      "(2 batch) loss: 1.186920\n",
      "(3 batch) loss: 1.198686\n",
      "(4 batch) loss: 1.276162\n",
      "(5 batch) loss: 1.234405\n",
      "(6 batch) loss: 1.252145\n",
      "(7 batch) loss: 1.260257\n",
      "(8 batch) loss: 1.192451\n",
      "(9 batch) loss: 1.214607\n",
      "(10 batch) loss: 1.237400\n",
      "(Epoch 8 / 10) train_acc: 0.486058; val_acc: 0.250000\n",
      "(0 batch) loss: 1.218572\n",
      "(1 batch) loss: 1.239779\n",
      "(2 batch) loss: 1.214824\n",
      "(3 batch) loss: 1.142653\n",
      "(4 batch) loss: 1.213053\n",
      "(5 batch) loss: 1.233950\n",
      "(6 batch) loss: 1.149827\n",
      "(7 batch) loss: 1.224229\n",
      "(8 batch) loss: 1.213703\n",
      "(9 batch) loss: 1.219707\n",
      "(10 batch) loss: 1.226589\n",
      "(Epoch 9 / 10) train_acc: 0.505288; val_acc: 0.270000\n",
      "(0 batch) loss: 1.172805\n",
      "(1 batch) loss: 1.182671\n",
      "(2 batch) loss: 1.171871\n",
      "(3 batch) loss: 1.173170\n",
      "(4 batch) loss: 1.203093\n",
      "(5 batch) loss: 1.182195\n",
      "(6 batch) loss: 1.177946\n",
      "(7 batch) loss: 1.184976\n",
      "(8 batch) loss: 1.220926\n",
      "(9 batch) loss: 1.193964\n",
      "(10 batch) loss: 1.250674\n",
      "(Epoch 10 / 10) train_acc: 0.506731; val_acc: 0.260000\n",
      "900\n",
      "(0 batch) loss: 1.460344\n",
      "(1 batch) loss: 1.465278\n",
      "(2 batch) loss: 1.438472\n",
      "(3 batch) loss: 1.430387\n",
      "(4 batch) loss: 1.451077\n",
      "(5 batch) loss: 1.492956\n",
      "(6 batch) loss: 1.467771\n",
      "(7 batch) loss: 1.437178\n",
      "(8 batch) loss: 1.444514\n",
      "(9 batch) loss: 1.455377\n",
      "(10 batch) loss: 1.421841\n",
      "(Epoch 1 / 10) train_acc: 0.307692; val_acc: 0.290000\n",
      "(0 batch) loss: 1.402470\n",
      "(1 batch) loss: 1.413931\n",
      "(2 batch) loss: 1.380888\n",
      "(3 batch) loss: 1.378291\n",
      "(4 batch) loss: 1.385586\n",
      "(5 batch) loss: 1.370281\n",
      "(6 batch) loss: 1.388112\n",
      "(7 batch) loss: 1.379356\n",
      "(8 batch) loss: 1.362487\n",
      "(9 batch) loss: 1.352557\n",
      "(10 batch) loss: 1.415257\n",
      "(Epoch 2 / 10) train_acc: 0.334615; val_acc: 0.320000\n",
      "(0 batch) loss: 1.308904\n",
      "(1 batch) loss: 1.318364\n",
      "(2 batch) loss: 1.342178\n",
      "(3 batch) loss: 1.331997\n",
      "(4 batch) loss: 1.306056\n",
      "(5 batch) loss: 1.339108\n",
      "(6 batch) loss: 1.395539\n",
      "(7 batch) loss: 1.324523\n",
      "(8 batch) loss: 1.360523\n",
      "(9 batch) loss: 1.318551\n",
      "(10 batch) loss: 1.364751\n",
      "(Epoch 3 / 10) train_acc: 0.364423; val_acc: 0.320000\n",
      "(0 batch) loss: 1.282256\n",
      "(1 batch) loss: 1.307569\n",
      "(2 batch) loss: 1.369164\n",
      "(3 batch) loss: 1.328222\n",
      "(4 batch) loss: 1.279173\n",
      "(5 batch) loss: 1.300496\n",
      "(6 batch) loss: 1.327086\n",
      "(7 batch) loss: 1.297229\n",
      "(8 batch) loss: 1.277088\n",
      "(9 batch) loss: 1.292299\n",
      "(10 batch) loss: 1.307516\n",
      "(Epoch 4 / 10) train_acc: 0.399519; val_acc: 0.380000\n",
      "(0 batch) loss: 1.251011\n",
      "(1 batch) loss: 1.319365\n",
      "(2 batch) loss: 1.281355\n",
      "(3 batch) loss: 1.310249\n",
      "(4 batch) loss: 1.276584\n",
      "(5 batch) loss: 1.227836\n",
      "(6 batch) loss: 1.293711\n",
      "(7 batch) loss: 1.298141\n",
      "(8 batch) loss: 1.274737\n",
      "(9 batch) loss: 1.272778\n",
      "(10 batch) loss: 1.279447\n",
      "(Epoch 5 / 10) train_acc: 0.425481; val_acc: 0.330000\n",
      "(0 batch) loss: 1.195829\n",
      "(1 batch) loss: 1.263263\n",
      "(2 batch) loss: 1.256935\n",
      "(3 batch) loss: 1.264104\n",
      "(4 batch) loss: 1.248655\n",
      "(5 batch) loss: 1.279945\n",
      "(6 batch) loss: 1.268442\n",
      "(7 batch) loss: 1.284577\n",
      "(8 batch) loss: 1.306461\n",
      "(9 batch) loss: 1.223343\n",
      "(10 batch) loss: 1.208563\n",
      "(Epoch 6 / 10) train_acc: 0.443269; val_acc: 0.320000\n",
      "(0 batch) loss: 1.281819\n",
      "(1 batch) loss: 1.237286\n",
      "(2 batch) loss: 1.203740\n",
      "(3 batch) loss: 1.260480\n",
      "(4 batch) loss: 1.228110\n",
      "(5 batch) loss: 1.242741\n",
      "(6 batch) loss: 1.210341\n",
      "(7 batch) loss: 1.259171\n",
      "(8 batch) loss: 1.232994\n",
      "(9 batch) loss: 1.223463\n",
      "(10 batch) loss: 1.286476\n",
      "(Epoch 7 / 10) train_acc: 0.468750; val_acc: 0.330000\n",
      "(0 batch) loss: 1.233101\n",
      "(1 batch) loss: 1.160721\n",
      "(2 batch) loss: 1.254010\n",
      "(3 batch) loss: 1.176468\n",
      "(4 batch) loss: 1.192134\n",
      "(5 batch) loss: 1.199320\n",
      "(6 batch) loss: 1.221757\n",
      "(7 batch) loss: 1.247460\n",
      "(8 batch) loss: 1.243284\n",
      "(9 batch) loss: 1.232661\n",
      "(10 batch) loss: 1.251874\n",
      "(Epoch 8 / 10) train_acc: 0.479327; val_acc: 0.340000\n",
      "(0 batch) loss: 1.197759\n",
      "(1 batch) loss: 1.174223\n",
      "(2 batch) loss: 1.242411\n",
      "(3 batch) loss: 1.174601\n",
      "(4 batch) loss: 1.178805\n",
      "(5 batch) loss: 1.208313\n",
      "(6 batch) loss: 1.147842\n",
      "(7 batch) loss: 1.232512\n",
      "(8 batch) loss: 1.216119\n",
      "(9 batch) loss: 1.197569\n",
      "(10 batch) loss: 1.216002\n",
      "(Epoch 9 / 10) train_acc: 0.500962; val_acc: 0.330000\n",
      "(0 batch) loss: 1.174910\n",
      "(1 batch) loss: 1.179206\n",
      "(2 batch) loss: 1.191065\n",
      "(3 batch) loss: 1.199741\n",
      "(4 batch) loss: 1.192736\n",
      "(5 batch) loss: 1.169064\n",
      "(6 batch) loss: 1.135929\n",
      "(7 batch) loss: 1.148021\n",
      "(8 batch) loss: 1.190745\n",
      "(9 batch) loss: 1.187385\n",
      "(10 batch) loss: 1.158326\n",
      "(Epoch 10 / 10) train_acc: 0.504327; val_acc: 0.300000\n",
      "1000\n",
      "(0 batch) loss: 1.402274\n",
      "(1 batch) loss: 1.503000\n",
      "(2 batch) loss: 1.466948\n",
      "(3 batch) loss: 1.486186\n",
      "(4 batch) loss: 1.493525\n",
      "(5 batch) loss: 1.434790\n",
      "(6 batch) loss: 1.450110\n",
      "(7 batch) loss: 1.491174\n",
      "(8 batch) loss: 1.439563\n",
      "(9 batch) loss: 1.456367\n",
      "(10 batch) loss: 1.514978\n",
      "(Epoch 1 / 10) train_acc: 0.309615; val_acc: 0.300000\n",
      "(0 batch) loss: 1.444492\n",
      "(1 batch) loss: 1.419156\n",
      "(2 batch) loss: 1.367512\n",
      "(3 batch) loss: 1.417930\n",
      "(4 batch) loss: 1.439955\n",
      "(5 batch) loss: 1.382829\n",
      "(6 batch) loss: 1.398257\n",
      "(7 batch) loss: 1.383645\n",
      "(8 batch) loss: 1.387880\n",
      "(9 batch) loss: 1.421968\n",
      "(10 batch) loss: 1.325542\n",
      "(Epoch 2 / 10) train_acc: 0.329327; val_acc: 0.270000\n",
      "(0 batch) loss: 1.361018\n",
      "(1 batch) loss: 1.315799\n",
      "(2 batch) loss: 1.345817\n",
      "(3 batch) loss: 1.376955\n",
      "(4 batch) loss: 1.359047\n",
      "(5 batch) loss: 1.385185\n",
      "(6 batch) loss: 1.373545\n",
      "(7 batch) loss: 1.341002\n",
      "(8 batch) loss: 1.329967\n",
      "(9 batch) loss: 1.381587\n",
      "(10 batch) loss: 1.414167\n",
      "(Epoch 3 / 10) train_acc: 0.360096; val_acc: 0.220000\n",
      "(0 batch) loss: 1.353512\n",
      "(1 batch) loss: 1.350014\n",
      "(2 batch) loss: 1.266501\n",
      "(3 batch) loss: 1.321462\n",
      "(4 batch) loss: 1.349509\n",
      "(5 batch) loss: 1.337393\n",
      "(6 batch) loss: 1.315145\n",
      "(7 batch) loss: 1.356043\n",
      "(8 batch) loss: 1.319637\n",
      "(9 batch) loss: 1.330992\n",
      "(10 batch) loss: 1.287314\n",
      "(Epoch 4 / 10) train_acc: 0.383173; val_acc: 0.220000\n",
      "(0 batch) loss: 1.317577\n",
      "(1 batch) loss: 1.266659\n",
      "(2 batch) loss: 1.313673\n",
      "(3 batch) loss: 1.332314\n",
      "(4 batch) loss: 1.245865\n",
      "(5 batch) loss: 1.319266\n",
      "(6 batch) loss: 1.334060\n",
      "(7 batch) loss: 1.338083\n",
      "(8 batch) loss: 1.283185\n",
      "(9 batch) loss: 1.330736\n",
      "(10 batch) loss: 1.313422\n",
      "(Epoch 5 / 10) train_acc: 0.402885; val_acc: 0.210000\n",
      "(0 batch) loss: 1.305253\n",
      "(1 batch) loss: 1.301114\n",
      "(2 batch) loss: 1.264657\n",
      "(3 batch) loss: 1.255909\n",
      "(4 batch) loss: 1.294467\n",
      "(5 batch) loss: 1.283822\n",
      "(6 batch) loss: 1.314032\n",
      "(7 batch) loss: 1.280954\n",
      "(8 batch) loss: 1.285223\n",
      "(9 batch) loss: 1.232896\n",
      "(10 batch) loss: 1.237231\n",
      "(Epoch 6 / 10) train_acc: 0.424519; val_acc: 0.210000\n",
      "(0 batch) loss: 1.251871\n",
      "(1 batch) loss: 1.267156\n",
      "(2 batch) loss: 1.256135\n",
      "(3 batch) loss: 1.244022\n",
      "(4 batch) loss: 1.260599\n",
      "(5 batch) loss: 1.225353\n",
      "(6 batch) loss: 1.273186\n",
      "(7 batch) loss: 1.265522\n",
      "(8 batch) loss: 1.279892\n",
      "(9 batch) loss: 1.273693\n",
      "(10 batch) loss: 1.247193\n",
      "(Epoch 7 / 10) train_acc: 0.428365; val_acc: 0.210000\n",
      "(0 batch) loss: 1.213094\n",
      "(1 batch) loss: 1.235649\n",
      "(2 batch) loss: 1.242915\n",
      "(3 batch) loss: 1.248481\n",
      "(4 batch) loss: 1.221912\n",
      "(5 batch) loss: 1.221458\n",
      "(6 batch) loss: 1.265800\n",
      "(7 batch) loss: 1.244167\n",
      "(8 batch) loss: 1.276658\n",
      "(9 batch) loss: 1.267976\n",
      "(10 batch) loss: 1.242060\n",
      "(Epoch 8 / 10) train_acc: 0.438942; val_acc: 0.240000\n",
      "(0 batch) loss: 1.226319\n",
      "(1 batch) loss: 1.200726\n",
      "(2 batch) loss: 1.234935\n",
      "(3 batch) loss: 1.219925\n",
      "(4 batch) loss: 1.287325\n",
      "(5 batch) loss: 1.215220\n",
      "(6 batch) loss: 1.267483\n",
      "(7 batch) loss: 1.265593\n",
      "(8 batch) loss: 1.214316\n",
      "(9 batch) loss: 1.246701\n",
      "(10 batch) loss: 1.304764\n",
      "(Epoch 9 / 10) train_acc: 0.451442; val_acc: 0.240000\n",
      "(0 batch) loss: 1.235360\n",
      "(1 batch) loss: 1.217939\n",
      "(2 batch) loss: 1.193756\n",
      "(3 batch) loss: 1.166772\n",
      "(4 batch) loss: 1.239652\n",
      "(5 batch) loss: 1.171162\n",
      "(6 batch) loss: 1.206035\n",
      "(7 batch) loss: 1.232557\n",
      "(8 batch) loss: 1.221101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9 batch) loss: 1.222080\n",
      "(10 batch) loss: 1.237725\n",
      "(Epoch 10 / 10) train_acc: 0.479327; val_acc: 0.280000\n"
     ]
    }
   ],
   "source": [
    "acc_t = []\n",
    "for t in range(10):\n",
    "    print ((t+1)*100)\n",
    "    bs_train = 200\n",
    "    bs_val = 100\n",
    "    bs_test = 100\n",
    "    data = data_utils.TensorDataset(torch.Tensor(X[:,:,0:((t+1)*100)]), torch.Tensor(y))\n",
    "    dset = {}\n",
    "    dataloaders = {}\n",
    "    dset['train'], dset['val'], dset['test'] = random_split(data, [N-bs_val-bs_test, bs_val, bs_test])\n",
    "    dataloaders['train'] = data_utils.DataLoader(dset['train'], batch_size=bs_train, shuffle=True, num_workers=1)\n",
    "    dataloaders['val'] = data_utils.DataLoader(dset['val'], batch_size=bs_val, shuffle=True, num_workers=1)\n",
    "    dataloaders['test'] = data_utils.DataLoader(dset['test'], batch_size=bs_test, shuffle=True, num_workers=1)\n",
    "    best_acc = 0\n",
    "    for epoch in range(num_epoches):\n",
    "        for i, data in enumerate(dataloaders['train'], 0):\n",
    "            X_train, y_train = data\n",
    "            # Wrap them in Variable\n",
    "            X_train, y_train = Variable(X_train), Variable(y_train)\n",
    "            # forward + backward + optimize\n",
    "            out = model(X_train.cuda())\n",
    "            # print (out)\n",
    "            loss = loss_fn(out, y_train.long().cuda())\n",
    "            print('(%d batch) loss: %f' % (i, loss))\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_acc = model.check_accuracy(dataloaders['train'])\n",
    "        val_acc = model.check_accuracy(dataloaders['val'])\n",
    "        print('(Epoch %d / %d) train_acc: %f; val_acc: %f' % (epoch+1, num_epoches, train_acc, val_acc))\n",
    "        if (val_acc > best_acc):\n",
    "            best_acc = val_acc\n",
    "            #torch.save(model, 'best_CHRONET.pt')\n",
    "    acc_t.append(best_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot acc vs time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.figure(1)\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('validation accuracy')\n",
    "plt.title('LSTM: Accuracy vs Time')\n",
    "t = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n",
    "plt.plot(t, acc_t)\n",
    "plt.show()\n",
    "plt.savefig('t_LSTM.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = torch.load('best_CHRONET.pt')\n",
    "print (best_model.check_accuracy(dataloaders['val']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
