{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data.dataset import random_split\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/markdown"
   },
   "source": [
    "# Helper Clases / Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_Data(num):\n",
    "    if (num == -1): # All data\n",
    "        X_all = []\n",
    "        y_all = []\n",
    "        for i in range(8):\n",
    "            file_path = './../project_datasets/A0' + str(i+1) + 'T_slice.mat'\n",
    "            data = h5py.File(file_path, 'r')\n",
    "            X = np.copy(data['image'])\n",
    "            y = np.copy(data['type'])\n",
    "            X = X[:, 0:23, :]\n",
    "            X_all.append(X)\n",
    "            y = y[0,0:X.shape[0]:1]\n",
    "            y_all.append(y)\n",
    "        A, N, E, T = np.shape(X_all)\n",
    "        X_all = np.reshape(X_all, (A*N, E, T))\n",
    "        y_all = np.reshape(y_all, (-1))\n",
    "        y_all = y_all - 769\n",
    "        ## Remove NAN\n",
    "        index_Nan = []\n",
    "        for i in range(A*N):\n",
    "            for j in range(E):\n",
    "                if (any(np.isnan(X_all[i,j])) == True):\n",
    "                    index_Nan.append(i)\n",
    "        index_Nan = list(set(index_Nan))\n",
    "        X_all = np.delete(X_all, index_Nan, axis=0)\n",
    "        y_all = np.delete(y_all, index_Nan)\n",
    "        return (X_all, y_all)\n",
    "    else:\n",
    "        file_path = './../project_datasets/A0' + str(num) + 'T_slice.mat'\n",
    "        data = h5py.File(file_path, 'r')\n",
    "        X = np.copy(data['image'])\n",
    "        y = np.copy(data['type'])\n",
    "        X = X[:, 0:23, :]\n",
    "        y = y[0,0:X.shape[0]:1]\n",
    "        y = y - 769\n",
    "         ## Remove NAN\n",
    "        N, E, T = np.shape(X)\n",
    "        index_Nan = []\n",
    "        for i in range(N):\n",
    "            for j in range(E):\n",
    "                if (any(np.isnan(X[i,j])) == True):\n",
    "                    index_Nan.append(i)\n",
    "        index_Nan = list(set(index_Nan))\n",
    "        X = np.delete(X, index_Nan, axis=0)\n",
    "        y = np.delete(y, index_Nan)\n",
    "        return (X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2280, 23, 1000)\n"
     ]
    }
   ],
   "source": [
    "X, y = Load_Data(-1) # -1 to load all datas\n",
    "N, E, T = np.shape(X)\n",
    "print (np.shape(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_train = 200\n",
    "bs_val = 100\n",
    "bs_test = 100\n",
    "data = data_utils.TensorDataset(torch.Tensor(X), torch.Tensor(y))\n",
    "dset = {}\n",
    "dataloaders = {}\n",
    "dset['train'], dset['val'], dset['test'] = random_split(data, [N-bs_val-bs_test, bs_val, bs_test])\n",
    "dataloaders['train'] = data_utils.DataLoader(dset['train'], batch_size=bs_train, shuffle=True, num_workers=1)\n",
    "dataloaders['val'] = data_utils.DataLoader(dset['val'], batch_size=bs_val, shuffle=True, num_workers=1)\n",
    "dataloaders['test'] = data_utils.DataLoader(dset['test'], batch_size=bs_test, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myChronoNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layer, num_class):\n",
    "        super(myChronoNet, self).__init__()\n",
    "        self.num_layer = num_layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # First Inception layer\n",
    "        self.conv11 = nn.Conv1d(23, 32, 2, stride=2)\n",
    "        self.conv12 = nn.Conv1d(23, 32, 4, stride=2, padding=1)\n",
    "        self.conv13 = nn.Conv1d(23, 32, 8, stride=2, padding=3)\n",
    "        # Second Inception layer\n",
    "        self.conv21 = nn.Conv1d(96, 32, 2, stride=2)\n",
    "        self.conv22 = nn.Conv1d(96, 32, 4, stride=2, padding=1)\n",
    "        self.conv23 = nn.Conv1d(96, 32, 8, stride=2, padding=3)\n",
    "        # Third Inception layer\n",
    "        self.conv31 = nn.Conv1d(96, 32, 2, stride=2)\n",
    "        self.conv32 = nn.Conv1d(96, 32, 4, stride=2, padding=1)\n",
    "        self.conv33 = nn.Conv1d(96, 32, 8, stride=2, padding=3)\n",
    "        #self.conv_13 = nn.Conv2d()\n",
    "        self.conv_elec = nn.Conv3d(1,23,tuple([40, 23, 1]))\n",
    "        self.gru1 = nn.GRU(32*3, hidden_dim, num_layer, dropout=0.5)\n",
    "        self.gru2 = nn.GRU(hidden_dim, hidden_dim, num_layer, dropout=0.5)\n",
    "        self.gru3 = nn.GRU(hidden_dim*2, hidden_dim, num_layer, dropout=0.5)\n",
    "        self.gru4 = nn.GRU(hidden_dim*3, hidden_dim, num_layer, dropout=0.5)\n",
    "        self.classifier = nn.Linear(hidden_dim, num_class)\n",
    "    def forward(self, x):\n",
    "        out_conv11 = self.conv11(x)\n",
    "        out_conv12 = self.conv12(x)\n",
    "        out_conv13 = self.conv13(x)\n",
    "        out_conv1 = torch.cat((out_conv11, out_conv12, out_conv13), 1)\n",
    "        out_conv21 = self.conv21(out_conv1)\n",
    "        out_conv22 = self.conv22(out_conv1)\n",
    "        out_conv23 = self.conv23(out_conv1)\n",
    "        out_conv2 = torch.cat((out_conv21, out_conv22, out_conv23), 1)\n",
    "        out_conv31 = self.conv31(out_conv2)\n",
    "        out_conv32 = self.conv32(out_conv2)\n",
    "        out_conv33 = self.conv33(out_conv2)\n",
    "        out_conv3 = torch.cat((out_conv31, out_conv32, out_conv33), 1)\n",
    "        # N, C, L --> L, N, C\n",
    "        out_conv3 = out_conv3.permute(2,0,1)\n",
    "        out_gru1, _ = self.gru1(out_conv3)\n",
    "        out_gru2, _ = self.gru2(out_gru1)\n",
    "        out_gru12 = torch.cat((out_gru1, out_gru2), 2)\n",
    "        out_gru3, _ = self.gru3(out_gru12)\n",
    "        out_gru321 = torch.cat((out_gru1, out_gru2, out_gru3), 2)\n",
    "        out_gru4, _ = self.gru4(out_gru321)\n",
    "        out_gru4 = out_gru4[-1, :, :] # taking the last time seq\n",
    "        out = self.classifier(out_gru4)\n",
    "        return out\n",
    "    def check_accuracy(self, dataloader):\n",
    "        total_correct = 0\n",
    "        total_label = 0\n",
    "        for i_batch, sample_batched in enumerate(dataloader):\n",
    "            X_sample, y_sample = sample_batched\n",
    "            X_sample, y_sample = Variable(X_sample), Variable(y_sample)\n",
    "            out = self.forward(X_sample.cuda())\n",
    "            _, pred = torch.max(out, 1)\n",
    "            num_correct = np.sum(pred.data.cpu().numpy() == y_sample.data.cpu().numpy())\n",
    "            total_correct += num_correct\n",
    "            total_label += len(pred)\n",
    "        return  total_correct / total_label\n",
    "\n",
    "class myGRU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layer, num_class):\n",
    "        super(myGRU, self).__init__()\n",
    "        self.num_layer = num_layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm1 = nn.GRU(input_dim, hidden_dim, num_layer, batch_first=True)\n",
    "        self.classifier = nn.Linear(hidden_dim, num_class)\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1,2)\n",
    "        out_gru1, _ = self.gru1(x)\n",
    "        out_gru1 = out_gru1[:,-1,:]\n",
    "        out_lin = self.classifier1(out_gru1)\n",
    "        out = self.classifier(out_lin)\n",
    "        return out\n",
    "    def check_accuracy(self, dataloader):\n",
    "        total_correct = 0\n",
    "        total_label = 0\n",
    "        for i_batch, sample_batched in enumerate(dataloader):\n",
    "            X_sample, y_sample = sample_batched\n",
    "            # Flip axis first\n",
    "            X_sample, y_sample = Variable(X_sample), Variable(y_sample)\n",
    "            out = self.forward(X_sample.cuda())\n",
    "            _, pred = torch.max(out, 1)\n",
    "            num_correct = np.sum(pred.data.cpu().numpy() == y_sample.data.cpu().numpy())\n",
    "            total_correct += num_correct\n",
    "            total_label += len(pred)\n",
    "        return  total_correct / total_label\n",
    "class myCONVGRU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layer, num_class):\n",
    "        super(myGRU, self).__init__()\n",
    "        self.num_layer = num_layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.conv_temp = nn.Conv2d(1,40,tuple([1,25]))\n",
    "        self.conv_elec = nn.Conv3d(1,23,tuple([40, 23, 1]))\n",
    "        self.gru1 = nn.GRU(input_dim, hidden_dim, num_layer)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.gru2 = nn.GRU(hidden_dim, hidden_dim, num_layer)\n",
    "        self.classifier = nn.Linear(hidden_dim, num_class)\n",
    "    def forward(self, x):\n",
    "        x = x.permute(2,0,1)\n",
    "        out_gru1, _ = self.gru1(x)\n",
    "        out_act1 = self.act1(out_gru1)\n",
    "        out_gru2, _ = self.gru2(out_act1)\n",
    "        out_gru2 = out_gru2[-1, :, :] # taking the last time seq\n",
    "        out = self.classifier(out_gru2)\n",
    "        return out\n",
    "    def check_accuracy(self, dataloader):\n",
    "        total_correct = 0\n",
    "        total_label = 0\n",
    "        for i_batch, sample_batched in enumerate(dataloader):\n",
    "            X_sample, y_sample = sample_batched\n",
    "            X_sample, y_sample = Variable(X_sample), Variable(y_sample)\n",
    "            out = self.forward(X_sample.cuda())\n",
    "            _, pred = torch.max(out, 1)\n",
    "            num_correct = np.sum(pred.data.cpu().numpy() == y_sample.data.cpu().numpy())\n",
    "            total_correct += num_correct\n",
    "            total_label += len(pred)\n",
    "        return  total_correct / total_label\n",
    "class myLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layer, num_class):\n",
    "        super(myLSTM, self).__init__()\n",
    "        self.num_layer = num_layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm1 = nn.LSTM(input_dim, hidden_dim, num_layer, batch_first=True)\n",
    "        self.classifier = nn.Linear(hidden_dim, num_class)\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1,2)\n",
    "        out_lstm1, _ = self.lstm1(x)\n",
    "        out_lstm1 = out_lstm1[:,-1,:]\n",
    "        out_lin = self.classifier1(out_lstm1)\n",
    "        out = self.classifier(out_lin)\n",
    "        return out\n",
    "    def check_accuracy(self, dataloader):\n",
    "        total_correct = 0\n",
    "        total_label = 0\n",
    "        for i_batch, sample_batched in enumerate(dataloader):\n",
    "            X_sample, y_sample = sample_batched\n",
    "            # Flip axis first\n",
    "            X_sample, y_sample = Variable(X_sample), Variable(y_sample)\n",
    "            out = self.forward(X_sample.cuda())\n",
    "            _, pred = torch.max(out, 1)\n",
    "            num_correct = np.sum(pred.data.cpu().numpy() == y_sample.data.cpu().numpy())\n",
    "            total_correct += num_correct\n",
    "            total_label += len(pred)\n",
    "        return  total_correct / total_label\n",
    "    \n",
    "class myLSTMDO(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layer, num_class):\n",
    "        super(myLSTMDO, self).__init__()\n",
    "        self.num_layer = num_layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm1 = nn.LSTM(input_dim, hidden_dim, num_layer, batch_first=True, dropout=0.5)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim, num_layer, batch_first=True, dropout=0.5)\n",
    "        self.lstm3 = nn.LSTM(hidden_dim, hidden_dim, num_layer, batch_first=True, dropout=0.5)\n",
    "        self.classifier = nn.Linear(hidden_dim, num_class)\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1,2)\n",
    "        out_lstm1, _ = self.lstm1(x)\n",
    "        out_lstm2, _ = self.lstm2(out_lstm1)\n",
    "        out_lstm3, _ = self.lstm3(out_lstm2)\n",
    "        out_lstm3 = out_lstm3[:,-1,:]\n",
    "        out_lin = self.classifier(out_lstm3)\n",
    "        out = self.classifier2(out_lin)\n",
    "        return out\n",
    "    def check_accuracy(self, dataloader):\n",
    "        total_correct = 0\n",
    "        total_label = 0\n",
    "        for i_batch, sample_batched in enumerate(dataloader):\n",
    "            X_sample, y_sample = sample_batched\n",
    "            # Flip axis first\n",
    "            X_sample, y_sample = Variable(X_sample), Variable(y_sample)\n",
    "            out = self.forward(X_sample.cuda())\n",
    "            _, pred = torch.max(out, 1)\n",
    "            num_correct = np.sum(pred.data.cpu().numpy() == y_sample.data.cpu().numpy())\n",
    "            total_correct += num_correct\n",
    "            total_label += len(pred)\n",
    "        return  total_correct / total_label\n",
    "\n",
    "class myCONVLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layer, num_class):\n",
    "        super(myCONVLSTM, self).__init__()\n",
    "        self.num_layer = num_layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.conv_temp = nn.Conv2d(1,40,tuple([1,25]))\n",
    "        self.conv_elec = nn.Conv3d(1,23,tuple([40, 23, 1]))\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layer, dropout=0.5)\n",
    "        self.classifier = nn.Linear(hidden_dim, num_class)\n",
    "    def forward(self, x):\n",
    "        x.unsqueeze_(1)\n",
    "        out_conv_temp = self.conv_temp(x)\n",
    "        out_conv_temp = out_conv_temp.unsqueeze_(1)\n",
    "        out_conv_elec = self.conv_elec(out_conv_temp)\n",
    "        out_conv_elec_sque= torch.squeeze(out_conv_elec)\n",
    "        out_swap = out_conv_elec_sque.permute(2,0,1)\n",
    "        out_lstm, _ = self.lstm(out_swap)\n",
    "        out_lstm = out_lstm[-1, :, :]\n",
    "        out = self.classifier(out_lstm)\n",
    "        return out\n",
    "    def check_accuracy(self, dataloader):\n",
    "        total_correct = 0\n",
    "        total_label = 0\n",
    "        for i_batch, sample_batched in enumerate(dataloader):\n",
    "            X_sample, y_sample = sample_batched\n",
    "            X_sample, y_sample = Variable(X_sample), Variable(y_sample)\n",
    "            out = self.forward(X_sample.cuda())\n",
    "            _, pred = torch.max(out, 1)\n",
    "            num_correct = np.sum(pred.data.cpu().numpy() == y_sample.data.cpu().numpy())\n",
    "            total_correct += num_correct\n",
    "            total_label += len(pred)\n",
    "        return  total_correct / total_label\n",
    "\n",
    "class mySHALLOWCONV(nn.Module):\n",
    "    def __init__(self, num_class):\n",
    "        super(mySHALLOWCONV, self).__init__()\n",
    "        self.conv_temp = nn.Conv2d(1,40,tuple([1,25]))\n",
    "        self.conv_elec = nn.Conv3d(1,40,tuple([40, 23, 1]))\n",
    "        self.pool = nn.AvgPool2d(tuple([1,47]))\n",
    "        self.classifier = nn.Linear(40*20, num_class)\n",
    "    def forward(self, x):\n",
    "        N, H, W = x.size()\n",
    "        x.unsqueeze_(1)\n",
    "        out_conv_temp = self.conv_temp(x)\n",
    "        out_conv_temp = out_conv_temp.unsqueeze_(1)\n",
    "        out_conv_elec = self.conv_elec(out_conv_temp)\n",
    "        out_conv_elec = torch.squeeze(out_conv_elec) # shape: [N, 40, 976]\n",
    "        out_conv_elec.unsqueeze_(1)\n",
    "        out_pool = self.pool(out_conv_elec) \n",
    "        out_pool = torch.squeeze(out_pool) # shape: [N, 40, 20]\n",
    "        out_pool = out_pool.view(N, -1) # shape: [N, 800]\n",
    "        out = self.classifier(out_pool)\n",
    "        return out\n",
    "    def check_accuracy(self, dataloader):\n",
    "        total_correct = 0\n",
    "        total_label = 0\n",
    "        for i_batch, sample_batched in enumerate(dataloader):\n",
    "            X_sample, y_sample = sample_batched\n",
    "            X_sample, y_sample = Variable(X_sample), Variable(y_sample)\n",
    "            out = self.forward(X_sample.cuda())\n",
    "            _, pred = torch.max(out, 1)\n",
    "            num_correct = np.sum(pred.data.cpu().numpy() == y_sample.data.cpu().numpy())\n",
    "            total_correct += num_correct\n",
    "            total_label += len(pred)\n",
    "        return  total_correct / total_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.cuda.FloatTensor\n",
    "hidden_dim = 32\n",
    "num_classes = 4\n",
    "num_epoches = 10\n",
    "#model = myGRU(E, hidden_dim, 1, num_classes)\n",
    "#model = myCONVGRU(E, hidden_dim, 1, num_classes)\n",
    "#model = myLSTM(E, hidden_dim, 1, num_classes)\n",
    "#model = myLSTMDO(E, hidden_dim, 1, num_classes)\n",
    "#model = myCONVLSTM(E, hidden_dim, 1, num_classes)\n",
    "#model = myChronoNet(E, hidden_dim, 1, num_classes)\n",
    "#model = mySHALLOWCONV(4)\n",
    "model.type(dtype)\n",
    "loss_fn = nn.CrossEntropyLoss().type(dtype)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Time from 0 to 100.000000\n",
      "(0 batch) loss: 1.401047\n",
      "(1 batch) loss: 1.384264\n",
      "(2 batch) loss: 1.373719\n",
      "(3 batch) loss: 1.422058\n",
      "(4 batch) loss: 1.387765\n",
      "(5 batch) loss: 1.359707\n",
      "(6 batch) loss: 1.370012\n",
      "(7 batch) loss: 1.389989\n",
      "(8 batch) loss: 1.376621\n",
      "(9 batch) loss: 1.345337\n",
      "(10 batch) loss: 1.333254\n",
      "(Epoch 1 / 10) train_acc: 0.351923; val_acc: 0.300000\n",
      "(0 batch) loss: 1.350254\n",
      "(1 batch) loss: 1.356690\n",
      "(2 batch) loss: 1.317568\n",
      "(3 batch) loss: 1.353078\n",
      "(4 batch) loss: 1.338410\n",
      "(5 batch) loss: 1.333074\n",
      "(6 batch) loss: 1.324486\n",
      "(7 batch) loss: 1.333457\n",
      "(8 batch) loss: 1.315872\n",
      "(9 batch) loss: 1.337314\n",
      "(10 batch) loss: 1.321870\n",
      "(Epoch 2 / 10) train_acc: 0.396635; val_acc: 0.390000\n",
      "(0 batch) loss: 1.316575\n",
      "(1 batch) loss: 1.277349\n",
      "(2 batch) loss: 1.312056\n",
      "(3 batch) loss: 1.281741\n",
      "(4 batch) loss: 1.311806\n",
      "(5 batch) loss: 1.294060\n",
      "(6 batch) loss: 1.314167\n",
      "(7 batch) loss: 1.276404\n",
      "(8 batch) loss: 1.287203\n",
      "(9 batch) loss: 1.299547\n",
      "(10 batch) loss: 1.263718\n",
      "(Epoch 3 / 10) train_acc: 0.420192; val_acc: 0.390000\n",
      "(0 batch) loss: 1.263617\n",
      "(1 batch) loss: 1.271772\n",
      "(2 batch) loss: 1.254275\n",
      "(3 batch) loss: 1.244370\n",
      "(4 batch) loss: 1.256706\n",
      "(5 batch) loss: 1.254390\n",
      "(6 batch) loss: 1.283217\n",
      "(7 batch) loss: 1.274011\n",
      "(8 batch) loss: 1.241365\n",
      "(9 batch) loss: 1.286944\n",
      "(10 batch) loss: 1.214389\n",
      "(Epoch 4 / 10) train_acc: 0.435577; val_acc: 0.390000\n",
      "(0 batch) loss: 1.205138\n",
      "(1 batch) loss: 1.213366\n",
      "(2 batch) loss: 1.284616\n",
      "(3 batch) loss: 1.210433\n",
      "(4 batch) loss: 1.245423\n",
      "(5 batch) loss: 1.188655\n",
      "(6 batch) loss: 1.247039\n",
      "(7 batch) loss: 1.254241\n",
      "(8 batch) loss: 1.172364\n",
      "(9 batch) loss: 1.266398\n",
      "(10 batch) loss: 1.254453\n",
      "(Epoch 5 / 10) train_acc: 0.429327; val_acc: 0.410000\n",
      "(0 batch) loss: 1.180311\n",
      "(1 batch) loss: 1.249952\n",
      "(2 batch) loss: 1.210478\n",
      "(3 batch) loss: 1.203926\n",
      "(4 batch) loss: 1.224169\n",
      "(5 batch) loss: 1.262198\n",
      "(6 batch) loss: 1.149089\n",
      "(7 batch) loss: 1.251782\n",
      "(8 batch) loss: 1.171474\n",
      "(9 batch) loss: 1.198444\n",
      "(10 batch) loss: 1.313158\n",
      "(Epoch 6 / 10) train_acc: 0.445673; val_acc: 0.410000\n",
      "(0 batch) loss: 1.212092\n",
      "(1 batch) loss: 1.221506\n",
      "(2 batch) loss: 1.194035\n",
      "(3 batch) loss: 1.202963\n",
      "(4 batch) loss: 1.188437\n",
      "(5 batch) loss: 1.243582\n",
      "(6 batch) loss: 1.200980\n",
      "(7 batch) loss: 1.196608\n",
      "(8 batch) loss: 1.166324\n",
      "(9 batch) loss: 1.163730\n",
      "(10 batch) loss: 1.122309\n",
      "(Epoch 7 / 10) train_acc: 0.458654; val_acc: 0.390000\n",
      "(0 batch) loss: 1.186506\n",
      "(1 batch) loss: 1.192920\n",
      "(2 batch) loss: 1.202920\n",
      "(3 batch) loss: 1.182107\n",
      "(4 batch) loss: 1.166367\n",
      "(5 batch) loss: 1.144969\n",
      "(6 batch) loss: 1.192112\n",
      "(7 batch) loss: 1.167194\n",
      "(8 batch) loss: 1.211957\n",
      "(9 batch) loss: 1.201339\n",
      "(10 batch) loss: 1.153015\n",
      "(Epoch 8 / 10) train_acc: 0.474038; val_acc: 0.440000\n",
      "(0 batch) loss: 1.138342\n",
      "(1 batch) loss: 1.129438\n",
      "(2 batch) loss: 1.216633\n",
      "(3 batch) loss: 1.172570\n",
      "(4 batch) loss: 1.162200\n",
      "(5 batch) loss: 1.195727\n",
      "(6 batch) loss: 1.161414\n",
      "(7 batch) loss: 1.217686\n",
      "(8 batch) loss: 1.179448\n",
      "(9 batch) loss: 1.138522\n",
      "(10 batch) loss: 1.152199\n",
      "(Epoch 9 / 10) train_acc: 0.475962; val_acc: 0.420000\n",
      "(0 batch) loss: 1.154164\n",
      "(1 batch) loss: 1.185152\n",
      "(2 batch) loss: 1.136874\n",
      "(3 batch) loss: 1.203468\n",
      "(4 batch) loss: 1.181734\n",
      "(5 batch) loss: 1.133349\n",
      "(6 batch) loss: 1.095794\n",
      "(7 batch) loss: 1.140570\n",
      "(8 batch) loss: 1.173594\n",
      "(9 batch) loss: 1.159828\n",
      "(10 batch) loss: 1.166557\n",
      "(Epoch 10 / 10) train_acc: 0.488942; val_acc: 0.460000\n",
      "200\n",
      "Time from 0 to 200.000000\n",
      "(0 batch) loss: 1.404553\n",
      "(1 batch) loss: 1.401959\n",
      "(2 batch) loss: 1.394447\n",
      "(3 batch) loss: 1.390471\n",
      "(4 batch) loss: 1.395341\n",
      "(5 batch) loss: 1.379143\n",
      "(6 batch) loss: 1.391476\n",
      "(7 batch) loss: 1.371707\n",
      "(8 batch) loss: 1.398788\n",
      "(9 batch) loss: 1.389172\n",
      "(10 batch) loss: 1.405370\n",
      "(Epoch 1 / 10) train_acc: 0.300000; val_acc: 0.400000\n",
      "(0 batch) loss: 1.369130\n",
      "(1 batch) loss: 1.373499\n",
      "(2 batch) loss: 1.376541\n",
      "(3 batch) loss: 1.381993\n",
      "(4 batch) loss: 1.373220\n",
      "(5 batch) loss: 1.364349\n",
      "(6 batch) loss: 1.365651\n",
      "(7 batch) loss: 1.370290\n",
      "(8 batch) loss: 1.351492\n",
      "(9 batch) loss: 1.368385\n",
      "(10 batch) loss: 1.397354\n",
      "(Epoch 2 / 10) train_acc: 0.318750; val_acc: 0.410000\n",
      "(0 batch) loss: 1.362339\n",
      "(1 batch) loss: 1.349869\n",
      "(2 batch) loss: 1.362968\n",
      "(3 batch) loss: 1.359813\n",
      "(4 batch) loss: 1.348563\n",
      "(5 batch) loss: 1.364633\n",
      "(6 batch) loss: 1.350815\n",
      "(7 batch) loss: 1.347727\n",
      "(8 batch) loss: 1.361812\n",
      "(9 batch) loss: 1.352007\n",
      "(10 batch) loss: 1.353079\n",
      "(Epoch 3 / 10) train_acc: 0.363942; val_acc: 0.410000\n",
      "(0 batch) loss: 1.366382\n",
      "(1 batch) loss: 1.343312\n",
      "(2 batch) loss: 1.348715\n",
      "(3 batch) loss: 1.355018\n",
      "(4 batch) loss: 1.343621\n",
      "(5 batch) loss: 1.331191\n",
      "(6 batch) loss: 1.320389\n",
      "(7 batch) loss: 1.327379\n",
      "(8 batch) loss: 1.323121\n",
      "(9 batch) loss: 1.340095\n",
      "(10 batch) loss: 1.331204\n",
      "(Epoch 4 / 10) train_acc: 0.386538; val_acc: 0.360000\n",
      "(0 batch) loss: 1.338977\n",
      "(1 batch) loss: 1.326931\n",
      "(2 batch) loss: 1.316426\n",
      "(3 batch) loss: 1.324808\n",
      "(4 batch) loss: 1.328470\n",
      "(5 batch) loss: 1.308369\n",
      "(6 batch) loss: 1.294210\n",
      "(7 batch) loss: 1.320236\n",
      "(8 batch) loss: 1.355354\n",
      "(9 batch) loss: 1.316585\n",
      "(10 batch) loss: 1.285485\n",
      "(Epoch 5 / 10) train_acc: 0.392788; val_acc: 0.340000\n",
      "(0 batch) loss: 1.300149\n",
      "(1 batch) loss: 1.313061\n",
      "(2 batch) loss: 1.338789\n",
      "(3 batch) loss: 1.306960\n",
      "(4 batch) loss: 1.292035\n",
      "(5 batch) loss: 1.284747\n",
      "(6 batch) loss: 1.279951\n",
      "(7 batch) loss: 1.321894\n",
      "(8 batch) loss: 1.321848\n",
      "(9 batch) loss: 1.270847\n",
      "(10 batch) loss: 1.306843\n",
      "(Epoch 6 / 10) train_acc: 0.414904; val_acc: 0.360000\n",
      "(0 batch) loss: 1.282518\n",
      "(1 batch) loss: 1.250124\n",
      "(2 batch) loss: 1.280167\n",
      "(3 batch) loss: 1.281381\n",
      "(4 batch) loss: 1.349192\n",
      "(5 batch) loss: 1.283208\n",
      "(6 batch) loss: 1.267601\n",
      "(7 batch) loss: 1.295465\n",
      "(8 batch) loss: 1.285594\n",
      "(9 batch) loss: 1.272637\n",
      "(10 batch) loss: 1.330309\n",
      "(Epoch 7 / 10) train_acc: 0.420673; val_acc: 0.390000\n",
      "(0 batch) loss: 1.284889\n",
      "(1 batch) loss: 1.264376\n",
      "(2 batch) loss: 1.252231\n",
      "(3 batch) loss: 1.269987\n",
      "(4 batch) loss: 1.274195\n",
      "(5 batch) loss: 1.273648\n",
      "(6 batch) loss: 1.266193\n",
      "(7 batch) loss: 1.277554\n",
      "(8 batch) loss: 1.299544\n",
      "(9 batch) loss: 1.287646\n",
      "(10 batch) loss: 1.186766\n",
      "(Epoch 8 / 10) train_acc: 0.433173; val_acc: 0.410000\n",
      "(0 batch) loss: 1.223825\n",
      "(1 batch) loss: 1.243002\n",
      "(2 batch) loss: 1.227845\n",
      "(3 batch) loss: 1.281325\n",
      "(4 batch) loss: 1.267493\n",
      "(5 batch) loss: 1.298361\n",
      "(6 batch) loss: 1.253418\n",
      "(7 batch) loss: 1.272231\n",
      "(8 batch) loss: 1.247880\n",
      "(9 batch) loss: 1.268735\n",
      "(10 batch) loss: 1.239217\n",
      "(Epoch 9 / 10) train_acc: 0.441827; val_acc: 0.380000\n",
      "(0 batch) loss: 1.251804\n",
      "(1 batch) loss: 1.178859\n",
      "(2 batch) loss: 1.241352\n",
      "(3 batch) loss: 1.251302\n",
      "(4 batch) loss: 1.238628\n",
      "(5 batch) loss: 1.266647\n",
      "(6 batch) loss: 1.236969\n",
      "(7 batch) loss: 1.207096\n",
      "(8 batch) loss: 1.305449\n",
      "(9 batch) loss: 1.221156\n",
      "(10 batch) loss: 1.319684\n",
      "(Epoch 10 / 10) train_acc: 0.444712; val_acc: 0.380000\n",
      "300\n",
      "Time from 0 to 300.000000\n",
      "(0 batch) loss: 1.417390\n",
      "(1 batch) loss: 1.375719\n",
      "(2 batch) loss: 1.385732\n",
      "(3 batch) loss: 1.391863\n",
      "(4 batch) loss: 1.396100\n",
      "(5 batch) loss: 1.377948\n",
      "(6 batch) loss: 1.383037\n",
      "(7 batch) loss: 1.387491\n",
      "(8 batch) loss: 1.375541\n",
      "(9 batch) loss: 1.378587\n",
      "(10 batch) loss: 1.394649\n",
      "(Epoch 1 / 10) train_acc: 0.318269; val_acc: 0.250000\n",
      "(0 batch) loss: 1.365046\n",
      "(1 batch) loss: 1.357380\n",
      "(2 batch) loss: 1.350723\n",
      "(3 batch) loss: 1.363173\n",
      "(4 batch) loss: 1.364056\n",
      "(5 batch) loss: 1.360656\n",
      "(6 batch) loss: 1.358429\n",
      "(7 batch) loss: 1.366395\n",
      "(8 batch) loss: 1.346699\n",
      "(9 batch) loss: 1.363451\n",
      "(10 batch) loss: 1.348390\n",
      "(Epoch 2 / 10) train_acc: 0.356250; val_acc: 0.280000\n",
      "(0 batch) loss: 1.331679\n",
      "(1 batch) loss: 1.345384\n",
      "(2 batch) loss: 1.336101\n",
      "(3 batch) loss: 1.353662\n",
      "(4 batch) loss: 1.368051\n",
      "(5 batch) loss: 1.317632\n",
      "(6 batch) loss: 1.358908\n",
      "(7 batch) loss: 1.356018\n",
      "(8 batch) loss: 1.331527\n",
      "(9 batch) loss: 1.348341\n",
      "(10 batch) loss: 1.323642\n",
      "(Epoch 3 / 10) train_acc: 0.374519; val_acc: 0.310000\n",
      "(0 batch) loss: 1.331925\n",
      "(1 batch) loss: 1.334528\n",
      "(2 batch) loss: 1.335948\n",
      "(3 batch) loss: 1.317036\n",
      "(4 batch) loss: 1.347289\n",
      "(5 batch) loss: 1.320720\n",
      "(6 batch) loss: 1.327238\n",
      "(7 batch) loss: 1.315862\n",
      "(8 batch) loss: 1.317300\n",
      "(9 batch) loss: 1.345744\n",
      "(10 batch) loss: 1.284427\n",
      "(Epoch 4 / 10) train_acc: 0.382692; val_acc: 0.350000\n",
      "(0 batch) loss: 1.323041\n",
      "(1 batch) loss: 1.313281\n",
      "(2 batch) loss: 1.306251\n",
      "(3 batch) loss: 1.293460\n",
      "(4 batch) loss: 1.292491\n",
      "(5 batch) loss: 1.331575\n",
      "(6 batch) loss: 1.359921\n",
      "(7 batch) loss: 1.313202\n",
      "(8 batch) loss: 1.330737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9 batch) loss: 1.317434\n",
      "(10 batch) loss: 1.247204\n",
      "(Epoch 5 / 10) train_acc: 0.383654; val_acc: 0.370000\n",
      "(0 batch) loss: 1.286668\n",
      "(1 batch) loss: 1.301221\n",
      "(2 batch) loss: 1.326022\n",
      "(3 batch) loss: 1.242051\n",
      "(4 batch) loss: 1.324915\n",
      "(5 batch) loss: 1.339112\n",
      "(6 batch) loss: 1.309374\n",
      "(7 batch) loss: 1.326441\n",
      "(8 batch) loss: 1.287148\n",
      "(9 batch) loss: 1.286741\n",
      "(10 batch) loss: 1.273506\n",
      "(Epoch 6 / 10) train_acc: 0.397596; val_acc: 0.380000\n",
      "(0 batch) loss: 1.324018\n",
      "(1 batch) loss: 1.332736\n",
      "(2 batch) loss: 1.286780\n",
      "(3 batch) loss: 1.267756\n",
      "(4 batch) loss: 1.274322\n",
      "(5 batch) loss: 1.242505\n",
      "(6 batch) loss: 1.274469\n",
      "(7 batch) loss: 1.294344\n",
      "(8 batch) loss: 1.304338\n",
      "(9 batch) loss: 1.306169\n",
      "(10 batch) loss: 1.295750\n",
      "(Epoch 7 / 10) train_acc: 0.402885; val_acc: 0.410000\n",
      "(0 batch) loss: 1.302429\n",
      "(1 batch) loss: 1.281523\n",
      "(2 batch) loss: 1.243677\n",
      "(3 batch) loss: 1.267371\n",
      "(4 batch) loss: 1.302567\n",
      "(5 batch) loss: 1.302703\n",
      "(6 batch) loss: 1.271092\n",
      "(7 batch) loss: 1.290732\n",
      "(8 batch) loss: 1.293114\n",
      "(9 batch) loss: 1.252978\n",
      "(10 batch) loss: 1.269259\n",
      "(Epoch 8 / 10) train_acc: 0.412019; val_acc: 0.370000\n",
      "(0 batch) loss: 1.286953\n",
      "(1 batch) loss: 1.301601\n",
      "(2 batch) loss: 1.279677\n",
      "(3 batch) loss: 1.296002\n",
      "(4 batch) loss: 1.239020\n",
      "(5 batch) loss: 1.276658\n",
      "(6 batch) loss: 1.272831\n",
      "(7 batch) loss: 1.262429\n",
      "(8 batch) loss: 1.246921\n",
      "(9 batch) loss: 1.230601\n",
      "(10 batch) loss: 1.267695\n",
      "(Epoch 9 / 10) train_acc: 0.427885; val_acc: 0.400000\n",
      "(0 batch) loss: 1.263450\n",
      "(1 batch) loss: 1.217863\n",
      "(2 batch) loss: 1.251801\n",
      "(3 batch) loss: 1.243613\n",
      "(4 batch) loss: 1.274571\n",
      "(5 batch) loss: 1.242584\n",
      "(6 batch) loss: 1.254553\n",
      "(7 batch) loss: 1.284562\n",
      "(8 batch) loss: 1.283145\n",
      "(9 batch) loss: 1.302378\n",
      "(10 batch) loss: 1.201359\n",
      "(Epoch 10 / 10) train_acc: 0.425481; val_acc: 0.380000\n",
      "400\n",
      "Time from 0 to 400.000000\n",
      "(0 batch) loss: 1.399469\n",
      "(1 batch) loss: 1.402987\n",
      "(2 batch) loss: 1.416625\n",
      "(3 batch) loss: 1.393010\n",
      "(4 batch) loss: 1.382366\n",
      "(5 batch) loss: 1.361693\n",
      "(6 batch) loss: 1.385578\n",
      "(7 batch) loss: 1.369970\n",
      "(8 batch) loss: 1.363809\n",
      "(9 batch) loss: 1.345908\n",
      "(10 batch) loss: 1.374497\n",
      "(Epoch 1 / 10) train_acc: 0.336538; val_acc: 0.300000\n",
      "(0 batch) loss: 1.359124\n",
      "(1 batch) loss: 1.347364\n",
      "(2 batch) loss: 1.382341\n",
      "(3 batch) loss: 1.333918\n",
      "(4 batch) loss: 1.339033\n",
      "(5 batch) loss: 1.343841\n",
      "(6 batch) loss: 1.354896\n",
      "(7 batch) loss: 1.339518\n",
      "(8 batch) loss: 1.358957\n",
      "(9 batch) loss: 1.370171\n",
      "(10 batch) loss: 1.310026\n",
      "(Epoch 2 / 10) train_acc: 0.371154; val_acc: 0.280000\n",
      "(0 batch) loss: 1.331094\n",
      "(1 batch) loss: 1.363202\n",
      "(2 batch) loss: 1.350551\n",
      "(3 batch) loss: 1.341381\n",
      "(4 batch) loss: 1.326918\n",
      "(5 batch) loss: 1.291629\n",
      "(6 batch) loss: 1.305196\n",
      "(7 batch) loss: 1.342124\n",
      "(8 batch) loss: 1.338467\n",
      "(9 batch) loss: 1.320006\n",
      "(10 batch) loss: 1.350402\n",
      "(Epoch 3 / 10) train_acc: 0.387019; val_acc: 0.340000\n",
      "(0 batch) loss: 1.330642\n",
      "(1 batch) loss: 1.307943\n",
      "(2 batch) loss: 1.299962\n",
      "(3 batch) loss: 1.325403\n",
      "(4 batch) loss: 1.306872\n",
      "(5 batch) loss: 1.332711\n",
      "(6 batch) loss: 1.330919\n",
      "(7 batch) loss: 1.316013\n",
      "(8 batch) loss: 1.307750\n",
      "(9 batch) loss: 1.327827\n",
      "(10 batch) loss: 1.297333\n",
      "(Epoch 4 / 10) train_acc: 0.402885; val_acc: 0.360000\n",
      "(0 batch) loss: 1.308109\n",
      "(1 batch) loss: 1.352450\n",
      "(2 batch) loss: 1.314389\n",
      "(3 batch) loss: 1.346914\n",
      "(4 batch) loss: 1.271999\n",
      "(5 batch) loss: 1.275806\n",
      "(6 batch) loss: 1.311142\n",
      "(7 batch) loss: 1.285376\n",
      "(8 batch) loss: 1.285969\n",
      "(9 batch) loss: 1.275886\n",
      "(10 batch) loss: 1.365633\n",
      "(Epoch 5 / 10) train_acc: 0.403365; val_acc: 0.350000\n",
      "(0 batch) loss: 1.291172\n",
      "(1 batch) loss: 1.305150\n",
      "(2 batch) loss: 1.267708\n",
      "(3 batch) loss: 1.294330\n",
      "(4 batch) loss: 1.302684\n",
      "(5 batch) loss: 1.275541\n",
      "(6 batch) loss: 1.288119\n",
      "(7 batch) loss: 1.276727\n",
      "(8 batch) loss: 1.316626\n",
      "(9 batch) loss: 1.315439\n",
      "(10 batch) loss: 1.292600\n",
      "(Epoch 6 / 10) train_acc: 0.409615; val_acc: 0.380000\n",
      "(0 batch) loss: 1.277889\n",
      "(1 batch) loss: 1.265315\n",
      "(2 batch) loss: 1.249577\n",
      "(3 batch) loss: 1.309904\n",
      "(4 batch) loss: 1.313902\n",
      "(5 batch) loss: 1.259442\n",
      "(6 batch) loss: 1.248588\n",
      "(7 batch) loss: 1.308147\n",
      "(8 batch) loss: 1.293466\n",
      "(9 batch) loss: 1.262792\n",
      "(10 batch) loss: 1.318830\n",
      "(Epoch 7 / 10) train_acc: 0.422115; val_acc: 0.320000\n",
      "(0 batch) loss: 1.264452\n",
      "(1 batch) loss: 1.246425\n",
      "(2 batch) loss: 1.283484\n",
      "(3 batch) loss: 1.261369\n",
      "(4 batch) loss: 1.213435\n",
      "(5 batch) loss: 1.288030\n",
      "(6 batch) loss: 1.271274\n",
      "(7 batch) loss: 1.264104\n",
      "(8 batch) loss: 1.286417\n",
      "(9 batch) loss: 1.283378\n",
      "(10 batch) loss: 1.343739\n",
      "(Epoch 8 / 10) train_acc: 0.438462; val_acc: 0.360000\n",
      "(0 batch) loss: 1.300557\n",
      "(1 batch) loss: 1.259601\n",
      "(2 batch) loss: 1.276497\n",
      "(3 batch) loss: 1.246133\n",
      "(4 batch) loss: 1.271468\n",
      "(5 batch) loss: 1.225196\n",
      "(6 batch) loss: 1.237983\n",
      "(7 batch) loss: 1.240234\n",
      "(8 batch) loss: 1.232057\n",
      "(9 batch) loss: 1.290928\n",
      "(10 batch) loss: 1.179688\n",
      "(Epoch 9 / 10) train_acc: 0.442308; val_acc: 0.340000\n",
      "(0 batch) loss: 1.273067\n",
      "(1 batch) loss: 1.205634\n",
      "(2 batch) loss: 1.210364\n",
      "(3 batch) loss: 1.234419\n",
      "(4 batch) loss: 1.254265\n",
      "(5 batch) loss: 1.251645\n",
      "(6 batch) loss: 1.207623\n",
      "(7 batch) loss: 1.296388\n",
      "(8 batch) loss: 1.250664\n",
      "(9 batch) loss: 1.258160\n",
      "(10 batch) loss: 1.231218\n",
      "(Epoch 10 / 10) train_acc: 0.452885; val_acc: 0.360000\n",
      "500\n",
      "Time from 0 to 500.000000\n",
      "(0 batch) loss: 1.419819\n",
      "(1 batch) loss: 1.395511\n",
      "(2 batch) loss: 1.395752\n",
      "(3 batch) loss: 1.426520\n",
      "(4 batch) loss: 1.406585\n",
      "(5 batch) loss: 1.400423\n",
      "(6 batch) loss: 1.397063\n",
      "(7 batch) loss: 1.390285\n",
      "(8 batch) loss: 1.402704\n",
      "(9 batch) loss: 1.397258\n",
      "(10 batch) loss: 1.397171\n",
      "(Epoch 1 / 10) train_acc: 0.273077; val_acc: 0.240000\n",
      "(0 batch) loss: 1.383755\n",
      "(1 batch) loss: 1.378934\n",
      "(2 batch) loss: 1.373910\n",
      "(3 batch) loss: 1.379556\n",
      "(4 batch) loss: 1.391163\n",
      "(5 batch) loss: 1.394567\n",
      "(6 batch) loss: 1.372185\n",
      "(7 batch) loss: 1.384964\n",
      "(8 batch) loss: 1.381433\n",
      "(9 batch) loss: 1.378160\n",
      "(10 batch) loss: 1.371734\n",
      "(Epoch 2 / 10) train_acc: 0.297115; val_acc: 0.260000\n",
      "(0 batch) loss: 1.381991\n",
      "(1 batch) loss: 1.372617\n",
      "(2 batch) loss: 1.370029\n",
      "(3 batch) loss: 1.368052\n",
      "(4 batch) loss: 1.366488\n",
      "(5 batch) loss: 1.375319\n",
      "(6 batch) loss: 1.385669\n",
      "(7 batch) loss: 1.374400\n",
      "(8 batch) loss: 1.370541\n",
      "(9 batch) loss: 1.378940\n",
      "(10 batch) loss: 1.354059\n",
      "(Epoch 3 / 10) train_acc: 0.313462; val_acc: 0.270000\n",
      "(0 batch) loss: 1.358616\n",
      "(1 batch) loss: 1.376620\n",
      "(2 batch) loss: 1.368451\n",
      "(3 batch) loss: 1.376151\n",
      "(4 batch) loss: 1.370386\n",
      "(5 batch) loss: 1.378105\n",
      "(6 batch) loss: 1.357606\n",
      "(7 batch) loss: 1.354587\n",
      "(8 batch) loss: 1.358464\n",
      "(9 batch) loss: 1.367386\n",
      "(10 batch) loss: 1.386725\n",
      "(Epoch 4 / 10) train_acc: 0.329327; val_acc: 0.310000\n",
      "(0 batch) loss: 1.364874\n",
      "(1 batch) loss: 1.358677\n",
      "(2 batch) loss: 1.353851\n",
      "(3 batch) loss: 1.369955\n",
      "(4 batch) loss: 1.359888\n",
      "(5 batch) loss: 1.371734\n",
      "(6 batch) loss: 1.353762\n",
      "(7 batch) loss: 1.362899\n",
      "(8 batch) loss: 1.355945\n",
      "(9 batch) loss: 1.352742\n",
      "(10 batch) loss: 1.379312\n",
      "(Epoch 5 / 10) train_acc: 0.333654; val_acc: 0.280000\n",
      "(0 batch) loss: 1.354371\n",
      "(1 batch) loss: 1.356346\n",
      "(2 batch) loss: 1.365609\n",
      "(3 batch) loss: 1.348106\n",
      "(4 batch) loss: 1.345570\n",
      "(5 batch) loss: 1.362550\n",
      "(6 batch) loss: 1.353810\n",
      "(7 batch) loss: 1.345119\n",
      "(8 batch) loss: 1.347487\n",
      "(9 batch) loss: 1.378548\n",
      "(10 batch) loss: 1.353569\n",
      "(Epoch 6 / 10) train_acc: 0.350000; val_acc: 0.300000\n",
      "(0 batch) loss: 1.349316\n",
      "(1 batch) loss: 1.348242\n",
      "(2 batch) loss: 1.335472\n",
      "(3 batch) loss: 1.357035\n",
      "(4 batch) loss: 1.359761\n",
      "(5 batch) loss: 1.350456\n",
      "(6 batch) loss: 1.335636\n",
      "(7 batch) loss: 1.330868\n",
      "(8 batch) loss: 1.354897\n",
      "(9 batch) loss: 1.353513\n",
      "(10 batch) loss: 1.357343\n",
      "(Epoch 7 / 10) train_acc: 0.348558; val_acc: 0.290000\n",
      "(0 batch) loss: 1.340271\n",
      "(1 batch) loss: 1.340063\n",
      "(2 batch) loss: 1.311051\n",
      "(3 batch) loss: 1.343093\n",
      "(4 batch) loss: 1.332578\n",
      "(5 batch) loss: 1.333591\n",
      "(6 batch) loss: 1.351111\n",
      "(7 batch) loss: 1.363104\n",
      "(8 batch) loss: 1.345094\n",
      "(9 batch) loss: 1.358464\n",
      "(10 batch) loss: 1.341304\n",
      "(Epoch 8 / 10) train_acc: 0.362981; val_acc: 0.300000\n",
      "(0 batch) loss: 1.306597\n",
      "(1 batch) loss: 1.341049\n",
      "(2 batch) loss: 1.342968\n",
      "(3 batch) loss: 1.322742\n",
      "(4 batch) loss: 1.347810\n",
      "(5 batch) loss: 1.350218\n",
      "(6 batch) loss: 1.331800\n",
      "(7 batch) loss: 1.335443\n",
      "(8 batch) loss: 1.321247\n",
      "(9 batch) loss: 1.327039\n",
      "(10 batch) loss: 1.348214\n",
      "(Epoch 9 / 10) train_acc: 0.374038; val_acc: 0.310000\n",
      "(0 batch) loss: 1.321120\n",
      "(1 batch) loss: 1.323187\n",
      "(2 batch) loss: 1.312726\n",
      "(3 batch) loss: 1.326425\n",
      "(4 batch) loss: 1.320580\n",
      "(5 batch) loss: 1.336061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6 batch) loss: 1.319657\n",
      "(7 batch) loss: 1.334506\n",
      "(8 batch) loss: 1.325462\n",
      "(9 batch) loss: 1.326951\n",
      "(10 batch) loss: 1.338414\n",
      "(Epoch 10 / 10) train_acc: 0.379327; val_acc: 0.280000\n",
      "600\n",
      "Time from 0 to 600.000000\n",
      "(0 batch) loss: 1.401519\n",
      "(1 batch) loss: 1.390023\n",
      "(2 batch) loss: 1.409256\n",
      "(3 batch) loss: 1.405951\n",
      "(4 batch) loss: 1.415252\n",
      "(5 batch) loss: 1.389635\n",
      "(6 batch) loss: 1.381028\n",
      "(7 batch) loss: 1.384746\n",
      "(8 batch) loss: 1.390738\n",
      "(9 batch) loss: 1.401672\n",
      "(10 batch) loss: 1.401675\n",
      "(Epoch 1 / 10) train_acc: 0.287019; val_acc: 0.210000\n",
      "(0 batch) loss: 1.380256\n",
      "(1 batch) loss: 1.379749\n",
      "(2 batch) loss: 1.379270\n",
      "(3 batch) loss: 1.387053\n",
      "(4 batch) loss: 1.384920\n",
      "(5 batch) loss: 1.380612\n",
      "(6 batch) loss: 1.379986\n",
      "(7 batch) loss: 1.374076\n",
      "(8 batch) loss: 1.375752\n",
      "(9 batch) loss: 1.392079\n",
      "(10 batch) loss: 1.371637\n",
      "(Epoch 2 / 10) train_acc: 0.306731; val_acc: 0.160000\n",
      "(0 batch) loss: 1.372209\n",
      "(1 batch) loss: 1.374454\n",
      "(2 batch) loss: 1.375935\n",
      "(3 batch) loss: 1.374650\n",
      "(4 batch) loss: 1.373869\n",
      "(5 batch) loss: 1.371632\n",
      "(6 batch) loss: 1.373779\n",
      "(7 batch) loss: 1.382612\n",
      "(8 batch) loss: 1.392375\n",
      "(9 batch) loss: 1.371879\n",
      "(10 batch) loss: 1.367398\n",
      "(Epoch 3 / 10) train_acc: 0.316827; val_acc: 0.190000\n",
      "(0 batch) loss: 1.367886\n",
      "(1 batch) loss: 1.381873\n",
      "(2 batch) loss: 1.374871\n",
      "(3 batch) loss: 1.360613\n",
      "(4 batch) loss: 1.361952\n",
      "(5 batch) loss: 1.369033\n",
      "(6 batch) loss: 1.360400\n",
      "(7 batch) loss: 1.385940\n",
      "(8 batch) loss: 1.360548\n",
      "(9 batch) loss: 1.379430\n",
      "(10 batch) loss: 1.375399\n",
      "(Epoch 4 / 10) train_acc: 0.331250; val_acc: 0.170000\n",
      "(0 batch) loss: 1.366094\n",
      "(1 batch) loss: 1.365989\n",
      "(2 batch) loss: 1.358907\n",
      "(3 batch) loss: 1.369101\n",
      "(4 batch) loss: 1.369474\n",
      "(5 batch) loss: 1.370797\n",
      "(6 batch) loss: 1.362198\n",
      "(7 batch) loss: 1.374549\n",
      "(8 batch) loss: 1.351326\n",
      "(9 batch) loss: 1.368990\n",
      "(10 batch) loss: 1.368673\n",
      "(Epoch 5 / 10) train_acc: 0.344712; val_acc: 0.210000\n",
      "(0 batch) loss: 1.370224\n",
      "(1 batch) loss: 1.356791\n",
      "(2 batch) loss: 1.365983\n",
      "(3 batch) loss: 1.356801\n",
      "(4 batch) loss: 1.350420\n",
      "(5 batch) loss: 1.362903\n",
      "(6 batch) loss: 1.371694\n",
      "(7 batch) loss: 1.362944\n",
      "(8 batch) loss: 1.354938\n",
      "(9 batch) loss: 1.357109\n",
      "(10 batch) loss: 1.361367\n",
      "(Epoch 6 / 10) train_acc: 0.343269; val_acc: 0.190000\n",
      "(0 batch) loss: 1.347098\n",
      "(1 batch) loss: 1.348220\n",
      "(2 batch) loss: 1.361871\n",
      "(3 batch) loss: 1.354734\n",
      "(4 batch) loss: 1.359452\n",
      "(5 batch) loss: 1.355356\n",
      "(6 batch) loss: 1.371073\n",
      "(7 batch) loss: 1.365345\n",
      "(8 batch) loss: 1.339202\n",
      "(9 batch) loss: 1.358288\n",
      "(10 batch) loss: 1.352368\n",
      "(Epoch 7 / 10) train_acc: 0.361058; val_acc: 0.210000\n",
      "(0 batch) loss: 1.350870\n",
      "(1 batch) loss: 1.355996\n",
      "(2 batch) loss: 1.352462\n",
      "(3 batch) loss: 1.352296\n",
      "(4 batch) loss: 1.347999\n",
      "(5 batch) loss: 1.352163\n",
      "(6 batch) loss: 1.354352\n",
      "(7 batch) loss: 1.337038\n",
      "(8 batch) loss: 1.347880\n",
      "(9 batch) loss: 1.345840\n",
      "(10 batch) loss: 1.336025\n",
      "(Epoch 8 / 10) train_acc: 0.369712; val_acc: 0.210000\n",
      "(0 batch) loss: 1.343796\n",
      "(1 batch) loss: 1.332996\n",
      "(2 batch) loss: 1.349979\n",
      "(3 batch) loss: 1.348675\n",
      "(4 batch) loss: 1.340676\n",
      "(5 batch) loss: 1.356190\n",
      "(6 batch) loss: 1.318062\n",
      "(7 batch) loss: 1.338528\n",
      "(8 batch) loss: 1.359801\n",
      "(9 batch) loss: 1.342800\n",
      "(10 batch) loss: 1.340679\n",
      "(Epoch 9 / 10) train_acc: 0.378365; val_acc: 0.200000\n",
      "(0 batch) loss: 1.358497\n",
      "(1 batch) loss: 1.342413\n",
      "(2 batch) loss: 1.324244\n",
      "(3 batch) loss: 1.339190\n",
      "(4 batch) loss: 1.335160\n",
      "(5 batch) loss: 1.332701\n",
      "(6 batch) loss: 1.333822\n",
      "(7 batch) loss: 1.337088\n",
      "(8 batch) loss: 1.321268\n",
      "(9 batch) loss: 1.338543\n",
      "(10 batch) loss: 1.344255\n",
      "(Epoch 10 / 10) train_acc: 0.383173; val_acc: 0.250000\n",
      "700\n",
      "Time from 0 to 700.000000\n",
      "(0 batch) loss: 1.414555\n",
      "(1 batch) loss: 1.421763\n",
      "(2 batch) loss: 1.402813\n",
      "(3 batch) loss: 1.407761\n",
      "(4 batch) loss: 1.386012\n",
      "(5 batch) loss: 1.399363\n",
      "(6 batch) loss: 1.390202\n",
      "(7 batch) loss: 1.399022\n",
      "(8 batch) loss: 1.395227\n",
      "(9 batch) loss: 1.394116\n",
      "(10 batch) loss: 1.391402\n",
      "(Epoch 1 / 10) train_acc: 0.269712; val_acc: 0.270000\n",
      "(0 batch) loss: 1.387093\n",
      "(1 batch) loss: 1.389330\n",
      "(2 batch) loss: 1.368963\n",
      "(3 batch) loss: 1.373018\n",
      "(4 batch) loss: 1.384488\n",
      "(5 batch) loss: 1.382213\n",
      "(6 batch) loss: 1.380577\n",
      "(7 batch) loss: 1.402761\n",
      "(8 batch) loss: 1.372093\n",
      "(9 batch) loss: 1.380579\n",
      "(10 batch) loss: 1.384771\n",
      "(Epoch 2 / 10) train_acc: 0.297596; val_acc: 0.240000\n",
      "(0 batch) loss: 1.377065\n",
      "(1 batch) loss: 1.379101\n",
      "(2 batch) loss: 1.379349\n",
      "(3 batch) loss: 1.387094\n",
      "(4 batch) loss: 1.377824\n",
      "(5 batch) loss: 1.360905\n",
      "(6 batch) loss: 1.375833\n",
      "(7 batch) loss: 1.365154\n",
      "(8 batch) loss: 1.382541\n",
      "(9 batch) loss: 1.385003\n",
      "(10 batch) loss: 1.382434\n",
      "(Epoch 3 / 10) train_acc: 0.309135; val_acc: 0.310000\n",
      "(0 batch) loss: 1.379796\n",
      "(1 batch) loss: 1.378537\n",
      "(2 batch) loss: 1.368244\n",
      "(3 batch) loss: 1.367252\n",
      "(4 batch) loss: 1.360888\n",
      "(5 batch) loss: 1.376506\n",
      "(6 batch) loss: 1.365682\n",
      "(7 batch) loss: 1.384642\n",
      "(8 batch) loss: 1.370013\n",
      "(9 batch) loss: 1.368396\n",
      "(10 batch) loss: 1.378247\n",
      "(Epoch 4 / 10) train_acc: 0.324038; val_acc: 0.250000\n",
      "(0 batch) loss: 1.356129\n",
      "(1 batch) loss: 1.370160\n",
      "(2 batch) loss: 1.373701\n",
      "(3 batch) loss: 1.368157\n",
      "(4 batch) loss: 1.363502\n",
      "(5 batch) loss: 1.364563\n",
      "(6 batch) loss: 1.380756\n",
      "(7 batch) loss: 1.368567\n",
      "(8 batch) loss: 1.370649\n",
      "(9 batch) loss: 1.358998\n",
      "(10 batch) loss: 1.386789\n",
      "(Epoch 5 / 10) train_acc: 0.332212; val_acc: 0.280000\n",
      "(0 batch) loss: 1.363448\n",
      "(1 batch) loss: 1.358364\n",
      "(2 batch) loss: 1.368845\n",
      "(3 batch) loss: 1.361171\n",
      "(4 batch) loss: 1.358580\n",
      "(5 batch) loss: 1.365445\n",
      "(6 batch) loss: 1.370172\n",
      "(7 batch) loss: 1.364092\n",
      "(8 batch) loss: 1.358845\n",
      "(9 batch) loss: 1.380736\n",
      "(10 batch) loss: 1.361027\n",
      "(Epoch 6 / 10) train_acc: 0.344712; val_acc: 0.280000\n",
      "(0 batch) loss: 1.351218\n",
      "(1 batch) loss: 1.372923\n",
      "(2 batch) loss: 1.369262\n",
      "(3 batch) loss: 1.358953\n",
      "(4 batch) loss: 1.361791\n",
      "(5 batch) loss: 1.357313\n",
      "(6 batch) loss: 1.353087\n",
      "(7 batch) loss: 1.355152\n",
      "(8 batch) loss: 1.362913\n",
      "(9 batch) loss: 1.368447\n",
      "(10 batch) loss: 1.348660\n",
      "(Epoch 7 / 10) train_acc: 0.350962; val_acc: 0.300000\n",
      "(0 batch) loss: 1.343204\n",
      "(1 batch) loss: 1.349187\n",
      "(2 batch) loss: 1.354330\n",
      "(3 batch) loss: 1.360612\n",
      "(4 batch) loss: 1.343295\n",
      "(5 batch) loss: 1.355714\n",
      "(6 batch) loss: 1.366971\n",
      "(7 batch) loss: 1.354300\n",
      "(8 batch) loss: 1.378220\n",
      "(9 batch) loss: 1.362094\n",
      "(10 batch) loss: 1.371783\n",
      "(Epoch 8 / 10) train_acc: 0.350000; val_acc: 0.250000\n",
      "(0 batch) loss: 1.354558\n",
      "(1 batch) loss: 1.335855\n",
      "(2 batch) loss: 1.370358\n",
      "(3 batch) loss: 1.344083\n",
      "(4 batch) loss: 1.342135\n",
      "(5 batch) loss: 1.337326\n",
      "(6 batch) loss: 1.354560\n",
      "(7 batch) loss: 1.357657\n",
      "(8 batch) loss: 1.355315\n",
      "(9 batch) loss: 1.357317\n",
      "(10 batch) loss: 1.361268\n",
      "(Epoch 9 / 10) train_acc: 0.355769; val_acc: 0.300000\n",
      "(0 batch) loss: 1.356599\n",
      "(1 batch) loss: 1.359722\n",
      "(2 batch) loss: 1.348943\n",
      "(3 batch) loss: 1.322100\n",
      "(4 batch) loss: 1.362402\n",
      "(5 batch) loss: 1.349360\n",
      "(6 batch) loss: 1.345738\n",
      "(7 batch) loss: 1.317220\n",
      "(8 batch) loss: 1.345911\n",
      "(9 batch) loss: 1.352367\n",
      "(10 batch) loss: 1.339441\n",
      "(Epoch 10 / 10) train_acc: 0.367788; val_acc: 0.250000\n",
      "800\n",
      "Time from 0 to 800.000000\n",
      "(0 batch) loss: 1.426638\n",
      "(1 batch) loss: 1.393374\n",
      "(2 batch) loss: 1.412956\n",
      "(3 batch) loss: 1.395592\n",
      "(4 batch) loss: 1.390575\n",
      "(5 batch) loss: 1.388950\n",
      "(6 batch) loss: 1.380727\n",
      "(7 batch) loss: 1.399113\n",
      "(8 batch) loss: 1.383707\n",
      "(9 batch) loss: 1.392290\n",
      "(10 batch) loss: 1.402549\n",
      "(Epoch 1 / 10) train_acc: 0.283654; val_acc: 0.180000\n",
      "(0 batch) loss: 1.379445\n",
      "(1 batch) loss: 1.378683\n",
      "(2 batch) loss: 1.390286\n",
      "(3 batch) loss: 1.373448\n",
      "(4 batch) loss: 1.381856\n",
      "(5 batch) loss: 1.379071\n",
      "(6 batch) loss: 1.388036\n",
      "(7 batch) loss: 1.386685\n",
      "(8 batch) loss: 1.381387\n",
      "(9 batch) loss: 1.376238\n",
      "(10 batch) loss: 1.393294\n",
      "(Epoch 2 / 10) train_acc: 0.296154; val_acc: 0.220000\n",
      "(0 batch) loss: 1.377123\n",
      "(1 batch) loss: 1.368607\n",
      "(2 batch) loss: 1.380159\n",
      "(3 batch) loss: 1.378541\n",
      "(4 batch) loss: 1.374564\n",
      "(5 batch) loss: 1.380405\n",
      "(6 batch) loss: 1.379178\n",
      "(7 batch) loss: 1.381300\n",
      "(8 batch) loss: 1.375739\n",
      "(9 batch) loss: 1.377696\n",
      "(10 batch) loss: 1.368352\n",
      "(Epoch 3 / 10) train_acc: 0.292788; val_acc: 0.200000\n",
      "(0 batch) loss: 1.369965\n",
      "(1 batch) loss: 1.362690\n",
      "(2 batch) loss: 1.364396\n",
      "(3 batch) loss: 1.378914\n",
      "(4 batch) loss: 1.375114\n",
      "(5 batch) loss: 1.378004\n",
      "(6 batch) loss: 1.363292\n",
      "(7 batch) loss: 1.373428\n",
      "(8 batch) loss: 1.384256\n",
      "(9 batch) loss: 1.371481\n",
      "(10 batch) loss: 1.377660\n",
      "(Epoch 4 / 10) train_acc: 0.311538; val_acc: 0.220000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0 batch) loss: 1.371206\n",
      "(1 batch) loss: 1.361458\n",
      "(2 batch) loss: 1.370886\n",
      "(3 batch) loss: 1.379047\n",
      "(4 batch) loss: 1.366247\n",
      "(5 batch) loss: 1.365577\n",
      "(6 batch) loss: 1.358485\n",
      "(7 batch) loss: 1.374956\n",
      "(8 batch) loss: 1.360062\n",
      "(9 batch) loss: 1.372615\n",
      "(10 batch) loss: 1.372892\n",
      "(Epoch 5 / 10) train_acc: 0.326923; val_acc: 0.230000\n",
      "(0 batch) loss: 1.365633\n",
      "(1 batch) loss: 1.370713\n",
      "(2 batch) loss: 1.366596\n",
      "(3 batch) loss: 1.354101\n",
      "(4 batch) loss: 1.375104\n",
      "(5 batch) loss: 1.361698\n",
      "(6 batch) loss: 1.367589\n",
      "(7 batch) loss: 1.348059\n",
      "(8 batch) loss: 1.372900\n",
      "(9 batch) loss: 1.358630\n",
      "(10 batch) loss: 1.359632\n",
      "(Epoch 6 / 10) train_acc: 0.321635; val_acc: 0.200000\n",
      "(0 batch) loss: 1.359576\n",
      "(1 batch) loss: 1.367538\n",
      "(2 batch) loss: 1.353649\n",
      "(3 batch) loss: 1.369946\n",
      "(4 batch) loss: 1.350749\n",
      "(5 batch) loss: 1.357267\n",
      "(6 batch) loss: 1.361283\n",
      "(7 batch) loss: 1.351822\n",
      "(8 batch) loss: 1.364002\n",
      "(9 batch) loss: 1.356188\n",
      "(10 batch) loss: 1.367660\n",
      "(Epoch 7 / 10) train_acc: 0.345192; val_acc: 0.210000\n",
      "(0 batch) loss: 1.346314\n",
      "(1 batch) loss: 1.350468\n",
      "(2 batch) loss: 1.353540\n",
      "(3 batch) loss: 1.378754\n",
      "(4 batch) loss: 1.335456\n",
      "(5 batch) loss: 1.345030\n",
      "(6 batch) loss: 1.358723\n",
      "(7 batch) loss: 1.352152\n",
      "(8 batch) loss: 1.363364\n",
      "(9 batch) loss: 1.358237\n",
      "(10 batch) loss: 1.355790\n",
      "(Epoch 8 / 10) train_acc: 0.352885; val_acc: 0.210000\n",
      "(0 batch) loss: 1.334374\n",
      "(1 batch) loss: 1.333151\n",
      "(2 batch) loss: 1.350590\n",
      "(3 batch) loss: 1.365453\n",
      "(4 batch) loss: 1.349113\n",
      "(5 batch) loss: 1.349532\n",
      "(6 batch) loss: 1.341985\n",
      "(7 batch) loss: 1.362459\n",
      "(8 batch) loss: 1.357824\n",
      "(9 batch) loss: 1.341224\n",
      "(10 batch) loss: 1.354519\n",
      "(Epoch 9 / 10) train_acc: 0.348077; val_acc: 0.190000\n",
      "(0 batch) loss: 1.351218\n",
      "(1 batch) loss: 1.347707\n",
      "(2 batch) loss: 1.337811\n",
      "(3 batch) loss: 1.326489\n",
      "(4 batch) loss: 1.320477\n",
      "(5 batch) loss: 1.353410\n",
      "(6 batch) loss: 1.340786\n",
      "(7 batch) loss: 1.343429\n",
      "(8 batch) loss: 1.357618\n",
      "(9 batch) loss: 1.344862\n",
      "(10 batch) loss: 1.344624\n",
      "(Epoch 10 / 10) train_acc: 0.369231; val_acc: 0.200000\n",
      "900\n",
      "Time from 0 to 900.000000\n",
      "(0 batch) loss: 1.405835\n",
      "(1 batch) loss: 1.395258\n",
      "(2 batch) loss: 1.392392\n",
      "(3 batch) loss: 1.391871\n",
      "(4 batch) loss: 1.399088\n",
      "(5 batch) loss: 1.394118\n",
      "(6 batch) loss: 1.392786\n",
      "(7 batch) loss: 1.397624\n",
      "(8 batch) loss: 1.412224\n",
      "(9 batch) loss: 1.384635\n",
      "(10 batch) loss: 1.381442\n",
      "(Epoch 1 / 10) train_acc: 0.274519; val_acc: 0.290000\n",
      "(0 batch) loss: 1.384656\n",
      "(1 batch) loss: 1.380457\n",
      "(2 batch) loss: 1.382864\n",
      "(3 batch) loss: 1.375370\n",
      "(4 batch) loss: 1.385318\n",
      "(5 batch) loss: 1.381344\n",
      "(6 batch) loss: 1.381453\n",
      "(7 batch) loss: 1.383409\n",
      "(8 batch) loss: 1.386104\n",
      "(9 batch) loss: 1.376395\n",
      "(10 batch) loss: 1.391536\n",
      "(Epoch 2 / 10) train_acc: 0.288942; val_acc: 0.260000\n",
      "(0 batch) loss: 1.371397\n",
      "(1 batch) loss: 1.365396\n",
      "(2 batch) loss: 1.387242\n",
      "(3 batch) loss: 1.359503\n",
      "(4 batch) loss: 1.370901\n",
      "(5 batch) loss: 1.369349\n",
      "(6 batch) loss: 1.391376\n",
      "(7 batch) loss: 1.363598\n",
      "(8 batch) loss: 1.377480\n",
      "(9 batch) loss: 1.386366\n",
      "(10 batch) loss: 1.385153\n",
      "(Epoch 3 / 10) train_acc: 0.317788; val_acc: 0.300000\n",
      "(0 batch) loss: 1.377030\n",
      "(1 batch) loss: 1.369637\n",
      "(2 batch) loss: 1.366467\n",
      "(3 batch) loss: 1.374978\n",
      "(4 batch) loss: 1.370981\n",
      "(5 batch) loss: 1.364734\n",
      "(6 batch) loss: 1.358184\n",
      "(7 batch) loss: 1.370736\n",
      "(8 batch) loss: 1.363195\n",
      "(9 batch) loss: 1.374683\n",
      "(10 batch) loss: 1.363643\n",
      "(Epoch 4 / 10) train_acc: 0.326923; val_acc: 0.260000\n",
      "(0 batch) loss: 1.368346\n",
      "(1 batch) loss: 1.348835\n",
      "(2 batch) loss: 1.365887\n",
      "(3 batch) loss: 1.349491\n",
      "(4 batch) loss: 1.360421\n",
      "(5 batch) loss: 1.371926\n",
      "(6 batch) loss: 1.377538\n",
      "(7 batch) loss: 1.362857\n",
      "(8 batch) loss: 1.359456\n",
      "(9 batch) loss: 1.373891\n",
      "(10 batch) loss: 1.362977\n",
      "(Epoch 5 / 10) train_acc: 0.329808; val_acc: 0.260000\n",
      "(0 batch) loss: 1.358947\n",
      "(1 batch) loss: 1.370695\n",
      "(2 batch) loss: 1.356072\n",
      "(3 batch) loss: 1.345561\n",
      "(4 batch) loss: 1.358428\n",
      "(5 batch) loss: 1.366675\n",
      "(6 batch) loss: 1.370932\n",
      "(7 batch) loss: 1.350635\n",
      "(8 batch) loss: 1.345559\n",
      "(9 batch) loss: 1.364856\n",
      "(10 batch) loss: 1.352838\n",
      "(Epoch 6 / 10) train_acc: 0.349519; val_acc: 0.270000\n",
      "(0 batch) loss: 1.357669\n",
      "(1 batch) loss: 1.362125\n",
      "(2 batch) loss: 1.353071\n",
      "(3 batch) loss: 1.362017\n",
      "(4 batch) loss: 1.356188\n",
      "(5 batch) loss: 1.337332\n",
      "(6 batch) loss: 1.358486\n",
      "(7 batch) loss: 1.345317\n",
      "(8 batch) loss: 1.345137\n",
      "(9 batch) loss: 1.345608\n",
      "(10 batch) loss: 1.371715\n",
      "(Epoch 7 / 10) train_acc: 0.360096; val_acc: 0.260000\n",
      "(0 batch) loss: 1.342299\n",
      "(1 batch) loss: 1.330612\n",
      "(2 batch) loss: 1.329139\n",
      "(3 batch) loss: 1.345075\n",
      "(4 batch) loss: 1.336104\n",
      "(5 batch) loss: 1.381118\n",
      "(6 batch) loss: 1.333297\n",
      "(7 batch) loss: 1.363128\n",
      "(8 batch) loss: 1.351302\n",
      "(9 batch) loss: 1.357610\n",
      "(10 batch) loss: 1.324746\n",
      "(Epoch 8 / 10) train_acc: 0.360577; val_acc: 0.260000\n",
      "(0 batch) loss: 1.348994\n",
      "(1 batch) loss: 1.339744\n",
      "(2 batch) loss: 1.349128\n",
      "(3 batch) loss: 1.343697\n",
      "(4 batch) loss: 1.331348\n",
      "(5 batch) loss: 1.333724\n",
      "(6 batch) loss: 1.360380\n",
      "(7 batch) loss: 1.319254\n",
      "(8 batch) loss: 1.346143\n",
      "(9 batch) loss: 1.324865\n",
      "(10 batch) loss: 1.330862\n",
      "(Epoch 9 / 10) train_acc: 0.373077; val_acc: 0.300000\n",
      "(0 batch) loss: 1.339550\n",
      "(1 batch) loss: 1.313147\n",
      "(2 batch) loss: 1.312812\n",
      "(3 batch) loss: 1.346589\n",
      "(4 batch) loss: 1.347251\n",
      "(5 batch) loss: 1.318023\n",
      "(6 batch) loss: 1.328295\n",
      "(7 batch) loss: 1.342214\n",
      "(8 batch) loss: 1.320566\n",
      "(9 batch) loss: 1.362227\n",
      "(10 batch) loss: 1.310468\n",
      "(Epoch 10 / 10) train_acc: 0.370192; val_acc: 0.230000\n",
      "1000\n",
      "Time from 0 to 1000.000000\n",
      "(0 batch) loss: 1.391315\n",
      "(1 batch) loss: 1.394034\n",
      "(2 batch) loss: 1.396665\n",
      "(3 batch) loss: 1.389082\n",
      "(4 batch) loss: 1.374844\n",
      "(5 batch) loss: 1.407027\n",
      "(6 batch) loss: 1.393845\n",
      "(7 batch) loss: 1.395323\n",
      "(8 batch) loss: 1.388453\n",
      "(9 batch) loss: 1.408666\n",
      "(10 batch) loss: 1.395336\n",
      "(Epoch 1 / 10) train_acc: 0.277404; val_acc: 0.270000\n",
      "(0 batch) loss: 1.383188\n",
      "(1 batch) loss: 1.390796\n",
      "(2 batch) loss: 1.361364\n",
      "(3 batch) loss: 1.388790\n",
      "(4 batch) loss: 1.367629\n",
      "(5 batch) loss: 1.382953\n",
      "(6 batch) loss: 1.380452\n",
      "(7 batch) loss: 1.395650\n",
      "(8 batch) loss: 1.379917\n",
      "(9 batch) loss: 1.378044\n",
      "(10 batch) loss: 1.389041\n",
      "(Epoch 2 / 10) train_acc: 0.291346; val_acc: 0.330000\n",
      "(0 batch) loss: 1.363367\n",
      "(1 batch) loss: 1.385778\n",
      "(2 batch) loss: 1.367599\n",
      "(3 batch) loss: 1.377191\n",
      "(4 batch) loss: 1.370117\n",
      "(5 batch) loss: 1.380381\n",
      "(6 batch) loss: 1.366583\n",
      "(7 batch) loss: 1.383597\n",
      "(8 batch) loss: 1.381241\n",
      "(9 batch) loss: 1.381341\n",
      "(10 batch) loss: 1.385002\n",
      "(Epoch 3 / 10) train_acc: 0.314423; val_acc: 0.300000\n",
      "(0 batch) loss: 1.377764\n",
      "(1 batch) loss: 1.365337\n",
      "(2 batch) loss: 1.378225\n",
      "(3 batch) loss: 1.372086\n",
      "(4 batch) loss: 1.358238\n",
      "(5 batch) loss: 1.360713\n",
      "(6 batch) loss: 1.376868\n",
      "(7 batch) loss: 1.375465\n",
      "(8 batch) loss: 1.359256\n",
      "(9 batch) loss: 1.388107\n",
      "(10 batch) loss: 1.374183\n",
      "(Epoch 4 / 10) train_acc: 0.313942; val_acc: 0.290000\n",
      "(0 batch) loss: 1.364888\n",
      "(1 batch) loss: 1.357134\n",
      "(2 batch) loss: 1.369439\n",
      "(3 batch) loss: 1.372383\n",
      "(4 batch) loss: 1.354364\n",
      "(5 batch) loss: 1.375099\n",
      "(6 batch) loss: 1.370605\n",
      "(7 batch) loss: 1.368936\n",
      "(8 batch) loss: 1.379253\n",
      "(9 batch) loss: 1.359088\n",
      "(10 batch) loss: 1.368906\n",
      "(Epoch 5 / 10) train_acc: 0.322115; val_acc: 0.290000\n",
      "(0 batch) loss: 1.360384\n",
      "(1 batch) loss: 1.364164\n",
      "(2 batch) loss: 1.356882\n",
      "(3 batch) loss: 1.364952\n",
      "(4 batch) loss: 1.381427\n",
      "(5 batch) loss: 1.351679\n",
      "(6 batch) loss: 1.351112\n",
      "(7 batch) loss: 1.363543\n",
      "(8 batch) loss: 1.367687\n",
      "(9 batch) loss: 1.365559\n",
      "(10 batch) loss: 1.339815\n",
      "(Epoch 6 / 10) train_acc: 0.346154; val_acc: 0.270000\n",
      "(0 batch) loss: 1.363284\n",
      "(1 batch) loss: 1.346997\n",
      "(2 batch) loss: 1.353663\n",
      "(3 batch) loss: 1.373521\n",
      "(4 batch) loss: 1.350786\n",
      "(5 batch) loss: 1.361375\n",
      "(6 batch) loss: 1.358117\n",
      "(7 batch) loss: 1.363508\n",
      "(8 batch) loss: 1.348589\n",
      "(9 batch) loss: 1.340692\n",
      "(10 batch) loss: 1.368844\n",
      "(Epoch 7 / 10) train_acc: 0.356250; val_acc: 0.270000\n",
      "(0 batch) loss: 1.336487\n",
      "(1 batch) loss: 1.344187\n",
      "(2 batch) loss: 1.336574\n",
      "(3 batch) loss: 1.367415\n",
      "(4 batch) loss: 1.343218\n",
      "(5 batch) loss: 1.359378\n",
      "(6 batch) loss: 1.345629\n",
      "(7 batch) loss: 1.354082\n",
      "(8 batch) loss: 1.368883\n",
      "(9 batch) loss: 1.357023\n",
      "(10 batch) loss: 1.362566\n",
      "(Epoch 8 / 10) train_acc: 0.366827; val_acc: 0.280000\n",
      "(0 batch) loss: 1.338114\n",
      "(1 batch) loss: 1.349079\n",
      "(2 batch) loss: 1.364902\n",
      "(3 batch) loss: 1.347598\n",
      "(4 batch) loss: 1.331541\n",
      "(5 batch) loss: 1.348970\n",
      "(6 batch) loss: 1.337693\n",
      "(7 batch) loss: 1.330497\n",
      "(8 batch) loss: 1.366671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9 batch) loss: 1.338119\n",
      "(10 batch) loss: 1.354939\n",
      "(Epoch 9 / 10) train_acc: 0.378846; val_acc: 0.260000\n",
      "(0 batch) loss: 1.304391\n",
      "(1 batch) loss: 1.327398\n",
      "(2 batch) loss: 1.356906\n",
      "(3 batch) loss: 1.324992\n",
      "(4 batch) loss: 1.342728\n",
      "(5 batch) loss: 1.346403\n",
      "(6 batch) loss: 1.344449\n",
      "(7 batch) loss: 1.351841\n",
      "(8 batch) loss: 1.327766\n",
      "(9 batch) loss: 1.341907\n",
      "(10 batch) loss: 1.335755\n",
      "(Epoch 10 / 10) train_acc: 0.389904; val_acc: 0.230000\n"
     ]
    }
   ],
   "source": [
    "acc_t = []\n",
    "for t in range(10):\n",
    "    print ((t+1)*100)\n",
    "    bs_train = 200\n",
    "    bs_val = 100\n",
    "    bs_test = 100\n",
    "    data = data_utils.TensorDataset(torch.Tensor(X[:,:,0:((t+1)*100)]), torch.Tensor(y))\n",
    "    dset = {}\n",
    "    dataloaders = {}\n",
    "    dset['train'], dset['val'], dset['test'] = random_split(data, [N-bs_val-bs_test, bs_val, bs_test])\n",
    "    dataloaders['train'] = data_utils.DataLoader(dset['train'], batch_size=bs_train, shuffle=True, num_workers=1)\n",
    "    dataloaders['val'] = data_utils.DataLoader(dset['val'], batch_size=bs_val, shuffle=True, num_workers=1)\n",
    "    dataloaders['test'] = data_utils.DataLoader(dset['test'], batch_size=bs_test, shuffle=True, num_workers=1)\n",
    "    best_acc = 0\n",
    "    print('Time from 0 to %f' % ((t+1)*100))\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    hidden_dim = 32\n",
    "    num_classes = 4\n",
    "    num_epoches = 10\n",
    "    #model = myLSTM(E, hidden_dim, 1, num_classes)\n",
    "    #model = myLSTMDO(E, hidden_dim, 1, num_classes)\n",
    "    #model = myCONVLSTM(E, hidden_dim, 1, num_classes)\n",
    "    model = myGRU(E, hidden_dim, 1, num_classes)\n",
    "    #model = myChronoNet(E, hidden_dim, 1, num_classes)\n",
    "    #model = mySHALLOWCONV(4)\n",
    "    model.type(dtype)\n",
    "    loss_fn = nn.CrossEntropyLoss().type(dtype)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    for epoch in range(num_epoches):\n",
    "        for i, data in enumerate(dataloaders['train'], 0):\n",
    "            X_train, y_train = data\n",
    "            # Wrap them in Variable\n",
    "            X_train, y_train = Variable(X_train), Variable(y_train)\n",
    "            # forward + backward + optimize\n",
    "            out = model(X_train.cuda())\n",
    "            # print (out)\n",
    "            loss = loss_fn(out, y_train.long().cuda())\n",
    "            print('(%d batch) loss: %f' % (i, loss))\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_acc = model.check_accuracy(dataloaders['train'])\n",
    "        val_acc = model.check_accuracy(dataloaders['val'])\n",
    "        print('(Epoch %d / %d) train_acc: %f; val_acc: %f' % (epoch+1, num_epoches, train_acc, val_acc))\n",
    "        if (val_acc > best_acc):\n",
    "            best_acc = val_acc\n",
    "            #torch.save(model, 'best_CHRONET.pt')\n",
    "    acc_t.append(best_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot acc vs time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.46, 0.41, 0.41, 0.38, 0.31, 0.25, 0.31, 0.23, 0.3, 0.33]\n"
     ]
    }
   ],
   "source": [
    "#plt.figure(1)\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('validation accuracy')\n",
    "plt.title('GRU: Accuracy vs Time')\n",
    "t = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n",
    "plt.plot(t, acc_t)\n",
    "#plt.savefig('t_GRU_test.png')\n",
    "plt.show()\n",
    "print (acc_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.79\n"
     ]
    }
   ],
   "source": [
    "#best_model = torch.load('best_CHRONET.pt')\n",
    "print (model.check_accuracy(dataloaders['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
