{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data.dataset import random_split\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/markdown"
   },
   "source": [
    "# Helper Clases / Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_Data(num):\n",
    "    if (num == -1): # All data\n",
    "        X_all = []\n",
    "        y_all = []\n",
    "        for i in range(8):\n",
    "            file_path = './../project_datasets/A0' + str(i+1) + 'T_slice.mat'\n",
    "            data = h5py.File(file_path, 'r')\n",
    "            X = np.copy(data['image'])\n",
    "            y = np.copy(data['type'])\n",
    "            X = X[:, 0:23, :]\n",
    "            X_all.append(X)\n",
    "            y = y[0,0:X.shape[0]:1]\n",
    "            y_all.append(y)\n",
    "        A, N, E, T = np.shape(X_all)\n",
    "        X_all = np.reshape(X_all, (A*N, E, T))\n",
    "        y_all = np.reshape(y_all, (-1))\n",
    "        y_all = y_all - 769\n",
    "        ## Remove NAN\n",
    "        index_Nan = []\n",
    "        for i in range(A*N):\n",
    "            for j in range(E):\n",
    "                if (any(np.isnan(X_all[i,j])) == True):\n",
    "                    index_Nan.append(i)\n",
    "        index_Nan = list(set(index_Nan))\n",
    "        X_all = np.delete(X_all, index_Nan, axis=0)\n",
    "        y_all = np.delete(y_all, index_Nan)\n",
    "        return (X_all, y_all)\n",
    "    else:\n",
    "        file_path = './../project_datasets/A0' + str(num) + 'T_slice.mat'\n",
    "        data = h5py.File(file_path, 'r')\n",
    "        X = np.copy(data['image'])\n",
    "        y = np.copy(data['type'])\n",
    "        X = X[:, 0:23, :]\n",
    "        y = y[0,0:X.shape[0]:1]\n",
    "        y = y - 769\n",
    "         ## Remove NAN\n",
    "        N, E, T = np.shape(X)\n",
    "        index_Nan = []\n",
    "        for i in range(N):\n",
    "            for j in range(E):\n",
    "                if (any(np.isnan(X[i,j])) == True):\n",
    "                    index_Nan.append(i)\n",
    "        index_Nan = list(set(index_Nan))\n",
    "        X = np.delete(X, index_Nan, axis=0)\n",
    "        y = np.delete(y, index_Nan)\n",
    "        return (X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2280, 23, 1000)\n"
     ]
    }
   ],
   "source": [
    "X, y = Load_Data(-1) # -1 to load all datas\n",
    "N, E, T = np.shape(X)\n",
    "print (np.shape(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_train = 200\n",
    "bs_val = 100\n",
    "bs_test = 100\n",
    "data = data_utils.TensorDataset(torch.Tensor(X), torch.Tensor(y))\n",
    "dset = {}\n",
    "dataloaders = {}\n",
    "dset['train'], dset['val'], dset['test'] = random_split(data, [N-bs_val-bs_test, bs_val, bs_test])\n",
    "dataloaders['train'] = data_utils.DataLoader(dset['train'], batch_size=bs_train, shuffle=True, num_workers=1)\n",
    "dataloaders['val'] = data_utils.DataLoader(dset['val'], batch_size=bs_val, shuffle=True, num_workers=1)\n",
    "dataloaders['test'] = data_utils.DataLoader(dset['test'], batch_size=bs_test, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myGRU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layer, num_class):\n",
    "        super(myGRU, self).__init__()\n",
    "        self.num_layer = num_layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # First Inception layer\n",
    "        self.conv11 = nn.Conv1d(23, 32, 2, stride=2)\n",
    "        self.conv12 = nn.Conv1d(23, 32, 4, stride=2, padding=1)\n",
    "        self.conv13 = nn.Conv1d(23, 32, 8, stride=2, padding=3)\n",
    "        # Second Inception layer\n",
    "        self.conv21 = nn.Conv1d(96, 32, 2, stride=2)\n",
    "        self.conv22 = nn.Conv1d(96, 32, 4, stride=2, padding=1)\n",
    "        self.conv23 = nn.Conv1d(96, 32, 8, stride=2, padding=3)\n",
    "        # Third Inception layer\n",
    "        self.conv31 = nn.Conv1d(96, 32, 2, stride=2)\n",
    "        self.conv32 = nn.Conv1d(96, 32, 4, stride=2, padding=1)\n",
    "        self.conv33 = nn.Conv1d(96, 32, 8, stride=2, padding=3)\n",
    "        #self.conv_13 = nn.Conv2d()\n",
    "        self.conv_elec = nn.Conv3d(1,23,tuple([40, 23, 1]))\n",
    "        self.gru1 = nn.GRU(32*3, hidden_dim, num_layer)\n",
    "        self.gru2 = nn.GRU(hidden_dim, hidden_dim, num_layer)\n",
    "        self.gru3 = nn.GRU(hidden_dim, hidden_dim, num_layer)\n",
    "        self.gru4 = nn.GRU(hidden_dim, hidden_dim, num_layer)\n",
    "        self.classifier = nn.Linear(hidden_dim, num_class)\n",
    "    def forward(self, x):\n",
    "        out_conv11 = self.conv11(x)\n",
    "        out_conv12 = self.conv12(x)\n",
    "        out_conv13 = self.conv13(x)\n",
    "        out_conv1 = torch.cat((out_conv11, out_conv12, out_conv13), 1)\n",
    "        out_conv21 = self.conv21(out_conv1)\n",
    "        out_conv22 = self.conv22(out_conv1)\n",
    "        out_conv23 = self.conv23(out_conv1)\n",
    "        out_conv2 = torch.cat((out_conv21, out_conv22, out_conv23), 1)\n",
    "        out_conv31 = self.conv31(out_conv2)\n",
    "        out_conv32 = self.conv32(out_conv2)\n",
    "        out_conv33 = self.conv33(out_conv2)\n",
    "        out_conv3 = torch.cat((out_conv31, out_conv32, out_conv33), 1)\n",
    "        # N, C, L --> L, N, C\n",
    "        out_conv3 = out_conv3.permute(2,0,1)\n",
    "        out_gru1, _ = self.gru1(out_conv3)\n",
    "        out_gru2, _ = self.gru2(out_gru1)\n",
    "        out_gru3, _ = self.gru3(out_gru2)\n",
    "        out_gru4, _ = self.gru4(out_gru3)\n",
    "        out_gru4 = out_gru4[-1, :, :] # taking the last time seq\n",
    "        out = self.classifier(out_gru4)\n",
    "        return out\n",
    "    def check_accuracy(self, dataloader):\n",
    "        total_correct = 0\n",
    "        total_label = 0\n",
    "        for i_batch, sample_batched in enumerate(dataloader):\n",
    "            X_sample, y_sample = sample_batched\n",
    "            X_sample, y_sample = Variable(X_sample), Variable(y_sample)\n",
    "            out = self.forward(X_sample.cuda())\n",
    "            _, pred = torch.max(out, 1)\n",
    "            num_correct = np.sum(pred.data.cpu().numpy() == y_sample.data.cpu().numpy())\n",
    "            total_correct += num_correct\n",
    "            total_label += len(pred)\n",
    "        return  total_correct / total_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.cuda.FloatTensor\n",
    "hidden_dim = 32\n",
    "num_classes = 4\n",
    "num_epoches = 100\n",
    "model = myGRU(E, hidden_dim, 1, num_classes)\n",
    "model.type(dtype)\n",
    "loss_fn = nn.CrossEntropyLoss().type(dtype)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0 batch) loss: 1.406681\n",
      "(1 batch) loss: 1.394054\n",
      "(2 batch) loss: 1.383358\n",
      "(3 batch) loss: 1.388829\n",
      "(4 batch) loss: 1.390692\n",
      "(5 batch) loss: 1.393853\n",
      "(6 batch) loss: 1.392676\n",
      "(7 batch) loss: 1.381942\n",
      "(8 batch) loss: 1.389934\n",
      "(9 batch) loss: 1.386938\n",
      "(10 batch) loss: 1.392554\n",
      "(Epoch 1 / 100) train_acc: 0.321154; val_acc: 0.230000\n",
      "(0 batch) loss: 1.381626\n",
      "(1 batch) loss: 1.374588\n",
      "(2 batch) loss: 1.381717\n",
      "(3 batch) loss: 1.381069\n",
      "(4 batch) loss: 1.383846\n",
      "(5 batch) loss: 1.380631\n",
      "(6 batch) loss: 1.380885\n",
      "(7 batch) loss: 1.377592\n",
      "(8 batch) loss: 1.382445\n",
      "(9 batch) loss: 1.374837\n",
      "(10 batch) loss: 1.377414\n",
      "(Epoch 2 / 100) train_acc: 0.323077; val_acc: 0.230000\n",
      "(0 batch) loss: 1.377000\n",
      "(1 batch) loss: 1.368068\n",
      "(2 batch) loss: 1.364079\n",
      "(3 batch) loss: 1.367991\n",
      "(4 batch) loss: 1.361467\n",
      "(5 batch) loss: 1.387448\n",
      "(6 batch) loss: 1.369591\n",
      "(7 batch) loss: 1.366379\n",
      "(8 batch) loss: 1.372883\n",
      "(9 batch) loss: 1.371547\n",
      "(10 batch) loss: 1.360708\n",
      "(Epoch 3 / 100) train_acc: 0.354808; val_acc: 0.280000\n",
      "(0 batch) loss: 1.353695\n",
      "(1 batch) loss: 1.354630\n",
      "(2 batch) loss: 1.364558\n",
      "(3 batch) loss: 1.338780\n",
      "(4 batch) loss: 1.365421\n",
      "(5 batch) loss: 1.362677\n",
      "(6 batch) loss: 1.353545\n",
      "(7 batch) loss: 1.363359\n",
      "(8 batch) loss: 1.344894\n",
      "(9 batch) loss: 1.360037\n",
      "(10 batch) loss: 1.390135\n",
      "(Epoch 4 / 100) train_acc: 0.385577; val_acc: 0.290000\n",
      "(0 batch) loss: 1.321186\n",
      "(1 batch) loss: 1.331961\n",
      "(2 batch) loss: 1.344300\n",
      "(3 batch) loss: 1.337713\n",
      "(4 batch) loss: 1.334816\n",
      "(5 batch) loss: 1.333434\n",
      "(6 batch) loss: 1.328746\n",
      "(7 batch) loss: 1.344984\n",
      "(8 batch) loss: 1.323405\n",
      "(9 batch) loss: 1.335478\n",
      "(10 batch) loss: 1.329097\n",
      "(Epoch 5 / 100) train_acc: 0.406731; val_acc: 0.270000\n",
      "(0 batch) loss: 1.294661\n",
      "(1 batch) loss: 1.310421\n",
      "(2 batch) loss: 1.291369\n",
      "(3 batch) loss: 1.321253\n",
      "(4 batch) loss: 1.308548\n",
      "(5 batch) loss: 1.366831\n",
      "(6 batch) loss: 1.357689\n",
      "(7 batch) loss: 1.307735\n",
      "(8 batch) loss: 1.298425\n",
      "(9 batch) loss: 1.338953\n",
      "(10 batch) loss: 1.327044\n",
      "(Epoch 6 / 100) train_acc: 0.410577; val_acc: 0.320000\n",
      "(0 batch) loss: 1.295899\n",
      "(1 batch) loss: 1.311416\n",
      "(2 batch) loss: 1.287202\n",
      "(3 batch) loss: 1.329352\n",
      "(4 batch) loss: 1.273310\n",
      "(5 batch) loss: 1.324868\n",
      "(6 batch) loss: 1.328966\n",
      "(7 batch) loss: 1.318887\n",
      "(8 batch) loss: 1.309965\n",
      "(9 batch) loss: 1.303604\n",
      "(10 batch) loss: 1.325230\n",
      "(Epoch 7 / 100) train_acc: 0.415385; val_acc: 0.300000\n",
      "(0 batch) loss: 1.264555\n",
      "(1 batch) loss: 1.289482\n",
      "(2 batch) loss: 1.281464\n",
      "(3 batch) loss: 1.251807\n",
      "(4 batch) loss: 1.274508\n",
      "(5 batch) loss: 1.265067\n",
      "(6 batch) loss: 1.289941\n",
      "(7 batch) loss: 1.277830\n",
      "(8 batch) loss: 1.328222\n",
      "(9 batch) loss: 1.278092\n",
      "(10 batch) loss: 1.292952\n",
      "(Epoch 8 / 100) train_acc: 0.449038; val_acc: 0.250000\n",
      "(0 batch) loss: 1.198500\n",
      "(1 batch) loss: 1.294897\n",
      "(2 batch) loss: 1.286393\n",
      "(3 batch) loss: 1.215147\n",
      "(4 batch) loss: 1.265644\n",
      "(5 batch) loss: 1.203029\n",
      "(6 batch) loss: 1.231832\n",
      "(7 batch) loss: 1.274871\n",
      "(8 batch) loss: 1.229805\n",
      "(9 batch) loss: 1.304723\n",
      "(10 batch) loss: 1.271596\n",
      "(Epoch 9 / 100) train_acc: 0.465385; val_acc: 0.370000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew/Desktop/Project/model_explore/.env/lib/python3.5/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type myGRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0 batch) loss: 1.152367\n",
      "(1 batch) loss: 1.201174\n",
      "(2 batch) loss: 1.263725\n",
      "(3 batch) loss: 1.264408\n",
      "(4 batch) loss: 1.243412\n",
      "(5 batch) loss: 1.257376\n",
      "(6 batch) loss: 1.184273\n",
      "(7 batch) loss: 1.253968\n",
      "(8 batch) loss: 1.231770\n",
      "(9 batch) loss: 1.254390\n",
      "(10 batch) loss: 1.203172\n",
      "(Epoch 10 / 100) train_acc: 0.481731; val_acc: 0.240000\n",
      "(0 batch) loss: 1.190406\n",
      "(1 batch) loss: 1.174992\n",
      "(2 batch) loss: 1.109456\n",
      "(3 batch) loss: 1.204846\n",
      "(4 batch) loss: 1.248220\n",
      "(5 batch) loss: 1.214810\n",
      "(6 batch) loss: 1.187058\n",
      "(7 batch) loss: 1.223312\n",
      "(8 batch) loss: 1.147130\n",
      "(9 batch) loss: 1.212725\n",
      "(10 batch) loss: 1.264375\n",
      "(Epoch 11 / 100) train_acc: 0.481250; val_acc: 0.260000\n",
      "(0 batch) loss: 1.175054\n",
      "(1 batch) loss: 1.210253\n",
      "(2 batch) loss: 1.219178\n",
      "(3 batch) loss: 1.156806\n",
      "(4 batch) loss: 1.127644\n",
      "(5 batch) loss: 1.123362\n",
      "(6 batch) loss: 1.183262\n",
      "(7 batch) loss: 1.196236\n",
      "(8 batch) loss: 1.226092\n",
      "(9 batch) loss: 1.201674\n",
      "(10 batch) loss: 1.184131\n",
      "(Epoch 12 / 100) train_acc: 0.542788; val_acc: 0.280000\n",
      "(0 batch) loss: 1.087133\n",
      "(1 batch) loss: 1.115434\n",
      "(2 batch) loss: 1.129731\n",
      "(3 batch) loss: 1.154501\n",
      "(4 batch) loss: 1.208497\n",
      "(5 batch) loss: 1.091962\n",
      "(6 batch) loss: 1.192412\n",
      "(7 batch) loss: 1.095220\n",
      "(8 batch) loss: 1.160751\n",
      "(9 batch) loss: 1.270510\n",
      "(10 batch) loss: 1.264130\n",
      "(Epoch 13 / 100) train_acc: 0.526442; val_acc: 0.300000\n",
      "(0 batch) loss: 1.102849\n",
      "(1 batch) loss: 1.091258\n",
      "(2 batch) loss: 1.094772\n",
      "(3 batch) loss: 1.169732\n",
      "(4 batch) loss: 1.134372\n",
      "(5 batch) loss: 1.197349\n",
      "(6 batch) loss: 1.117799\n",
      "(7 batch) loss: 1.163294\n",
      "(8 batch) loss: 1.154902\n",
      "(9 batch) loss: 1.142636\n",
      "(10 batch) loss: 1.238964\n",
      "(Epoch 14 / 100) train_acc: 0.514423; val_acc: 0.320000\n",
      "(0 batch) loss: 1.085881\n",
      "(1 batch) loss: 1.117735\n",
      "(2 batch) loss: 1.196650\n",
      "(3 batch) loss: 1.220859\n",
      "(4 batch) loss: 1.198095\n",
      "(5 batch) loss: 1.076342\n",
      "(6 batch) loss: 1.165075\n",
      "(7 batch) loss: 1.247434\n",
      "(8 batch) loss: 1.175554\n",
      "(9 batch) loss: 1.195291\n",
      "(10 batch) loss: 1.097297\n",
      "(Epoch 15 / 100) train_acc: 0.511058; val_acc: 0.300000\n",
      "(0 batch) loss: 1.087453\n",
      "(1 batch) loss: 1.119938\n",
      "(2 batch) loss: 1.157594\n",
      "(3 batch) loss: 1.218089\n",
      "(4 batch) loss: 1.148773\n",
      "(5 batch) loss: 1.210922\n",
      "(6 batch) loss: 1.200099\n",
      "(7 batch) loss: 1.188263\n",
      "(8 batch) loss: 1.123902\n",
      "(9 batch) loss: 1.172814\n",
      "(10 batch) loss: 1.276738\n",
      "(Epoch 16 / 100) train_acc: 0.525962; val_acc: 0.250000\n",
      "(0 batch) loss: 1.117817\n",
      "(1 batch) loss: 1.106284\n",
      "(2 batch) loss: 1.148706\n",
      "(3 batch) loss: 1.238591\n",
      "(4 batch) loss: 1.155723\n",
      "(5 batch) loss: 1.170433\n",
      "(6 batch) loss: 1.174532\n",
      "(7 batch) loss: 1.165926\n",
      "(8 batch) loss: 1.224414\n",
      "(9 batch) loss: 1.138158\n",
      "(10 batch) loss: 1.124019\n",
      "(Epoch 17 / 100) train_acc: 0.503365; val_acc: 0.290000\n",
      "(0 batch) loss: 1.099038\n",
      "(1 batch) loss: 1.179541\n",
      "(2 batch) loss: 1.197661\n",
      "(3 batch) loss: 1.113196\n",
      "(4 batch) loss: 1.126940\n",
      "(5 batch) loss: 1.175903\n",
      "(6 batch) loss: 1.115359\n",
      "(7 batch) loss: 1.180928\n",
      "(8 batch) loss: 1.143358\n",
      "(9 batch) loss: 1.221278\n",
      "(10 batch) loss: 1.312560\n",
      "(Epoch 18 / 100) train_acc: 0.558173; val_acc: 0.250000\n",
      "(0 batch) loss: 1.040629\n",
      "(1 batch) loss: 1.081047\n",
      "(2 batch) loss: 1.059577\n",
      "(3 batch) loss: 1.089616\n",
      "(4 batch) loss: 1.115221\n",
      "(5 batch) loss: 1.088155\n",
      "(6 batch) loss: 1.121042\n",
      "(7 batch) loss: 1.179797\n",
      "(8 batch) loss: 1.103733\n",
      "(9 batch) loss: 1.137504\n",
      "(10 batch) loss: 1.165177\n",
      "(Epoch 19 / 100) train_acc: 0.570673; val_acc: 0.260000\n",
      "(0 batch) loss: 1.058103\n",
      "(1 batch) loss: 1.038068\n",
      "(2 batch) loss: 1.076305\n",
      "(3 batch) loss: 1.139899\n",
      "(4 batch) loss: 1.098519\n",
      "(5 batch) loss: 1.050749\n",
      "(6 batch) loss: 1.076052\n",
      "(7 batch) loss: 1.153183\n",
      "(8 batch) loss: 1.100153\n",
      "(9 batch) loss: 1.106755\n",
      "(10 batch) loss: 1.228317\n",
      "(Epoch 20 / 100) train_acc: 0.573077; val_acc: 0.270000\n",
      "(0 batch) loss: 0.997878\n",
      "(1 batch) loss: 1.092063\n",
      "(2 batch) loss: 1.149956\n",
      "(3 batch) loss: 1.007721\n",
      "(4 batch) loss: 1.094914\n",
      "(5 batch) loss: 1.103019\n",
      "(6 batch) loss: 1.086452\n",
      "(7 batch) loss: 1.099865\n",
      "(8 batch) loss: 1.097068\n",
      "(9 batch) loss: 1.154208\n",
      "(10 batch) loss: 1.034185\n",
      "(Epoch 21 / 100) train_acc: 0.555769; val_acc: 0.230000\n",
      "(0 batch) loss: 1.107689\n",
      "(1 batch) loss: 1.039945\n",
      "(2 batch) loss: 1.057595\n",
      "(3 batch) loss: 1.050254\n",
      "(4 batch) loss: 1.098994\n",
      "(5 batch) loss: 1.066123\n",
      "(6 batch) loss: 1.164890\n",
      "(7 batch) loss: 1.133631\n",
      "(8 batch) loss: 1.127134\n",
      "(9 batch) loss: 1.138739\n",
      "(10 batch) loss: 1.125372\n",
      "(Epoch 22 / 100) train_acc: 0.570673; val_acc: 0.280000\n",
      "(0 batch) loss: 0.989524\n",
      "(1 batch) loss: 1.043630\n",
      "(2 batch) loss: 1.009978\n",
      "(3 batch) loss: 1.049348\n",
      "(4 batch) loss: 1.013872\n",
      "(5 batch) loss: 1.098641\n",
      "(6 batch) loss: 1.072348\n",
      "(7 batch) loss: 1.081975\n",
      "(8 batch) loss: 1.074043\n",
      "(9 batch) loss: 1.120939\n",
      "(10 batch) loss: 1.226949\n",
      "(Epoch 23 / 100) train_acc: 0.567308; val_acc: 0.210000\n",
      "(0 batch) loss: 1.098436\n",
      "(1 batch) loss: 1.072883\n",
      "(2 batch) loss: 1.117210\n",
      "(3 batch) loss: 1.030403\n",
      "(4 batch) loss: 1.035701\n",
      "(5 batch) loss: 0.973697\n",
      "(6 batch) loss: 1.065313\n",
      "(7 batch) loss: 1.002132\n",
      "(8 batch) loss: 1.069938\n",
      "(9 batch) loss: 1.015667\n",
      "(10 batch) loss: 1.008052\n",
      "(Epoch 24 / 100) train_acc: 0.615385; val_acc: 0.210000\n",
      "(0 batch) loss: 0.949976\n",
      "(1 batch) loss: 0.945143\n",
      "(2 batch) loss: 1.000969\n",
      "(3 batch) loss: 0.941634\n",
      "(4 batch) loss: 1.035009\n",
      "(5 batch) loss: 1.014719\n",
      "(6 batch) loss: 1.086756\n",
      "(7 batch) loss: 1.025195\n",
      "(8 batch) loss: 1.046971\n",
      "(9 batch) loss: 1.039320\n",
      "(10 batch) loss: 1.034681\n",
      "(Epoch 25 / 100) train_acc: 0.611538; val_acc: 0.360000\n",
      "(0 batch) loss: 0.951889\n",
      "(1 batch) loss: 0.991484\n",
      "(2 batch) loss: 1.082441\n",
      "(3 batch) loss: 1.042629\n",
      "(4 batch) loss: 1.058493\n",
      "(5 batch) loss: 0.954640\n",
      "(6 batch) loss: 1.062149\n",
      "(7 batch) loss: 1.079923\n",
      "(8 batch) loss: 1.097125\n",
      "(9 batch) loss: 1.100561\n",
      "(10 batch) loss: 1.206297\n",
      "(Epoch 26 / 100) train_acc: 0.569712; val_acc: 0.300000\n",
      "(0 batch) loss: 1.018714\n",
      "(1 batch) loss: 1.018920\n",
      "(2 batch) loss: 1.057179\n",
      "(3 batch) loss: 1.157562\n",
      "(4 batch) loss: 1.077369\n",
      "(5 batch) loss: 1.060013\n",
      "(6 batch) loss: 1.004489\n",
      "(7 batch) loss: 0.975816\n",
      "(8 batch) loss: 1.033462\n",
      "(9 batch) loss: 1.074820\n",
      "(10 batch) loss: 1.218408\n",
      "(Epoch 27 / 100) train_acc: 0.587019; val_acc: 0.290000\n",
      "(0 batch) loss: 1.011790\n",
      "(1 batch) loss: 0.962106\n",
      "(2 batch) loss: 1.014234\n",
      "(3 batch) loss: 1.019846\n",
      "(4 batch) loss: 1.098126\n",
      "(5 batch) loss: 1.021673\n",
      "(6 batch) loss: 0.989066\n",
      "(7 batch) loss: 1.012938\n",
      "(8 batch) loss: 1.049432\n",
      "(9 batch) loss: 0.999672\n",
      "(10 batch) loss: 1.030058\n",
      "(Epoch 28 / 100) train_acc: 0.604327; val_acc: 0.250000\n",
      "(0 batch) loss: 0.984959\n",
      "(1 batch) loss: 0.988198\n",
      "(2 batch) loss: 1.010291\n",
      "(3 batch) loss: 1.022287\n",
      "(4 batch) loss: 0.973649\n",
      "(5 batch) loss: 0.948814\n",
      "(6 batch) loss: 0.990210\n",
      "(7 batch) loss: 1.014126\n",
      "(8 batch) loss: 1.104375\n",
      "(9 batch) loss: 1.091074\n",
      "(10 batch) loss: 1.097470\n",
      "(Epoch 29 / 100) train_acc: 0.590865; val_acc: 0.330000\n",
      "(0 batch) loss: 0.959047\n",
      "(1 batch) loss: 1.070215\n",
      "(2 batch) loss: 0.958346\n",
      "(3 batch) loss: 1.021921\n",
      "(4 batch) loss: 1.041563\n",
      "(5 batch) loss: 1.011612\n",
      "(6 batch) loss: 1.032414\n",
      "(7 batch) loss: 1.005348\n",
      "(8 batch) loss: 1.079792\n",
      "(9 batch) loss: 1.077021\n",
      "(10 batch) loss: 1.105470\n",
      "(Epoch 30 / 100) train_acc: 0.607692; val_acc: 0.250000\n",
      "(0 batch) loss: 0.989570\n",
      "(1 batch) loss: 0.951082\n",
      "(2 batch) loss: 0.964958\n",
      "(3 batch) loss: 0.953105\n",
      "(4 batch) loss: 1.015646\n",
      "(5 batch) loss: 0.996635\n",
      "(6 batch) loss: 1.003186\n",
      "(7 batch) loss: 0.923794\n",
      "(8 batch) loss: 1.095166\n",
      "(9 batch) loss: 1.028726\n",
      "(10 batch) loss: 1.182341\n",
      "(Epoch 31 / 100) train_acc: 0.614423; val_acc: 0.270000\n",
      "(0 batch) loss: 0.975463\n",
      "(1 batch) loss: 0.898645\n",
      "(2 batch) loss: 0.899169\n",
      "(3 batch) loss: 0.947344\n",
      "(4 batch) loss: 1.012790\n",
      "(5 batch) loss: 0.967040\n",
      "(6 batch) loss: 1.059203\n",
      "(7 batch) loss: 1.057289\n",
      "(8 batch) loss: 0.970839\n",
      "(9 batch) loss: 1.022656\n",
      "(10 batch) loss: 0.866014\n",
      "(Epoch 32 / 100) train_acc: 0.617788; val_acc: 0.310000\n",
      "(0 batch) loss: 0.903262\n",
      "(1 batch) loss: 0.983732\n",
      "(2 batch) loss: 0.920955\n",
      "(3 batch) loss: 1.045707\n",
      "(4 batch) loss: 0.972120\n",
      "(5 batch) loss: 0.882245\n",
      "(6 batch) loss: 1.013540\n",
      "(7 batch) loss: 1.041217\n",
      "(8 batch) loss: 0.970412\n",
      "(9 batch) loss: 0.911313\n",
      "(10 batch) loss: 1.069098\n",
      "(Epoch 33 / 100) train_acc: 0.657692; val_acc: 0.290000\n",
      "(0 batch) loss: 0.925073\n",
      "(1 batch) loss: 0.863396\n",
      "(2 batch) loss: 0.976809\n",
      "(3 batch) loss: 0.948072\n",
      "(4 batch) loss: 0.930718\n",
      "(5 batch) loss: 0.972851\n",
      "(6 batch) loss: 0.810415\n",
      "(7 batch) loss: 0.992372\n",
      "(8 batch) loss: 0.892425\n",
      "(9 batch) loss: 0.872965\n",
      "(10 batch) loss: 0.903782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 34 / 100) train_acc: 0.664423; val_acc: 0.320000\n",
      "(0 batch) loss: 0.863215\n",
      "(1 batch) loss: 0.878012\n",
      "(2 batch) loss: 0.820602\n",
      "(3 batch) loss: 0.839150\n",
      "(4 batch) loss: 0.880896\n",
      "(5 batch) loss: 0.898408\n",
      "(6 batch) loss: 0.968772\n",
      "(7 batch) loss: 0.923268\n",
      "(8 batch) loss: 0.909196\n",
      "(9 batch) loss: 0.910115\n",
      "(10 batch) loss: 1.034934\n",
      "(Epoch 35 / 100) train_acc: 0.659135; val_acc: 0.330000\n",
      "(0 batch) loss: 0.916872\n",
      "(1 batch) loss: 0.807903\n",
      "(2 batch) loss: 0.813681\n",
      "(3 batch) loss: 0.950817\n",
      "(4 batch) loss: 0.907225\n",
      "(5 batch) loss: 0.788461\n",
      "(6 batch) loss: 0.941141\n",
      "(7 batch) loss: 0.909570\n",
      "(8 batch) loss: 0.894880\n",
      "(9 batch) loss: 1.022481\n",
      "(10 batch) loss: 1.062752\n",
      "(Epoch 36 / 100) train_acc: 0.653846; val_acc: 0.300000\n",
      "(0 batch) loss: 0.969594\n",
      "(1 batch) loss: 0.806366\n",
      "(2 batch) loss: 1.017641\n",
      "(3 batch) loss: 0.952178\n",
      "(4 batch) loss: 0.922520\n",
      "(5 batch) loss: 0.893696\n",
      "(6 batch) loss: 1.011737\n",
      "(7 batch) loss: 1.020333\n",
      "(8 batch) loss: 0.963595\n",
      "(9 batch) loss: 0.847178\n",
      "(10 batch) loss: 0.857101\n",
      "(Epoch 37 / 100) train_acc: 0.659135; val_acc: 0.360000\n",
      "(0 batch) loss: 0.881473\n",
      "(1 batch) loss: 0.776988\n",
      "(2 batch) loss: 0.809451\n",
      "(3 batch) loss: 1.016567\n",
      "(4 batch) loss: 0.867944\n",
      "(5 batch) loss: 0.925115\n",
      "(6 batch) loss: 0.925974\n",
      "(7 batch) loss: 1.061216\n",
      "(8 batch) loss: 0.983683\n",
      "(9 batch) loss: 1.026922\n",
      "(10 batch) loss: 1.010351\n",
      "(Epoch 38 / 100) train_acc: 0.639423; val_acc: 0.250000\n",
      "(0 batch) loss: 0.964226\n",
      "(1 batch) loss: 0.922701\n",
      "(2 batch) loss: 0.932925\n",
      "(3 batch) loss: 1.028482\n",
      "(4 batch) loss: 1.000655\n",
      "(5 batch) loss: 1.038824\n",
      "(6 batch) loss: 0.995347\n",
      "(7 batch) loss: 0.934524\n",
      "(8 batch) loss: 0.947931\n",
      "(9 batch) loss: 0.988153\n",
      "(10 batch) loss: 1.024715\n",
      "(Epoch 39 / 100) train_acc: 0.632212; val_acc: 0.270000\n",
      "(0 batch) loss: 0.925839\n",
      "(1 batch) loss: 0.974892\n",
      "(2 batch) loss: 0.875124\n",
      "(3 batch) loss: 0.882748\n",
      "(4 batch) loss: 0.867042\n",
      "(5 batch) loss: 0.933802\n",
      "(6 batch) loss: 0.889938\n",
      "(7 batch) loss: 0.909722\n",
      "(8 batch) loss: 1.054705\n",
      "(9 batch) loss: 0.906214\n",
      "(10 batch) loss: 0.900901\n",
      "(Epoch 40 / 100) train_acc: 0.669712; val_acc: 0.210000\n",
      "(0 batch) loss: 0.825182\n",
      "(1 batch) loss: 0.817902\n",
      "(2 batch) loss: 0.947604\n",
      "(3 batch) loss: 0.842612\n",
      "(4 batch) loss: 0.805001\n",
      "(5 batch) loss: 0.904879\n",
      "(6 batch) loss: 0.785384\n",
      "(7 batch) loss: 1.024959\n",
      "(8 batch) loss: 0.883027\n",
      "(9 batch) loss: 0.957824\n",
      "(10 batch) loss: 0.901371\n",
      "(Epoch 41 / 100) train_acc: 0.700000; val_acc: 0.220000\n",
      "(0 batch) loss: 0.838539\n",
      "(1 batch) loss: 0.812364\n",
      "(2 batch) loss: 0.850523\n",
      "(3 batch) loss: 0.744093\n",
      "(4 batch) loss: 0.787660\n",
      "(5 batch) loss: 0.804782\n",
      "(6 batch) loss: 0.826379\n",
      "(7 batch) loss: 0.872528\n",
      "(8 batch) loss: 0.827727\n",
      "(9 batch) loss: 0.912842\n",
      "(10 batch) loss: 0.940843\n",
      "(Epoch 42 / 100) train_acc: 0.708173; val_acc: 0.240000\n",
      "(0 batch) loss: 0.910585\n",
      "(1 batch) loss: 0.777271\n",
      "(2 batch) loss: 0.793131\n",
      "(3 batch) loss: 0.795918\n",
      "(4 batch) loss: 0.719771\n",
      "(5 batch) loss: 0.839054\n",
      "(6 batch) loss: 0.865012\n",
      "(7 batch) loss: 0.795463\n",
      "(8 batch) loss: 0.885313\n",
      "(9 batch) loss: 1.003001\n",
      "(10 batch) loss: 0.904874\n",
      "(Epoch 43 / 100) train_acc: 0.690865; val_acc: 0.290000\n",
      "(0 batch) loss: 0.827385\n",
      "(1 batch) loss: 0.839634\n",
      "(2 batch) loss: 0.893996\n",
      "(3 batch) loss: 0.811906\n",
      "(4 batch) loss: 0.934037\n",
      "(5 batch) loss: 1.007219\n",
      "(6 batch) loss: 0.847863\n",
      "(7 batch) loss: 0.918065\n",
      "(8 batch) loss: 0.897116\n",
      "(9 batch) loss: 0.973866\n",
      "(10 batch) loss: 0.797230\n",
      "(Epoch 44 / 100) train_acc: 0.634135; val_acc: 0.280000\n",
      "(0 batch) loss: 0.891983\n",
      "(1 batch) loss: 0.975114\n",
      "(2 batch) loss: 0.911467\n",
      "(3 batch) loss: 0.988910\n",
      "(4 batch) loss: 0.824456\n",
      "(5 batch) loss: 0.926225\n",
      "(6 batch) loss: 1.015746\n",
      "(7 batch) loss: 0.907646\n",
      "(8 batch) loss: 1.087095\n",
      "(9 batch) loss: 1.028828\n",
      "(10 batch) loss: 0.895664\n",
      "(Epoch 45 / 100) train_acc: 0.624038; val_acc: 0.320000\n",
      "(0 batch) loss: 0.995081\n",
      "(1 batch) loss: 0.933922\n",
      "(2 batch) loss: 0.928674\n",
      "(3 batch) loss: 0.897036\n",
      "(4 batch) loss: 0.901735\n",
      "(5 batch) loss: 0.944689\n",
      "(6 batch) loss: 0.991514\n",
      "(7 batch) loss: 1.001414\n",
      "(8 batch) loss: 1.071958\n",
      "(9 batch) loss: 0.909214\n",
      "(10 batch) loss: 0.929329\n",
      "(Epoch 46 / 100) train_acc: 0.627404; val_acc: 0.300000\n",
      "(0 batch) loss: 0.844836\n",
      "(1 batch) loss: 0.916139\n",
      "(2 batch) loss: 0.935704\n",
      "(3 batch) loss: 1.016126\n",
      "(4 batch) loss: 0.889253\n",
      "(5 batch) loss: 0.942195\n",
      "(6 batch) loss: 0.918665\n",
      "(7 batch) loss: 1.015274\n",
      "(8 batch) loss: 0.946013\n",
      "(9 batch) loss: 0.913536\n",
      "(10 batch) loss: 1.218193\n",
      "(Epoch 47 / 100) train_acc: 0.626923; val_acc: 0.230000\n",
      "(0 batch) loss: 0.934607\n",
      "(1 batch) loss: 0.985387\n",
      "(2 batch) loss: 0.987411\n",
      "(3 batch) loss: 0.943574\n",
      "(4 batch) loss: 0.883608\n",
      "(5 batch) loss: 0.925766\n",
      "(6 batch) loss: 0.905273\n",
      "(7 batch) loss: 1.033911\n",
      "(8 batch) loss: 0.951223\n",
      "(9 batch) loss: 0.832622\n",
      "(10 batch) loss: 0.982662\n",
      "(Epoch 48 / 100) train_acc: 0.661538; val_acc: 0.340000\n",
      "(0 batch) loss: 0.822765\n",
      "(1 batch) loss: 0.878881\n",
      "(2 batch) loss: 0.827021\n",
      "(3 batch) loss: 0.923751\n",
      "(4 batch) loss: 0.880600\n",
      "(5 batch) loss: 0.963924\n",
      "(6 batch) loss: 0.875067\n",
      "(7 batch) loss: 0.903501\n",
      "(8 batch) loss: 0.897209\n",
      "(9 batch) loss: 0.975706\n",
      "(10 batch) loss: 0.913298\n",
      "(Epoch 49 / 100) train_acc: 0.645673; val_acc: 0.280000\n",
      "(0 batch) loss: 0.837266\n",
      "(1 batch) loss: 0.939295\n",
      "(2 batch) loss: 0.938009\n",
      "(3 batch) loss: 0.923599\n",
      "(4 batch) loss: 1.021225\n",
      "(5 batch) loss: 0.915200\n",
      "(6 batch) loss: 0.882057\n",
      "(7 batch) loss: 0.963937\n",
      "(8 batch) loss: 0.958388\n",
      "(9 batch) loss: 0.881499\n",
      "(10 batch) loss: 1.049063\n",
      "(Epoch 50 / 100) train_acc: 0.628846; val_acc: 0.260000\n",
      "(0 batch) loss: 0.871074\n",
      "(1 batch) loss: 0.913755\n",
      "(2 batch) loss: 0.918576\n",
      "(3 batch) loss: 0.954937\n",
      "(4 batch) loss: 0.957246\n",
      "(5 batch) loss: 0.913178\n",
      "(6 batch) loss: 0.902963\n",
      "(7 batch) loss: 0.869482\n",
      "(8 batch) loss: 1.041795\n",
      "(9 batch) loss: 0.950700\n",
      "(10 batch) loss: 1.033737\n",
      "(Epoch 51 / 100) train_acc: 0.638462; val_acc: 0.290000\n",
      "(0 batch) loss: 0.887223\n",
      "(1 batch) loss: 0.897338\n",
      "(2 batch) loss: 0.876888\n",
      "(3 batch) loss: 0.802428\n",
      "(4 batch) loss: 0.919452\n",
      "(5 batch) loss: 0.907739\n",
      "(6 batch) loss: 0.884878\n",
      "(7 batch) loss: 0.863370\n",
      "(8 batch) loss: 0.940703\n",
      "(9 batch) loss: 0.956180\n",
      "(10 batch) loss: 1.010980\n",
      "(Epoch 52 / 100) train_acc: 0.670673; val_acc: 0.330000\n",
      "(0 batch) loss: 0.813272\n",
      "(1 batch) loss: 0.796610\n",
      "(2 batch) loss: 0.929141\n",
      "(3 batch) loss: 0.846127\n",
      "(4 batch) loss: 0.878254\n",
      "(5 batch) loss: 0.964174\n",
      "(6 batch) loss: 0.909953\n",
      "(7 batch) loss: 0.885310\n",
      "(8 batch) loss: 0.901974\n",
      "(9 batch) loss: 0.867587\n",
      "(10 batch) loss: 1.052801\n",
      "(Epoch 53 / 100) train_acc: 0.639423; val_acc: 0.300000\n",
      "(0 batch) loss: 0.794805\n",
      "(1 batch) loss: 1.031833\n",
      "(2 batch) loss: 0.955358\n",
      "(3 batch) loss: 1.007770\n",
      "(4 batch) loss: 0.905327\n",
      "(5 batch) loss: 1.018015\n",
      "(6 batch) loss: 0.863201\n",
      "(7 batch) loss: 1.055284\n",
      "(8 batch) loss: 1.039144\n",
      "(9 batch) loss: 1.007574\n",
      "(10 batch) loss: 0.958625\n",
      "(Epoch 54 / 100) train_acc: 0.615385; val_acc: 0.270000\n",
      "(0 batch) loss: 0.917694\n",
      "(1 batch) loss: 0.906461\n",
      "(2 batch) loss: 0.957265\n",
      "(3 batch) loss: 0.825057\n",
      "(4 batch) loss: 0.945581\n",
      "(5 batch) loss: 0.925361\n",
      "(6 batch) loss: 0.956423\n",
      "(7 batch) loss: 0.975133\n",
      "(8 batch) loss: 1.017041\n",
      "(9 batch) loss: 1.012164\n",
      "(10 batch) loss: 0.955219\n",
      "(Epoch 55 / 100) train_acc: 0.642308; val_acc: 0.290000\n",
      "(0 batch) loss: 0.926717\n",
      "(1 batch) loss: 0.809089\n",
      "(2 batch) loss: 0.831916\n",
      "(3 batch) loss: 0.909390\n",
      "(4 batch) loss: 0.945209\n",
      "(5 batch) loss: 0.988727\n",
      "(6 batch) loss: 0.869110\n",
      "(7 batch) loss: 0.850883\n",
      "(8 batch) loss: 1.058513\n",
      "(9 batch) loss: 0.949219\n",
      "(10 batch) loss: 1.038099\n",
      "(Epoch 56 / 100) train_acc: 0.637019; val_acc: 0.290000\n",
      "(0 batch) loss: 0.959476\n",
      "(1 batch) loss: 0.874532\n",
      "(2 batch) loss: 0.956539\n",
      "(3 batch) loss: 0.947238\n",
      "(4 batch) loss: 0.872938\n",
      "(5 batch) loss: 0.868775\n",
      "(6 batch) loss: 0.894837\n",
      "(7 batch) loss: 0.902579\n",
      "(8 batch) loss: 0.911209\n",
      "(9 batch) loss: 0.955895\n",
      "(10 batch) loss: 0.905954\n",
      "(Epoch 57 / 100) train_acc: 0.653365; val_acc: 0.270000\n",
      "(0 batch) loss: 0.865595\n",
      "(1 batch) loss: 0.807210\n",
      "(2 batch) loss: 0.884736\n",
      "(3 batch) loss: 0.978303\n",
      "(4 batch) loss: 1.018985\n",
      "(5 batch) loss: 0.949072\n",
      "(6 batch) loss: 0.910199\n",
      "(7 batch) loss: 1.049989\n",
      "(8 batch) loss: 0.980628\n",
      "(9 batch) loss: 0.903767\n",
      "(10 batch) loss: 0.993898\n",
      "(Epoch 58 / 100) train_acc: 0.648077; val_acc: 0.240000\n",
      "(0 batch) loss: 0.878713\n",
      "(1 batch) loss: 0.843531\n",
      "(2 batch) loss: 0.787674\n",
      "(3 batch) loss: 0.938216\n",
      "(4 batch) loss: 0.851905\n",
      "(5 batch) loss: 0.894785\n",
      "(6 batch) loss: 0.955166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7 batch) loss: 1.000154\n",
      "(8 batch) loss: 0.899248\n",
      "(9 batch) loss: 0.933547\n",
      "(10 batch) loss: 0.875632\n",
      "(Epoch 59 / 100) train_acc: 0.667788; val_acc: 0.250000\n",
      "(0 batch) loss: 0.894687\n",
      "(1 batch) loss: 0.781886\n",
      "(2 batch) loss: 0.908883\n",
      "(3 batch) loss: 0.936447\n",
      "(4 batch) loss: 0.858201\n",
      "(5 batch) loss: 0.937011\n",
      "(6 batch) loss: 0.897885\n",
      "(7 batch) loss: 1.070326\n",
      "(8 batch) loss: 0.911178\n",
      "(9 batch) loss: 0.937916\n",
      "(10 batch) loss: 0.882809\n",
      "(Epoch 60 / 100) train_acc: 0.646154; val_acc: 0.230000\n",
      "(0 batch) loss: 0.802470\n",
      "(1 batch) loss: 0.831645\n",
      "(2 batch) loss: 0.866271\n",
      "(3 batch) loss: 0.960139\n",
      "(4 batch) loss: 0.881583\n",
      "(5 batch) loss: 0.929146\n",
      "(6 batch) loss: 0.950199\n",
      "(7 batch) loss: 0.901900\n",
      "(8 batch) loss: 0.965763\n",
      "(9 batch) loss: 1.003978\n",
      "(10 batch) loss: 1.004749\n",
      "(Epoch 61 / 100) train_acc: 0.629327; val_acc: 0.270000\n",
      "(0 batch) loss: 0.919446\n",
      "(1 batch) loss: 0.849333\n",
      "(2 batch) loss: 0.871869\n",
      "(3 batch) loss: 0.879656\n",
      "(4 batch) loss: 0.936000\n",
      "(5 batch) loss: 0.948588\n",
      "(6 batch) loss: 0.995162\n",
      "(7 batch) loss: 0.971827\n",
      "(8 batch) loss: 0.878980\n",
      "(9 batch) loss: 1.094374\n",
      "(10 batch) loss: 0.907679\n",
      "(Epoch 62 / 100) train_acc: 0.667308; val_acc: 0.310000\n",
      "(0 batch) loss: 0.956831\n",
      "(1 batch) loss: 0.847871\n",
      "(2 batch) loss: 0.859399\n",
      "(3 batch) loss: 0.876572\n",
      "(4 batch) loss: 0.835989\n",
      "(5 batch) loss: 0.842808\n",
      "(6 batch) loss: 0.793924\n",
      "(7 batch) loss: 0.820162\n",
      "(8 batch) loss: 0.921114\n",
      "(9 batch) loss: 0.834830\n",
      "(10 batch) loss: 0.834483\n",
      "(Epoch 63 / 100) train_acc: 0.668750; val_acc: 0.250000\n",
      "(0 batch) loss: 0.832467\n",
      "(1 batch) loss: 0.809758\n",
      "(2 batch) loss: 0.803807\n",
      "(3 batch) loss: 0.843474\n",
      "(4 batch) loss: 0.841652\n",
      "(5 batch) loss: 0.862852\n",
      "(6 batch) loss: 0.834732\n",
      "(7 batch) loss: 0.879927\n",
      "(8 batch) loss: 0.814075\n",
      "(9 batch) loss: 0.928269\n",
      "(10 batch) loss: 0.883248\n",
      "(Epoch 64 / 100) train_acc: 0.684615; val_acc: 0.260000\n",
      "(0 batch) loss: 0.917601\n",
      "(1 batch) loss: 0.826124\n",
      "(2 batch) loss: 0.824789\n",
      "(3 batch) loss: 0.760835\n",
      "(4 batch) loss: 0.840548\n",
      "(5 batch) loss: 0.830979\n",
      "(6 batch) loss: 0.911921\n",
      "(7 batch) loss: 0.895509\n",
      "(8 batch) loss: 0.895643\n",
      "(9 batch) loss: 0.781510\n",
      "(10 batch) loss: 0.759159\n",
      "(Epoch 65 / 100) train_acc: 0.695673; val_acc: 0.260000\n",
      "(0 batch) loss: 0.723331\n",
      "(1 batch) loss: 0.767801\n",
      "(2 batch) loss: 0.824321\n",
      "(3 batch) loss: 0.877893\n",
      "(4 batch) loss: 0.791211\n",
      "(5 batch) loss: 0.817554\n",
      "(6 batch) loss: 0.910493\n",
      "(7 batch) loss: 0.886891\n",
      "(8 batch) loss: 0.802310\n",
      "(9 batch) loss: 0.910937\n",
      "(10 batch) loss: 0.712369\n",
      "(Epoch 66 / 100) train_acc: 0.679327; val_acc: 0.230000\n",
      "(0 batch) loss: 0.807657\n",
      "(1 batch) loss: 0.723100\n",
      "(2 batch) loss: 0.804237\n",
      "(3 batch) loss: 0.822186\n",
      "(4 batch) loss: 0.838881\n",
      "(5 batch) loss: 0.858507\n",
      "(6 batch) loss: 0.734140\n",
      "(7 batch) loss: 0.851353\n",
      "(8 batch) loss: 0.783814\n",
      "(9 batch) loss: 0.860590\n",
      "(10 batch) loss: 0.781775\n",
      "(Epoch 67 / 100) train_acc: 0.698077; val_acc: 0.230000\n",
      "(0 batch) loss: 0.733202\n",
      "(1 batch) loss: 0.880126\n",
      "(2 batch) loss: 0.702426\n",
      "(3 batch) loss: 0.727169\n",
      "(4 batch) loss: 0.732210\n",
      "(5 batch) loss: 0.772445\n",
      "(6 batch) loss: 0.775024\n",
      "(7 batch) loss: 0.814193\n",
      "(8 batch) loss: 0.826014\n",
      "(9 batch) loss: 0.833186\n",
      "(10 batch) loss: 0.843141\n",
      "(Epoch 68 / 100) train_acc: 0.709615; val_acc: 0.250000\n",
      "(0 batch) loss: 0.742263\n",
      "(1 batch) loss: 0.864941\n",
      "(2 batch) loss: 0.911543\n",
      "(3 batch) loss: 0.848598\n",
      "(4 batch) loss: 0.776459\n",
      "(5 batch) loss: 0.698101\n",
      "(6 batch) loss: 0.772211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-248:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/andrew/Desktop/Project/model_explore/.env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7 batch) loss: 0.846874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x7fbb0499f630>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/andrew/Desktop/Project/model_explore/.env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 333, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/andrew/Desktop/Project/model_explore/.env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 319, in _shutdown_workers\n",
      "    self.data_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 345, in get\n",
      "    return ForkingPickler.loads(res)\n",
      "  File \"/home/andrew/Desktop/Project/model_explore/.env/lib/python3.5/site-packages/torch/multiprocessing/reductions.py\", line 70, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 487, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 614, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-859a4f2b1171>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# print (out)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'(%d batch) loss: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# zero the parameter gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Project/model_explore/.env/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mCudaTransfer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Project/model_explore/.env/lib/python3.5/site-packages/torch/autograd/_functions/tensor.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, i, device, async)\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Project/model_explore/.env/lib/python3.5/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epoches):\n",
    "    for i, data in enumerate(dataloaders['train'], 0):\n",
    "        X_train, y_train = data\n",
    "        # Wrap them in Variable\n",
    "        X_train, y_train = Variable(X_train), Variable(y_train)\n",
    "        # forward + backward + optimize\n",
    "        out = model(X_train.cuda())\n",
    "        # print (out)\n",
    "        loss = loss_fn(out, y_train.long().cuda())\n",
    "        print('(%d batch) loss: %f' % (i, loss))\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_acc = model.check_accuracy(dataloaders['train'])\n",
    "    val_acc = model.check_accuracy(dataloaders['val'])\n",
    "    print('(Epoch %d / %d) train_acc: %f; val_acc: %f' % (epoch+1, num_epoches, train_acc, val_acc))\n",
    "    if (val_acc > best_acc):\n",
    "        best_acc = val_acc\n",
    "        torch.save(model, 'best_CHRONET.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = torch.load('best_CHRONET.pt')\n",
    "print (best_model.check_accuracy(dataloaders['val']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
