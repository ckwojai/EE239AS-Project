{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew/Desktop/Project/model_explore/.env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data.dataset import random_split\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/markdown"
   },
   "source": [
    "# Helper Clases / Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_Data(num):\n",
    "    if (num == -1): # All data\n",
    "        X_all = []\n",
    "        y_all = []\n",
    "        for i in range(8):\n",
    "            file_path = './../project_datasets/A0' + str(i+1) + 'T_slice.mat'\n",
    "            data = h5py.File(file_path, 'r')\n",
    "            X = np.copy(data['image'])\n",
    "            y = np.copy(data['type'])\n",
    "            X = X[:, 0:23, :]\n",
    "            X_all.append(X)\n",
    "            y = y[0,0:X.shape[0]:1]\n",
    "            y_all.append(y)\n",
    "        A, N, E, T = np.shape(X_all)\n",
    "        X_all = np.reshape(X_all, (A*N, E, T))\n",
    "        y_all = np.reshape(y_all, (-1))\n",
    "        y_all = y_all - 769\n",
    "        ## Remove NAN\n",
    "        index_Nan = []\n",
    "        for i in range(A*N):\n",
    "            for j in range(E):\n",
    "                if (any(np.isnan(X_all[i,j])) == True):\n",
    "                    index_Nan.append(i)\n",
    "        index_Nan = list(set(index_Nan))\n",
    "        X_all = np.delete(X_all, index_Nan, axis=0)\n",
    "        y_all = np.delete(y_all, index_Nan)\n",
    "        return (X_all, y_all)\n",
    "    else:\n",
    "        file_path = './../project_datasets/A0' + str(num) + 'T_slice.mat'\n",
    "        data = h5py.File(file_path, 'r')\n",
    "        X = np.copy(data['image'])\n",
    "        y = np.copy(data['type'])\n",
    "        X = X[:, 0:23, :]\n",
    "        y = y[0,0:X.shape[0]:1]\n",
    "        y = y - 769\n",
    "         ## Remove NAN\n",
    "        N, E, T = np.shape(X)\n",
    "        index_Nan = []\n",
    "        for i in range(N):\n",
    "            for j in range(E):\n",
    "                if (any(np.isnan(X[i,j])) == True):\n",
    "                    index_Nan.append(i)\n",
    "        index_Nan = list(set(index_Nan))\n",
    "        X = np.delete(X, index_Nan, axis=0)\n",
    "        y = np.delete(y, index_Nan)\n",
    "        return (X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2280, 23, 1000)\n"
     ]
    }
   ],
   "source": [
    "X, y = Load_Data(-1) # -1 to load all datas\n",
    "N, E, T = np.shape(X)\n",
    "print (np.shape(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_train = 200\n",
    "bs_val = 100\n",
    "bs_test = 100\n",
    "data = data_utils.TensorDataset(torch.Tensor(X), torch.Tensor(y))\n",
    "dset = {}\n",
    "dataloaders = {}\n",
    "dset['train'], dset['val'], dset['test'] = random_split(data, [N-bs_val-bs_test, bs_val, bs_test])\n",
    "dataloaders['train'] = data_utils.DataLoader(dset['train'], batch_size=bs_train, shuffle=True, num_workers=1)\n",
    "dataloaders['val'] = data_utils.DataLoader(dset['val'], batch_size=bs_val, shuffle=True, num_workers=1)\n",
    "dataloaders['test'] = data_utils.DataLoader(dset['test'], batch_size=bs_test, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myConv(nn.Module):\n",
    "    def __init__(self, num_class):\n",
    "        super(myConv, self).__init__()\n",
    "        self.conv_temp = nn.Conv2d(1,40,tuple([1,25]))\n",
    "        self.conv_elec = nn.Conv3d(1,40,tuple([40, 23, 1]))\n",
    "        self.pool = nn.AvgPool2d(tuple([1,47]))\n",
    "        self.classifier = nn.Linear(40*20, num_class)\n",
    "    def forward(self, x):\n",
    "        N, H, W = x.size()\n",
    "        x.unsqueeze_(1)\n",
    "        out_conv_temp = self.conv_temp(x)\n",
    "        out_conv_temp = out_conv_temp.unsqueeze_(1)\n",
    "        out_conv_elec = self.conv_elec(out_conv_temp)\n",
    "        out_conv_elec = torch.squeeze(out_conv_elec) # shape: [N, 40, 976]\n",
    "        out_conv_elec.unsqueeze_(1)\n",
    "        out_pool = self.pool(out_conv_elec) \n",
    "        out_pool = torch.squeeze(out_pool) # shape: [N, 40, 20]\n",
    "        out_pool = out_pool.view(N, -1) # shape: [N, 800]\n",
    "        out = self.classifier(out_pool)\n",
    "        return out\n",
    "    def check_accuracy(self, dataloader):\n",
    "        total_correct = 0\n",
    "        total_label = 0\n",
    "        for i_batch, sample_batched in enumerate(dataloader):\n",
    "            X_sample, y_sample = sample_batched\n",
    "            X_sample, y_sample = Variable(X_sample), Variable(y_sample)\n",
    "            out = self.forward(X_sample.cuda())\n",
    "            _, pred = torch.max(out, 1)\n",
    "            num_correct = np.sum(pred.data.cpu().numpy() == y_sample.data.cpu().numpy())\n",
    "            total_correct += num_correct\n",
    "            total_label += len(pred)\n",
    "        return  total_correct / total_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.cuda.FloatTensor\n",
    "num_classes = 4\n",
    "num_epoches = 100\n",
    "model = myConv(num_classes)\n",
    "model.type(dtype)\n",
    "loss_fn = nn.CrossEntropyLoss().type(dtype)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0 batch) loss: 1.681994\n",
      "(1 batch) loss: 1.741327\n",
      "(2 batch) loss: 1.476816\n",
      "(3 batch) loss: 1.490850\n",
      "(4 batch) loss: 1.450372\n",
      "(5 batch) loss: 1.467518\n",
      "(6 batch) loss: 1.280683\n",
      "(7 batch) loss: 1.316637\n",
      "(8 batch) loss: 1.334978\n",
      "(9 batch) loss: 1.406701\n",
      "(10 batch) loss: 1.278842\n",
      "(Epoch 1 / 100) train_acc: 0.481250; val_acc: 0.420000\n",
      "(0 batch) loss: 1.216806\n",
      "(1 batch) loss: 1.124170\n",
      "(2 batch) loss: 1.207967\n",
      "(3 batch) loss: 1.136848\n",
      "(4 batch) loss: 1.125348\n",
      "(5 batch) loss: 1.079948\n",
      "(6 batch) loss: 1.139096\n",
      "(7 batch) loss: 1.120370\n",
      "(8 batch) loss: 1.204879\n",
      "(9 batch) loss: 1.114526\n",
      "(10 batch) loss: 1.051921\n",
      "(Epoch 2 / 100) train_acc: 0.573077; val_acc: 0.540000\n",
      "(0 batch) loss: 1.112021\n",
      "(1 batch) loss: 1.051784\n",
      "(2 batch) loss: 0.960751\n",
      "(3 batch) loss: 0.957773\n",
      "(4 batch) loss: 0.992380\n",
      "(5 batch) loss: 0.987003\n",
      "(6 batch) loss: 0.966693\n",
      "(7 batch) loss: 1.061503\n",
      "(8 batch) loss: 0.951873\n",
      "(9 batch) loss: 1.016487\n",
      "(10 batch) loss: 0.934715\n",
      "(Epoch 3 / 100) train_acc: 0.615385; val_acc: 0.620000\n",
      "(0 batch) loss: 1.015584\n",
      "(1 batch) loss: 0.876205\n",
      "(2 batch) loss: 0.900151\n",
      "(3 batch) loss: 0.908475\n",
      "(4 batch) loss: 0.972950\n",
      "(5 batch) loss: 0.950814\n",
      "(6 batch) loss: 0.951314\n",
      "(7 batch) loss: 1.006198\n",
      "(8 batch) loss: 0.936996\n",
      "(9 batch) loss: 0.908134\n",
      "(10 batch) loss: 0.881341\n",
      "(Epoch 4 / 100) train_acc: 0.653365; val_acc: 0.630000\n",
      "(0 batch) loss: 0.908506\n",
      "(1 batch) loss: 0.958392\n",
      "(2 batch) loss: 0.933804\n",
      "(3 batch) loss: 0.784595\n",
      "(4 batch) loss: 0.918518\n",
      "(5 batch) loss: 0.931844\n",
      "(6 batch) loss: 0.856674\n",
      "(7 batch) loss: 0.871892\n",
      "(8 batch) loss: 0.791232\n",
      "(9 batch) loss: 0.864127\n",
      "(10 batch) loss: 0.883158\n",
      "(Epoch 5 / 100) train_acc: 0.692788; val_acc: 0.620000\n",
      "(0 batch) loss: 0.831647\n",
      "(1 batch) loss: 0.717417\n",
      "(2 batch) loss: 0.796483\n",
      "(3 batch) loss: 0.835907\n",
      "(4 batch) loss: 0.771529\n",
      "(5 batch) loss: 0.871112\n",
      "(6 batch) loss: 0.853642\n",
      "(7 batch) loss: 0.792863\n",
      "(8 batch) loss: 0.850082\n",
      "(9 batch) loss: 0.771613\n",
      "(10 batch) loss: 0.956040\n",
      "(Epoch 6 / 100) train_acc: 0.703846; val_acc: 0.630000\n",
      "(0 batch) loss: 0.749797\n",
      "(1 batch) loss: 0.753139\n",
      "(2 batch) loss: 0.750712\n",
      "(3 batch) loss: 0.813622\n",
      "(4 batch) loss: 0.794436\n",
      "(5 batch) loss: 0.828696\n",
      "(6 batch) loss: 0.870061\n",
      "(7 batch) loss: 0.719646\n",
      "(8 batch) loss: 0.682205\n",
      "(9 batch) loss: 0.820059\n",
      "(10 batch) loss: 0.785653\n",
      "(Epoch 7 / 100) train_acc: 0.705769; val_acc: 0.610000\n",
      "(0 batch) loss: 0.754770\n",
      "(1 batch) loss: 0.788677\n",
      "(2 batch) loss: 0.642392\n",
      "(3 batch) loss: 0.786316\n",
      "(4 batch) loss: 0.753407\n",
      "(5 batch) loss: 0.722180\n",
      "(6 batch) loss: 0.733684\n",
      "(7 batch) loss: 0.640259\n",
      "(8 batch) loss: 0.759287\n",
      "(9 batch) loss: 0.775525\n",
      "(10 batch) loss: 0.882331\n",
      "(Epoch 8 / 100) train_acc: 0.695192; val_acc: 0.590000\n",
      "(0 batch) loss: 0.735899\n",
      "(1 batch) loss: 0.712664\n",
      "(2 batch) loss: 0.728611\n",
      "(3 batch) loss: 0.775156\n",
      "(4 batch) loss: 0.720791\n",
      "(5 batch) loss: 0.831272\n",
      "(6 batch) loss: 0.674759\n",
      "(7 batch) loss: 0.696571\n",
      "(8 batch) loss: 0.680355\n",
      "(9 batch) loss: 0.746015\n",
      "(10 batch) loss: 0.749692\n",
      "(Epoch 9 / 100) train_acc: 0.727404; val_acc: 0.600000\n",
      "(0 batch) loss: 0.666687\n",
      "(1 batch) loss: 0.773001\n",
      "(2 batch) loss: 0.699206\n",
      "(3 batch) loss: 0.633663\n",
      "(4 batch) loss: 0.660295\n",
      "(5 batch) loss: 0.768391\n",
      "(6 batch) loss: 0.717963\n",
      "(7 batch) loss: 0.756510\n",
      "(8 batch) loss: 0.758751\n",
      "(9 batch) loss: 0.688585\n",
      "(10 batch) loss: 0.671321\n",
      "(Epoch 10 / 100) train_acc: 0.715385; val_acc: 0.660000\n",
      "(0 batch) loss: 0.718967\n",
      "(1 batch) loss: 0.800797\n",
      "(2 batch) loss: 0.656196\n",
      "(3 batch) loss: 0.636922\n",
      "(4 batch) loss: 0.687254\n",
      "(5 batch) loss: 0.637078\n",
      "(6 batch) loss: 0.736231\n",
      "(7 batch) loss: 0.804180\n",
      "(8 batch) loss: 0.845929\n",
      "(9 batch) loss: 0.641513\n",
      "(10 batch) loss: 0.691024\n",
      "(Epoch 11 / 100) train_acc: 0.746635; val_acc: 0.640000\n",
      "(0 batch) loss: 0.620131\n",
      "(1 batch) loss: 0.675306\n",
      "(2 batch) loss: 0.631321\n",
      "(3 batch) loss: 0.724280\n",
      "(4 batch) loss: 0.632032\n",
      "(5 batch) loss: 0.677552\n",
      "(6 batch) loss: 0.740090\n",
      "(7 batch) loss: 0.561695\n",
      "(8 batch) loss: 0.726411\n",
      "(9 batch) loss: 0.791036\n",
      "(10 batch) loss: 0.656026\n",
      "(Epoch 12 / 100) train_acc: 0.743269; val_acc: 0.630000\n",
      "(0 batch) loss: 0.710809\n",
      "(1 batch) loss: 0.563594\n",
      "(2 batch) loss: 0.729926\n",
      "(3 batch) loss: 0.690440\n",
      "(4 batch) loss: 0.669612\n",
      "(5 batch) loss: 0.597067\n",
      "(6 batch) loss: 0.671330\n",
      "(7 batch) loss: 0.601268\n",
      "(8 batch) loss: 0.678078\n",
      "(9 batch) loss: 0.695795\n",
      "(10 batch) loss: 0.627820\n",
      "(Epoch 13 / 100) train_acc: 0.765865; val_acc: 0.550000\n",
      "(0 batch) loss: 0.701404\n",
      "(1 batch) loss: 0.596879\n",
      "(2 batch) loss: 0.604497\n",
      "(3 batch) loss: 0.708821\n",
      "(4 batch) loss: 0.608247\n",
      "(5 batch) loss: 0.691290\n",
      "(6 batch) loss: 0.646738\n",
      "(7 batch) loss: 0.598515\n",
      "(8 batch) loss: 0.597741\n",
      "(9 batch) loss: 0.629579\n",
      "(10 batch) loss: 0.707914\n",
      "(Epoch 14 / 100) train_acc: 0.778365; val_acc: 0.650000\n",
      "(0 batch) loss: 0.554898\n",
      "(1 batch) loss: 0.601885\n",
      "(2 batch) loss: 0.597426\n",
      "(3 batch) loss: 0.638038\n",
      "(4 batch) loss: 0.679728\n",
      "(5 batch) loss: 0.567786\n",
      "(6 batch) loss: 0.653428\n",
      "(7 batch) loss: 0.585824\n",
      "(8 batch) loss: 0.628412\n",
      "(9 batch) loss: 0.613616\n",
      "(10 batch) loss: 0.739603\n",
      "(Epoch 15 / 100) train_acc: 0.766346; val_acc: 0.610000\n",
      "(0 batch) loss: 0.707658\n",
      "(1 batch) loss: 0.515763\n",
      "(2 batch) loss: 0.585608\n",
      "(3 batch) loss: 0.567801\n",
      "(4 batch) loss: 0.570612\n",
      "(5 batch) loss: 0.506099\n",
      "(6 batch) loss: 0.658501\n",
      "(7 batch) loss: 0.636071\n",
      "(8 batch) loss: 0.679791\n",
      "(9 batch) loss: 0.606918\n",
      "(10 batch) loss: 0.756682\n",
      "(Epoch 16 / 100) train_acc: 0.779808; val_acc: 0.610000\n",
      "(0 batch) loss: 0.520973\n",
      "(1 batch) loss: 0.600399\n",
      "(2 batch) loss: 0.590801\n",
      "(3 batch) loss: 0.579918\n",
      "(4 batch) loss: 0.689197\n",
      "(5 batch) loss: 0.608841\n",
      "(6 batch) loss: 0.589733\n",
      "(7 batch) loss: 0.620272\n",
      "(8 batch) loss: 0.575786\n",
      "(9 batch) loss: 0.632800\n",
      "(10 batch) loss: 0.638811\n",
      "(Epoch 17 / 100) train_acc: 0.810577; val_acc: 0.630000\n",
      "(0 batch) loss: 0.526146\n",
      "(1 batch) loss: 0.478026\n",
      "(2 batch) loss: 0.609520\n",
      "(3 batch) loss: 0.649207\n",
      "(4 batch) loss: 0.533485\n",
      "(5 batch) loss: 0.586642\n",
      "(6 batch) loss: 0.688320\n",
      "(7 batch) loss: 0.553561\n",
      "(8 batch) loss: 0.595128\n",
      "(9 batch) loss: 0.660051\n",
      "(10 batch) loss: 0.636273\n",
      "(Epoch 18 / 100) train_acc: 0.801442; val_acc: 0.610000\n",
      "(0 batch) loss: 0.573761\n",
      "(1 batch) loss: 0.574607\n",
      "(2 batch) loss: 0.497500\n",
      "(3 batch) loss: 0.540322\n",
      "(4 batch) loss: 0.577852\n",
      "(5 batch) loss: 0.501622\n",
      "(6 batch) loss: 0.537144\n",
      "(7 batch) loss: 0.512952\n",
      "(8 batch) loss: 0.592686\n",
      "(9 batch) loss: 0.543734\n",
      "(10 batch) loss: 0.651562\n",
      "(Epoch 19 / 100) train_acc: 0.786058; val_acc: 0.680000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew/Desktop/Project/model_explore/.env/lib/python3.5/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type myConv. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0 batch) loss: 0.604576\n",
      "(1 batch) loss: 0.617040\n",
      "(2 batch) loss: 0.462088\n",
      "(3 batch) loss: 0.565822\n",
      "(4 batch) loss: 0.571278\n",
      "(5 batch) loss: 0.563454\n",
      "(6 batch) loss: 0.589214\n",
      "(7 batch) loss: 0.528381\n",
      "(8 batch) loss: 0.599452\n",
      "(9 batch) loss: 0.616774\n",
      "(10 batch) loss: 0.546906\n",
      "(Epoch 20 / 100) train_acc: 0.812019; val_acc: 0.640000\n",
      "(0 batch) loss: 0.532901\n",
      "(1 batch) loss: 0.586928\n",
      "(2 batch) loss: 0.603047\n",
      "(3 batch) loss: 0.524598\n",
      "(4 batch) loss: 0.540848\n",
      "(5 batch) loss: 0.670017\n",
      "(6 batch) loss: 0.502278\n",
      "(7 batch) loss: 0.429817\n",
      "(8 batch) loss: 0.628474\n",
      "(9 batch) loss: 0.547757\n",
      "(10 batch) loss: 0.563726\n",
      "(Epoch 21 / 100) train_acc: 0.816346; val_acc: 0.650000\n",
      "(0 batch) loss: 0.473851\n",
      "(1 batch) loss: 0.457771\n",
      "(2 batch) loss: 0.520813\n",
      "(3 batch) loss: 0.570726\n",
      "(4 batch) loss: 0.617338\n",
      "(5 batch) loss: 0.618141\n",
      "(6 batch) loss: 0.559964\n",
      "(7 batch) loss: 0.502003\n",
      "(8 batch) loss: 0.582777\n",
      "(9 batch) loss: 0.533281\n",
      "(10 batch) loss: 0.476705\n",
      "(Epoch 22 / 100) train_acc: 0.806731; val_acc: 0.590000\n",
      "(0 batch) loss: 0.472966\n",
      "(1 batch) loss: 0.520082\n",
      "(2 batch) loss: 0.484182\n",
      "(3 batch) loss: 0.571231\n",
      "(4 batch) loss: 0.586426\n",
      "(5 batch) loss: 0.520639\n",
      "(6 batch) loss: 0.526087\n",
      "(7 batch) loss: 0.545956\n",
      "(8 batch) loss: 0.628769\n",
      "(9 batch) loss: 0.467550\n",
      "(10 batch) loss: 0.712531\n",
      "(Epoch 23 / 100) train_acc: 0.805288; val_acc: 0.650000\n",
      "(0 batch) loss: 0.508628\n",
      "(1 batch) loss: 0.451711\n",
      "(2 batch) loss: 0.597624\n",
      "(3 batch) loss: 0.529976\n",
      "(4 batch) loss: 0.554264\n",
      "(5 batch) loss: 0.523861\n",
      "(6 batch) loss: 0.505584\n",
      "(7 batch) loss: 0.585869\n",
      "(8 batch) loss: 0.487436\n",
      "(9 batch) loss: 0.560483\n",
      "(10 batch) loss: 0.376491\n",
      "(Epoch 24 / 100) train_acc: 0.765865; val_acc: 0.570000\n",
      "(0 batch) loss: 0.772349\n",
      "(1 batch) loss: 0.537991\n",
      "(2 batch) loss: 0.434712\n",
      "(3 batch) loss: 0.504103\n",
      "(4 batch) loss: 0.510791\n",
      "(5 batch) loss: 0.455511\n",
      "(6 batch) loss: 0.586305\n",
      "(7 batch) loss: 0.438183\n",
      "(8 batch) loss: 0.572334\n",
      "(9 batch) loss: 0.632658\n",
      "(10 batch) loss: 0.482418\n",
      "(Epoch 25 / 100) train_acc: 0.829808; val_acc: 0.570000\n",
      "(0 batch) loss: 0.522100\n",
      "(1 batch) loss: 0.488744\n",
      "(2 batch) loss: 0.446553\n",
      "(3 batch) loss: 0.479626\n",
      "(4 batch) loss: 0.476465\n",
      "(5 batch) loss: 0.561228\n",
      "(6 batch) loss: 0.430406\n",
      "(7 batch) loss: 0.405596\n",
      "(8 batch) loss: 0.577565\n",
      "(9 batch) loss: 0.465092\n",
      "(10 batch) loss: 0.742386\n",
      "(Epoch 26 / 100) train_acc: 0.820192; val_acc: 0.620000\n",
      "(0 batch) loss: 0.446369\n",
      "(1 batch) loss: 0.525294\n",
      "(2 batch) loss: 0.463480\n",
      "(3 batch) loss: 0.465264\n",
      "(4 batch) loss: 0.481124\n",
      "(5 batch) loss: 0.651069\n",
      "(6 batch) loss: 0.614953\n",
      "(7 batch) loss: 0.392554\n",
      "(8 batch) loss: 0.425639\n",
      "(9 batch) loss: 0.612666\n",
      "(10 batch) loss: 0.590073\n",
      "(Epoch 27 / 100) train_acc: 0.816827; val_acc: 0.560000\n",
      "(0 batch) loss: 0.458501\n",
      "(1 batch) loss: 0.549353\n",
      "(2 batch) loss: 0.486750\n",
      "(3 batch) loss: 0.459837\n",
      "(4 batch) loss: 0.546881\n",
      "(5 batch) loss: 0.442347\n",
      "(6 batch) loss: 0.519080\n",
      "(7 batch) loss: 0.575780\n",
      "(8 batch) loss: 0.535597\n",
      "(9 batch) loss: 0.570534\n",
      "(10 batch) loss: 0.492732\n",
      "(Epoch 28 / 100) train_acc: 0.846154; val_acc: 0.630000\n",
      "(0 batch) loss: 0.478551\n",
      "(1 batch) loss: 0.493772\n",
      "(2 batch) loss: 0.445066\n",
      "(3 batch) loss: 0.451785\n",
      "(4 batch) loss: 0.447284\n",
      "(5 batch) loss: 0.576313\n",
      "(6 batch) loss: 0.413571\n",
      "(7 batch) loss: 0.420448\n",
      "(8 batch) loss: 0.488160\n",
      "(9 batch) loss: 0.561664\n",
      "(10 batch) loss: 0.548804\n",
      "(Epoch 29 / 100) train_acc: 0.847115; val_acc: 0.630000\n",
      "(0 batch) loss: 0.426452\n",
      "(1 batch) loss: 0.349124\n",
      "(2 batch) loss: 0.509672\n",
      "(3 batch) loss: 0.493897\n",
      "(4 batch) loss: 0.506585\n",
      "(5 batch) loss: 0.455700\n",
      "(6 batch) loss: 0.525870\n",
      "(7 batch) loss: 0.465365\n",
      "(8 batch) loss: 0.448736\n",
      "(9 batch) loss: 0.487775\n",
      "(10 batch) loss: 0.541485\n",
      "(Epoch 30 / 100) train_acc: 0.818269; val_acc: 0.600000\n",
      "(0 batch) loss: 0.430553\n",
      "(1 batch) loss: 0.385022\n",
      "(2 batch) loss: 0.499195\n",
      "(3 batch) loss: 0.442208\n",
      "(4 batch) loss: 0.440131\n",
      "(5 batch) loss: 0.436274\n",
      "(6 batch) loss: 0.402674\n",
      "(7 batch) loss: 0.462936\n",
      "(8 batch) loss: 0.317177\n",
      "(9 batch) loss: 0.577032\n",
      "(10 batch) loss: 0.496596\n",
      "(Epoch 31 / 100) train_acc: 0.845673; val_acc: 0.640000\n",
      "(0 batch) loss: 0.391165\n",
      "(1 batch) loss: 0.460668\n",
      "(2 batch) loss: 0.430545\n",
      "(3 batch) loss: 0.372992\n",
      "(4 batch) loss: 0.408850\n",
      "(5 batch) loss: 0.411471\n",
      "(6 batch) loss: 0.454128\n",
      "(7 batch) loss: 0.445271\n",
      "(8 batch) loss: 0.484460\n",
      "(9 batch) loss: 0.469844\n",
      "(10 batch) loss: 0.581646\n",
      "(Epoch 32 / 100) train_acc: 0.873558; val_acc: 0.600000\n",
      "(0 batch) loss: 0.391169\n",
      "(1 batch) loss: 0.381187\n",
      "(2 batch) loss: 0.271770\n",
      "(3 batch) loss: 0.390071\n",
      "(4 batch) loss: 0.486190\n",
      "(5 batch) loss: 0.391207\n",
      "(6 batch) loss: 0.426574\n",
      "(7 batch) loss: 0.410855\n",
      "(8 batch) loss: 0.465006\n",
      "(9 batch) loss: 0.466737\n",
      "(10 batch) loss: 0.374242\n",
      "(Epoch 33 / 100) train_acc: 0.864423; val_acc: 0.610000\n",
      "(0 batch) loss: 0.407941\n",
      "(1 batch) loss: 0.410819\n",
      "(2 batch) loss: 0.391503\n",
      "(3 batch) loss: 0.361349\n",
      "(4 batch) loss: 0.365270\n",
      "(5 batch) loss: 0.391915\n",
      "(6 batch) loss: 0.425890\n",
      "(7 batch) loss: 0.406983\n",
      "(8 batch) loss: 0.374496\n",
      "(9 batch) loss: 0.424446\n",
      "(10 batch) loss: 0.444398\n",
      "(Epoch 34 / 100) train_acc: 0.864423; val_acc: 0.630000\n",
      "(0 batch) loss: 0.323059\n",
      "(1 batch) loss: 0.477427\n",
      "(2 batch) loss: 0.348625\n",
      "(3 batch) loss: 0.436565\n",
      "(4 batch) loss: 0.394491\n",
      "(5 batch) loss: 0.375914\n",
      "(6 batch) loss: 0.374427\n",
      "(7 batch) loss: 0.407765\n",
      "(8 batch) loss: 0.378818\n",
      "(9 batch) loss: 0.440345\n",
      "(10 batch) loss: 0.455162\n",
      "(Epoch 35 / 100) train_acc: 0.869231; val_acc: 0.640000\n",
      "(0 batch) loss: 0.371868\n",
      "(1 batch) loss: 0.393184\n",
      "(2 batch) loss: 0.390513\n",
      "(3 batch) loss: 0.359769\n",
      "(4 batch) loss: 0.407002\n",
      "(5 batch) loss: 0.381678\n",
      "(6 batch) loss: 0.351776\n",
      "(7 batch) loss: 0.574879\n",
      "(8 batch) loss: 0.423793\n",
      "(9 batch) loss: 0.439553\n",
      "(10 batch) loss: 0.454186\n",
      "(Epoch 36 / 100) train_acc: 0.858654; val_acc: 0.610000\n",
      "(0 batch) loss: 0.353276\n",
      "(1 batch) loss: 0.411319\n",
      "(2 batch) loss: 0.486534\n",
      "(3 batch) loss: 0.328139\n",
      "(4 batch) loss: 0.359714\n",
      "(5 batch) loss: 0.405708\n",
      "(6 batch) loss: 0.478761\n",
      "(7 batch) loss: 0.281864\n",
      "(8 batch) loss: 0.330754\n",
      "(9 batch) loss: 0.453784\n",
      "(10 batch) loss: 0.300433\n",
      "(Epoch 37 / 100) train_acc: 0.865865; val_acc: 0.610000\n",
      "(0 batch) loss: 0.322128\n",
      "(1 batch) loss: 0.470404\n",
      "(2 batch) loss: 0.389007\n",
      "(3 batch) loss: 0.345403\n",
      "(4 batch) loss: 0.401673\n",
      "(5 batch) loss: 0.446879\n",
      "(6 batch) loss: 0.374463\n",
      "(7 batch) loss: 0.351631\n",
      "(8 batch) loss: 0.340699\n",
      "(9 batch) loss: 0.388431\n",
      "(10 batch) loss: 0.441623\n",
      "(Epoch 38 / 100) train_acc: 0.888462; val_acc: 0.650000\n",
      "(0 batch) loss: 0.222769\n",
      "(1 batch) loss: 0.379466\n",
      "(2 batch) loss: 0.415904\n",
      "(3 batch) loss: 0.335199\n",
      "(4 batch) loss: 0.349131\n",
      "(5 batch) loss: 0.368877\n",
      "(6 batch) loss: 0.393097\n",
      "(7 batch) loss: 0.336509\n",
      "(8 batch) loss: 0.474098\n",
      "(9 batch) loss: 0.396145\n",
      "(10 batch) loss: 0.565197\n",
      "(Epoch 39 / 100) train_acc: 0.881731; val_acc: 0.580000\n",
      "(0 batch) loss: 0.382835\n",
      "(1 batch) loss: 0.294426\n",
      "(2 batch) loss: 0.399890\n",
      "(3 batch) loss: 0.382783\n",
      "(4 batch) loss: 0.323138\n",
      "(5 batch) loss: 0.365358\n",
      "(6 batch) loss: 0.350979\n",
      "(7 batch) loss: 0.448267\n",
      "(8 batch) loss: 0.370527\n",
      "(9 batch) loss: 0.414121\n",
      "(10 batch) loss: 0.477576\n",
      "(Epoch 40 / 100) train_acc: 0.841827; val_acc: 0.590000\n",
      "(0 batch) loss: 0.387773\n",
      "(1 batch) loss: 0.415124\n",
      "(2 batch) loss: 0.383611\n",
      "(3 batch) loss: 0.288432\n",
      "(4 batch) loss: 0.392058\n",
      "(5 batch) loss: 0.372743\n",
      "(6 batch) loss: 0.436832\n",
      "(7 batch) loss: 0.371327\n",
      "(8 batch) loss: 0.479115\n",
      "(9 batch) loss: 0.369952\n",
      "(10 batch) loss: 0.419781\n",
      "(Epoch 41 / 100) train_acc: 0.881250; val_acc: 0.630000\n",
      "(0 batch) loss: 0.326293\n",
      "(1 batch) loss: 0.321023\n",
      "(2 batch) loss: 0.461759\n",
      "(3 batch) loss: 0.323109\n",
      "(4 batch) loss: 0.343075\n",
      "(5 batch) loss: 0.337170\n",
      "(6 batch) loss: 0.292587\n",
      "(7 batch) loss: 0.397828\n",
      "(8 batch) loss: 0.458587\n",
      "(9 batch) loss: 0.406656\n",
      "(10 batch) loss: 0.492200\n",
      "(Epoch 42 / 100) train_acc: 0.889904; val_acc: 0.630000\n",
      "(0 batch) loss: 0.286742\n",
      "(1 batch) loss: 0.335924\n",
      "(2 batch) loss: 0.343664\n",
      "(3 batch) loss: 0.372097\n",
      "(4 batch) loss: 0.341663\n",
      "(5 batch) loss: 0.269486\n",
      "(6 batch) loss: 0.520029\n",
      "(7 batch) loss: 0.380525\n",
      "(8 batch) loss: 0.411737\n",
      "(9 batch) loss: 0.395165\n",
      "(10 batch) loss: 0.477769\n",
      "(Epoch 43 / 100) train_acc: 0.877404; val_acc: 0.640000\n",
      "(0 batch) loss: 0.341422\n",
      "(1 batch) loss: 0.335932\n",
      "(2 batch) loss: 0.338321\n",
      "(3 batch) loss: 0.355709\n",
      "(4 batch) loss: 0.368915\n",
      "(5 batch) loss: 0.397425\n",
      "(6 batch) loss: 0.432048\n",
      "(7 batch) loss: 0.390946\n",
      "(8 batch) loss: 0.393532\n",
      "(9 batch) loss: 0.409222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10 batch) loss: 0.483190\n",
      "(Epoch 44 / 100) train_acc: 0.859615; val_acc: 0.600000\n",
      "(0 batch) loss: 0.372405\n",
      "(1 batch) loss: 0.344371\n",
      "(2 batch) loss: 0.402747\n",
      "(3 batch) loss: 0.447088\n",
      "(4 batch) loss: 0.490947\n",
      "(5 batch) loss: 0.308106\n",
      "(6 batch) loss: 0.451739\n",
      "(7 batch) loss: 0.492055\n",
      "(8 batch) loss: 0.385574\n",
      "(9 batch) loss: 0.437264\n",
      "(10 batch) loss: 0.425404\n",
      "(Epoch 45 / 100) train_acc: 0.864423; val_acc: 0.640000\n",
      "(0 batch) loss: 0.382344\n",
      "(1 batch) loss: 0.339638\n",
      "(2 batch) loss: 0.452390\n",
      "(3 batch) loss: 0.346529\n",
      "(4 batch) loss: 0.355850\n",
      "(5 batch) loss: 0.349666\n",
      "(6 batch) loss: 0.335504\n",
      "(7 batch) loss: 0.386716\n",
      "(8 batch) loss: 0.433690\n",
      "(9 batch) loss: 0.358216\n",
      "(10 batch) loss: 0.480096\n",
      "(Epoch 46 / 100) train_acc: 0.893269; val_acc: 0.620000\n",
      "(0 batch) loss: 0.370888\n",
      "(1 batch) loss: 0.291829\n",
      "(2 batch) loss: 0.354600\n",
      "(3 batch) loss: 0.314838\n",
      "(4 batch) loss: 0.345580\n",
      "(5 batch) loss: 0.391930\n",
      "(6 batch) loss: 0.408138\n",
      "(7 batch) loss: 0.413015\n",
      "(8 batch) loss: 0.267310\n",
      "(9 batch) loss: 0.338601\n",
      "(10 batch) loss: 0.367809\n",
      "(Epoch 47 / 100) train_acc: 0.872596; val_acc: 0.600000\n",
      "(0 batch) loss: 0.307419\n",
      "(1 batch) loss: 0.293648\n",
      "(2 batch) loss: 0.286330\n",
      "(3 batch) loss: 0.346364\n",
      "(4 batch) loss: 0.352936\n",
      "(5 batch) loss: 0.347577\n",
      "(6 batch) loss: 0.378849\n",
      "(7 batch) loss: 0.357655\n",
      "(8 batch) loss: 0.386747\n",
      "(9 batch) loss: 0.441180\n",
      "(10 batch) loss: 0.395766\n",
      "(Epoch 48 / 100) train_acc: 0.881731; val_acc: 0.600000\n",
      "(0 batch) loss: 0.266193\n",
      "(1 batch) loss: 0.272522\n",
      "(2 batch) loss: 0.324341\n",
      "(3 batch) loss: 0.337692\n",
      "(4 batch) loss: 0.342456\n",
      "(5 batch) loss: 0.263787\n",
      "(6 batch) loss: 0.323276\n",
      "(7 batch) loss: 0.305508\n",
      "(8 batch) loss: 0.414964\n",
      "(9 batch) loss: 0.373335\n",
      "(10 batch) loss: 0.329778\n",
      "(Epoch 49 / 100) train_acc: 0.895192; val_acc: 0.580000\n",
      "(0 batch) loss: 0.262443\n",
      "(1 batch) loss: 0.295521\n",
      "(2 batch) loss: 0.375471\n",
      "(3 batch) loss: 0.281910\n",
      "(4 batch) loss: 0.315563\n",
      "(5 batch) loss: 0.290387\n",
      "(6 batch) loss: 0.245774\n",
      "(7 batch) loss: 0.427615\n",
      "(8 batch) loss: 0.390851\n",
      "(9 batch) loss: 0.275193\n",
      "(10 batch) loss: 0.378755\n",
      "(Epoch 50 / 100) train_acc: 0.918750; val_acc: 0.640000\n",
      "(0 batch) loss: 0.246243\n",
      "(1 batch) loss: 0.335760\n",
      "(2 batch) loss: 0.357641\n",
      "(3 batch) loss: 0.326741\n",
      "(4 batch) loss: 0.366428\n",
      "(5 batch) loss: 0.291959\n",
      "(6 batch) loss: 0.280681\n",
      "(7 batch) loss: 0.283253\n",
      "(8 batch) loss: 0.246703\n",
      "(9 batch) loss: 0.273101\n",
      "(10 batch) loss: 0.307630\n",
      "(Epoch 51 / 100) train_acc: 0.872596; val_acc: 0.630000\n",
      "(0 batch) loss: 0.365550\n",
      "(1 batch) loss: 0.296008\n",
      "(2 batch) loss: 0.250772\n",
      "(3 batch) loss: 0.327216\n",
      "(4 batch) loss: 0.315557\n",
      "(5 batch) loss: 0.319904\n",
      "(6 batch) loss: 0.320689\n",
      "(7 batch) loss: 0.375300\n",
      "(8 batch) loss: 0.350109\n",
      "(9 batch) loss: 0.333210\n",
      "(10 batch) loss: 0.369110\n",
      "(Epoch 52 / 100) train_acc: 0.906250; val_acc: 0.620000\n",
      "(0 batch) loss: 0.229059\n",
      "(1 batch) loss: 0.272383\n",
      "(2 batch) loss: 0.299161\n",
      "(3 batch) loss: 0.408080\n",
      "(4 batch) loss: 0.290673\n",
      "(5 batch) loss: 0.306541\n",
      "(6 batch) loss: 0.267090\n",
      "(7 batch) loss: 0.292417\n",
      "(8 batch) loss: 0.421645\n",
      "(9 batch) loss: 0.379146\n",
      "(10 batch) loss: 0.223658\n",
      "(Epoch 53 / 100) train_acc: 0.887981; val_acc: 0.590000\n",
      "(0 batch) loss: 0.358655\n",
      "(1 batch) loss: 0.282350\n",
      "(2 batch) loss: 0.281126\n",
      "(3 batch) loss: 0.276226\n",
      "(4 batch) loss: 0.271263\n",
      "(5 batch) loss: 0.335573\n",
      "(6 batch) loss: 0.266077\n",
      "(7 batch) loss: 0.451595\n",
      "(8 batch) loss: 0.370841\n",
      "(9 batch) loss: 0.371316\n",
      "(10 batch) loss: 0.281097\n",
      "(Epoch 54 / 100) train_acc: 0.852404; val_acc: 0.630000\n",
      "(0 batch) loss: 0.368730\n",
      "(1 batch) loss: 0.419221\n",
      "(2 batch) loss: 0.295567\n",
      "(3 batch) loss: 0.269117\n",
      "(4 batch) loss: 0.540539\n",
      "(5 batch) loss: 0.467109\n",
      "(6 batch) loss: 0.411886\n",
      "(7 batch) loss: 0.340994\n",
      "(8 batch) loss: 0.376556\n",
      "(9 batch) loss: 0.390220\n",
      "(10 batch) loss: 0.398110\n",
      "(Epoch 55 / 100) train_acc: 0.883654; val_acc: 0.580000\n",
      "(0 batch) loss: 0.270220\n",
      "(1 batch) loss: 0.293443\n",
      "(2 batch) loss: 0.328909\n",
      "(3 batch) loss: 0.355263\n",
      "(4 batch) loss: 0.361356\n",
      "(5 batch) loss: 0.367674\n",
      "(6 batch) loss: 0.349873\n",
      "(7 batch) loss: 0.477257\n",
      "(8 batch) loss: 0.383522\n",
      "(9 batch) loss: 0.379257\n",
      "(10 batch) loss: 0.402910\n",
      "(Epoch 56 / 100) train_acc: 0.849519; val_acc: 0.560000\n",
      "(0 batch) loss: 0.389464\n",
      "(1 batch) loss: 0.414276\n",
      "(2 batch) loss: 0.285755\n",
      "(3 batch) loss: 0.324635\n",
      "(4 batch) loss: 0.476568\n",
      "(5 batch) loss: 0.468152\n",
      "(6 batch) loss: 0.356257\n",
      "(7 batch) loss: 0.397754\n",
      "(8 batch) loss: 0.374915\n",
      "(9 batch) loss: 0.364332\n",
      "(10 batch) loss: 0.383959\n",
      "(Epoch 57 / 100) train_acc: 0.864904; val_acc: 0.610000\n",
      "(0 batch) loss: 0.350592\n",
      "(1 batch) loss: 0.275258\n",
      "(2 batch) loss: 0.387322\n",
      "(3 batch) loss: 0.313624\n",
      "(4 batch) loss: 0.320342\n",
      "(5 batch) loss: 0.278810\n",
      "(6 batch) loss: 0.306649\n",
      "(7 batch) loss: 0.278542\n",
      "(8 batch) loss: 0.310445\n",
      "(9 batch) loss: 0.294655\n",
      "(10 batch) loss: 0.289767\n",
      "(Epoch 58 / 100) train_acc: 0.906731; val_acc: 0.590000\n",
      "(0 batch) loss: 0.306657\n",
      "(1 batch) loss: 0.249719\n",
      "(2 batch) loss: 0.268056\n",
      "(3 batch) loss: 0.252633\n",
      "(4 batch) loss: 0.291976\n",
      "(5 batch) loss: 0.260568\n",
      "(6 batch) loss: 0.325209\n",
      "(7 batch) loss: 0.264023\n",
      "(8 batch) loss: 0.376880\n",
      "(9 batch) loss: 0.315475\n",
      "(10 batch) loss: 0.352888\n",
      "(Epoch 59 / 100) train_acc: 0.906731; val_acc: 0.610000\n",
      "(0 batch) loss: 0.265541\n",
      "(1 batch) loss: 0.292166\n",
      "(2 batch) loss: 0.249967\n",
      "(3 batch) loss: 0.252102\n",
      "(4 batch) loss: 0.285518\n",
      "(5 batch) loss: 0.354561\n",
      "(6 batch) loss: 0.305952\n",
      "(7 batch) loss: 0.278275\n",
      "(8 batch) loss: 0.323174\n",
      "(9 batch) loss: 0.317683\n",
      "(10 batch) loss: 0.267608\n",
      "(Epoch 60 / 100) train_acc: 0.918750; val_acc: 0.620000\n",
      "(0 batch) loss: 0.170842\n",
      "(1 batch) loss: 0.278565\n",
      "(2 batch) loss: 0.339613\n",
      "(3 batch) loss: 0.242380\n",
      "(4 batch) loss: 0.263402\n",
      "(5 batch) loss: 0.300137\n",
      "(6 batch) loss: 0.242672\n",
      "(7 batch) loss: 0.274230\n",
      "(8 batch) loss: 0.299317\n",
      "(9 batch) loss: 0.286969\n",
      "(10 batch) loss: 0.328127\n",
      "(Epoch 61 / 100) train_acc: 0.888462; val_acc: 0.540000\n",
      "(0 batch) loss: 0.277916\n",
      "(1 batch) loss: 0.270592\n",
      "(2 batch) loss: 0.258298\n",
      "(3 batch) loss: 0.267960\n",
      "(4 batch) loss: 0.293700\n",
      "(5 batch) loss: 0.358759\n",
      "(6 batch) loss: 0.256164\n",
      "(7 batch) loss: 0.349873\n",
      "(8 batch) loss: 0.374198\n",
      "(9 batch) loss: 0.341437\n",
      "(10 batch) loss: 0.414156\n",
      "(Epoch 62 / 100) train_acc: 0.912981; val_acc: 0.550000\n",
      "(0 batch) loss: 0.267051\n",
      "(1 batch) loss: 0.251861\n",
      "(2 batch) loss: 0.285775\n",
      "(3 batch) loss: 0.226681\n",
      "(4 batch) loss: 0.296931\n",
      "(5 batch) loss: 0.226785\n",
      "(6 batch) loss: 0.271643\n",
      "(7 batch) loss: 0.308534\n",
      "(8 batch) loss: 0.255391\n",
      "(9 batch) loss: 0.243681\n",
      "(10 batch) loss: 0.274216\n",
      "(Epoch 63 / 100) train_acc: 0.915385; val_acc: 0.620000\n",
      "(0 batch) loss: 0.289972\n",
      "(1 batch) loss: 0.229906\n",
      "(2 batch) loss: 0.226647\n",
      "(3 batch) loss: 0.257863\n",
      "(4 batch) loss: 0.329994\n",
      "(5 batch) loss: 0.314534\n",
      "(6 batch) loss: 0.258519\n",
      "(7 batch) loss: 0.271259\n",
      "(8 batch) loss: 0.252304\n",
      "(9 batch) loss: 0.259964\n",
      "(10 batch) loss: 0.264768\n",
      "(Epoch 64 / 100) train_acc: 0.927885; val_acc: 0.610000\n",
      "(0 batch) loss: 0.171109\n",
      "(1 batch) loss: 0.304798\n",
      "(2 batch) loss: 0.258062\n",
      "(3 batch) loss: 0.215957\n",
      "(4 batch) loss: 0.238992\n",
      "(5 batch) loss: 0.260443\n",
      "(6 batch) loss: 0.229254\n",
      "(7 batch) loss: 0.300605\n",
      "(8 batch) loss: 0.258621\n",
      "(9 batch) loss: 0.281299\n",
      "(10 batch) loss: 0.336593\n",
      "(Epoch 65 / 100) train_acc: 0.915385; val_acc: 0.590000\n",
      "(0 batch) loss: 0.223141\n",
      "(1 batch) loss: 0.236712\n",
      "(2 batch) loss: 0.210624\n",
      "(3 batch) loss: 0.235452\n",
      "(4 batch) loss: 0.205663\n",
      "(5 batch) loss: 0.310372\n",
      "(6 batch) loss: 0.266058\n",
      "(7 batch) loss: 0.259420\n",
      "(8 batch) loss: 0.291643\n",
      "(9 batch) loss: 0.226975\n",
      "(10 batch) loss: 0.340550\n",
      "(Epoch 66 / 100) train_acc: 0.904808; val_acc: 0.590000\n",
      "(0 batch) loss: 0.244619\n",
      "(1 batch) loss: 0.213617\n",
      "(2 batch) loss: 0.258079\n",
      "(3 batch) loss: 0.242795\n",
      "(4 batch) loss: 0.239192\n",
      "(5 batch) loss: 0.242778\n",
      "(6 batch) loss: 0.232448\n",
      "(7 batch) loss: 0.281472\n",
      "(8 batch) loss: 0.222146\n",
      "(9 batch) loss: 0.372798\n",
      "(10 batch) loss: 0.456751\n",
      "(Epoch 67 / 100) train_acc: 0.920673; val_acc: 0.620000\n",
      "(0 batch) loss: 0.224182\n",
      "(1 batch) loss: 0.267927\n",
      "(2 batch) loss: 0.339828\n",
      "(3 batch) loss: 0.297517\n",
      "(4 batch) loss: 0.279875\n",
      "(5 batch) loss: 0.258876\n",
      "(6 batch) loss: 0.308610\n",
      "(7 batch) loss: 0.249005\n",
      "(8 batch) loss: 0.211150\n",
      "(9 batch) loss: 0.374152\n",
      "(10 batch) loss: 0.291055\n",
      "(Epoch 68 / 100) train_acc: 0.900962; val_acc: 0.580000\n",
      "(0 batch) loss: 0.249295\n",
      "(1 batch) loss: 0.325485\n",
      "(2 batch) loss: 0.299101\n",
      "(3 batch) loss: 0.280873\n",
      "(4 batch) loss: 0.335813\n",
      "(5 batch) loss: 0.342254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6 batch) loss: 0.370798\n",
      "(7 batch) loss: 0.314949\n",
      "(8 batch) loss: 0.299147\n",
      "(9 batch) loss: 0.320156\n",
      "(10 batch) loss: 0.250713\n",
      "(Epoch 69 / 100) train_acc: 0.897596; val_acc: 0.500000\n",
      "(0 batch) loss: 0.290989\n",
      "(1 batch) loss: 0.276039\n",
      "(2 batch) loss: 0.362930\n",
      "(3 batch) loss: 0.253117\n",
      "(4 batch) loss: 0.232330\n",
      "(5 batch) loss: 0.253719\n",
      "(6 batch) loss: 0.280766\n",
      "(7 batch) loss: 0.235467\n",
      "(8 batch) loss: 0.212853\n",
      "(9 batch) loss: 0.372074\n",
      "(10 batch) loss: 0.206477\n",
      "(Epoch 70 / 100) train_acc: 0.919712; val_acc: 0.580000\n",
      "(0 batch) loss: 0.241923\n",
      "(1 batch) loss: 0.255481\n",
      "(2 batch) loss: 0.276816\n",
      "(3 batch) loss: 0.245853\n",
      "(4 batch) loss: 0.263462\n",
      "(5 batch) loss: 0.320013\n",
      "(6 batch) loss: 0.231150\n",
      "(7 batch) loss: 0.277787\n",
      "(8 batch) loss: 0.310023\n",
      "(9 batch) loss: 0.279387\n",
      "(10 batch) loss: 0.240825\n",
      "(Epoch 71 / 100) train_acc: 0.937981; val_acc: 0.610000\n",
      "(0 batch) loss: 0.193157\n",
      "(1 batch) loss: 0.208396\n",
      "(2 batch) loss: 0.239227\n",
      "(3 batch) loss: 0.198579\n",
      "(4 batch) loss: 0.199297\n",
      "(5 batch) loss: 0.267776\n",
      "(6 batch) loss: 0.186847\n",
      "(7 batch) loss: 0.220700\n",
      "(8 batch) loss: 0.221769\n",
      "(9 batch) loss: 0.249746\n",
      "(10 batch) loss: 0.298944\n",
      "(Epoch 72 / 100) train_acc: 0.907692; val_acc: 0.600000\n",
      "(0 batch) loss: 0.292372\n",
      "(1 batch) loss: 0.164964\n",
      "(2 batch) loss: 0.168429\n",
      "(3 batch) loss: 0.219572\n",
      "(4 batch) loss: 0.192940\n",
      "(5 batch) loss: 0.256481\n",
      "(6 batch) loss: 0.187521\n",
      "(7 batch) loss: 0.203344\n",
      "(8 batch) loss: 0.237406\n",
      "(9 batch) loss: 0.197167\n",
      "(10 batch) loss: 0.297397\n",
      "(Epoch 73 / 100) train_acc: 0.943269; val_acc: 0.630000\n",
      "(0 batch) loss: 0.147540\n",
      "(1 batch) loss: 0.189062\n",
      "(2 batch) loss: 0.198533\n",
      "(3 batch) loss: 0.205960\n",
      "(4 batch) loss: 0.160952\n",
      "(5 batch) loss: 0.165217\n",
      "(6 batch) loss: 0.239112\n",
      "(7 batch) loss: 0.206316\n",
      "(8 batch) loss: 0.193378\n",
      "(9 batch) loss: 0.214745\n",
      "(10 batch) loss: 0.239900\n",
      "(Epoch 74 / 100) train_acc: 0.949038; val_acc: 0.640000\n",
      "(0 batch) loss: 0.153987\n",
      "(1 batch) loss: 0.153761\n",
      "(2 batch) loss: 0.204153\n",
      "(3 batch) loss: 0.183611\n",
      "(4 batch) loss: 0.163965\n",
      "(5 batch) loss: 0.205930\n",
      "(6 batch) loss: 0.199272\n",
      "(7 batch) loss: 0.168933\n",
      "(8 batch) loss: 0.187850\n",
      "(9 batch) loss: 0.209963\n",
      "(10 batch) loss: 0.249960\n",
      "(Epoch 75 / 100) train_acc: 0.957212; val_acc: 0.600000\n",
      "(0 batch) loss: 0.191810\n",
      "(1 batch) loss: 0.124329\n",
      "(2 batch) loss: 0.156571\n",
      "(3 batch) loss: 0.185616\n",
      "(4 batch) loss: 0.194373\n",
      "(5 batch) loss: 0.211975\n",
      "(6 batch) loss: 0.177155\n",
      "(7 batch) loss: 0.183627\n",
      "(8 batch) loss: 0.274544\n",
      "(9 batch) loss: 0.176942\n",
      "(10 batch) loss: 0.201318\n",
      "(Epoch 76 / 100) train_acc: 0.957212; val_acc: 0.610000\n",
      "(0 batch) loss: 0.126941\n",
      "(1 batch) loss: 0.200306\n",
      "(2 batch) loss: 0.148223\n",
      "(3 batch) loss: 0.146904\n",
      "(4 batch) loss: 0.163868\n",
      "(5 batch) loss: 0.221564\n",
      "(6 batch) loss: 0.194108\n",
      "(7 batch) loss: 0.237341\n",
      "(8 batch) loss: 0.179479\n",
      "(9 batch) loss: 0.220120\n",
      "(10 batch) loss: 0.132904\n",
      "(Epoch 77 / 100) train_acc: 0.953365; val_acc: 0.590000\n",
      "(0 batch) loss: 0.149556\n",
      "(1 batch) loss: 0.148399\n",
      "(2 batch) loss: 0.119447\n",
      "(3 batch) loss: 0.173436\n",
      "(4 batch) loss: 0.197436\n",
      "(5 batch) loss: 0.175873\n",
      "(6 batch) loss: 0.213309\n",
      "(7 batch) loss: 0.188857\n",
      "(8 batch) loss: 0.184968\n",
      "(9 batch) loss: 0.173157\n",
      "(10 batch) loss: 0.199733\n",
      "(Epoch 78 / 100) train_acc: 0.937500; val_acc: 0.620000\n",
      "(0 batch) loss: 0.206734\n",
      "(1 batch) loss: 0.176558\n",
      "(2 batch) loss: 0.196290\n",
      "(3 batch) loss: 0.184623\n",
      "(4 batch) loss: 0.161393\n",
      "(5 batch) loss: 0.187698\n",
      "(6 batch) loss: 0.191840\n",
      "(7 batch) loss: 0.169811\n",
      "(8 batch) loss: 0.207423\n",
      "(9 batch) loss: 0.171011\n",
      "(10 batch) loss: 0.265398\n",
      "(Epoch 79 / 100) train_acc: 0.938942; val_acc: 0.570000\n",
      "(0 batch) loss: 0.160277\n",
      "(1 batch) loss: 0.199695\n",
      "(2 batch) loss: 0.151393\n",
      "(3 batch) loss: 0.158744\n",
      "(4 batch) loss: 0.220385\n",
      "(5 batch) loss: 0.246682\n",
      "(6 batch) loss: 0.234258\n",
      "(7 batch) loss: 0.180690\n",
      "(8 batch) loss: 0.237505\n",
      "(9 batch) loss: 0.239110\n",
      "(10 batch) loss: 0.258087\n",
      "(Epoch 80 / 100) train_acc: 0.942308; val_acc: 0.620000\n",
      "(0 batch) loss: 0.190934\n",
      "(1 batch) loss: 0.205771\n",
      "(2 batch) loss: 0.220079\n",
      "(3 batch) loss: 0.237408\n",
      "(4 batch) loss: 0.192557\n",
      "(5 batch) loss: 0.190854\n",
      "(6 batch) loss: 0.316586\n",
      "(7 batch) loss: 0.259038\n",
      "(8 batch) loss: 0.200634\n",
      "(9 batch) loss: 0.345993\n",
      "(10 batch) loss: 0.452395\n",
      "(Epoch 81 / 100) train_acc: 0.937500; val_acc: 0.580000\n",
      "(0 batch) loss: 0.208688\n",
      "(1 batch) loss: 0.197795\n",
      "(2 batch) loss: 0.247303\n",
      "(3 batch) loss: 0.267887\n",
      "(4 batch) loss: 0.213869\n",
      "(5 batch) loss: 0.272375\n",
      "(6 batch) loss: 0.282548\n",
      "(7 batch) loss: 0.322595\n",
      "(8 batch) loss: 0.202667\n",
      "(9 batch) loss: 0.297764\n",
      "(10 batch) loss: 0.380486\n",
      "(Epoch 82 / 100) train_acc: 0.892788; val_acc: 0.610000\n",
      "(0 batch) loss: 0.265265\n",
      "(1 batch) loss: 0.278196\n",
      "(2 batch) loss: 0.238979\n",
      "(3 batch) loss: 0.314405\n",
      "(4 batch) loss: 0.375838\n",
      "(5 batch) loss: 0.240258\n",
      "(6 batch) loss: 0.229790\n",
      "(7 batch) loss: 0.238555\n",
      "(8 batch) loss: 0.369514\n",
      "(9 batch) loss: 0.262206\n",
      "(10 batch) loss: 0.287312\n",
      "(Epoch 83 / 100) train_acc: 0.925962; val_acc: 0.590000\n",
      "(0 batch) loss: 0.198852\n",
      "(1 batch) loss: 0.219810\n",
      "(2 batch) loss: 0.342377\n",
      "(3 batch) loss: 0.234392\n",
      "(4 batch) loss: 0.280410\n",
      "(5 batch) loss: 0.284257\n",
      "(6 batch) loss: 0.229858\n",
      "(7 batch) loss: 0.291526\n",
      "(8 batch) loss: 0.233469\n",
      "(9 batch) loss: 0.283815\n",
      "(10 batch) loss: 0.280095\n",
      "(Epoch 84 / 100) train_acc: 0.912981; val_acc: 0.550000\n",
      "(0 batch) loss: 0.252283\n",
      "(1 batch) loss: 0.231066\n",
      "(2 batch) loss: 0.143175\n",
      "(3 batch) loss: 0.202783\n",
      "(4 batch) loss: 0.202183\n",
      "(5 batch) loss: 0.256875\n",
      "(6 batch) loss: 0.185956\n",
      "(7 batch) loss: 0.145765\n",
      "(8 batch) loss: 0.298518\n",
      "(9 batch) loss: 0.265388\n",
      "(10 batch) loss: 0.254408\n",
      "(Epoch 85 / 100) train_acc: 0.920673; val_acc: 0.600000\n",
      "(0 batch) loss: 0.223428\n",
      "(1 batch) loss: 0.192514\n",
      "(2 batch) loss: 0.179831\n",
      "(3 batch) loss: 0.168596\n",
      "(4 batch) loss: 0.209815\n",
      "(5 batch) loss: 0.142486\n",
      "(6 batch) loss: 0.193795\n",
      "(7 batch) loss: 0.184308\n",
      "(8 batch) loss: 0.219135\n",
      "(9 batch) loss: 0.196310\n",
      "(10 batch) loss: 0.161524\n",
      "(Epoch 86 / 100) train_acc: 0.939904; val_acc: 0.580000\n",
      "(0 batch) loss: 0.181591\n",
      "(1 batch) loss: 0.206766\n",
      "(2 batch) loss: 0.190961\n",
      "(3 batch) loss: 0.145405\n",
      "(4 batch) loss: 0.130608\n",
      "(5 batch) loss: 0.206583\n",
      "(6 batch) loss: 0.182949\n",
      "(7 batch) loss: 0.127070\n",
      "(8 batch) loss: 0.167501\n",
      "(9 batch) loss: 0.198638\n",
      "(10 batch) loss: 0.092644\n",
      "(Epoch 87 / 100) train_acc: 0.952885; val_acc: 0.580000\n",
      "(0 batch) loss: 0.127160\n",
      "(1 batch) loss: 0.152277\n",
      "(2 batch) loss: 0.181129\n",
      "(3 batch) loss: 0.181628\n",
      "(4 batch) loss: 0.151128\n",
      "(5 batch) loss: 0.120552\n",
      "(6 batch) loss: 0.151563\n",
      "(7 batch) loss: 0.185674\n",
      "(8 batch) loss: 0.159386\n",
      "(9 batch) loss: 0.153227\n",
      "(10 batch) loss: 0.120582\n",
      "(Epoch 88 / 100) train_acc: 0.963462; val_acc: 0.580000\n",
      "(0 batch) loss: 0.127604\n",
      "(1 batch) loss: 0.137723\n",
      "(2 batch) loss: 0.144880\n",
      "(3 batch) loss: 0.181621\n",
      "(4 batch) loss: 0.131678\n",
      "(5 batch) loss: 0.164334\n",
      "(6 batch) loss: 0.134885\n",
      "(7 batch) loss: 0.162682\n",
      "(8 batch) loss: 0.140910\n",
      "(9 batch) loss: 0.162272\n",
      "(10 batch) loss: 0.080492\n",
      "(Epoch 89 / 100) train_acc: 0.958654; val_acc: 0.580000\n",
      "(0 batch) loss: 0.125662\n",
      "(1 batch) loss: 0.141092\n",
      "(2 batch) loss: 0.208756\n",
      "(3 batch) loss: 0.132520\n",
      "(4 batch) loss: 0.148669\n",
      "(5 batch) loss: 0.237498\n",
      "(6 batch) loss: 0.123794\n",
      "(7 batch) loss: 0.116699\n",
      "(8 batch) loss: 0.160824\n",
      "(9 batch) loss: 0.202152\n",
      "(10 batch) loss: 0.138945\n",
      "(Epoch 90 / 100) train_acc: 0.954808; val_acc: 0.600000\n",
      "(0 batch) loss: 0.200225\n",
      "(1 batch) loss: 0.122317\n",
      "(2 batch) loss: 0.117678\n",
      "(3 batch) loss: 0.137677\n",
      "(4 batch) loss: 0.139755\n",
      "(5 batch) loss: 0.156210\n",
      "(6 batch) loss: 0.164929\n",
      "(7 batch) loss: 0.177663\n",
      "(8 batch) loss: 0.163974\n",
      "(9 batch) loss: 0.157236\n",
      "(10 batch) loss: 0.167991\n",
      "(Epoch 91 / 100) train_acc: 0.946635; val_acc: 0.530000\n",
      "(0 batch) loss: 0.167963\n",
      "(1 batch) loss: 0.199172\n",
      "(2 batch) loss: 0.117513\n",
      "(3 batch) loss: 0.126998\n",
      "(4 batch) loss: 0.126861\n",
      "(5 batch) loss: 0.167503\n",
      "(6 batch) loss: 0.171401\n",
      "(7 batch) loss: 0.148080\n",
      "(8 batch) loss: 0.121711\n",
      "(9 batch) loss: 0.149985\n",
      "(10 batch) loss: 0.201134\n",
      "(Epoch 92 / 100) train_acc: 0.943750; val_acc: 0.550000\n",
      "(0 batch) loss: 0.231039\n",
      "(1 batch) loss: 0.128795\n",
      "(2 batch) loss: 0.159263\n",
      "(3 batch) loss: 0.160668\n",
      "(4 batch) loss: 0.194646\n",
      "(5 batch) loss: 0.232383\n",
      "(6 batch) loss: 0.130404\n",
      "(7 batch) loss: 0.185703\n",
      "(8 batch) loss: 0.289417\n",
      "(9 batch) loss: 0.140478\n",
      "(10 batch) loss: 0.180017\n",
      "(Epoch 93 / 100) train_acc: 0.960096; val_acc: 0.610000\n",
      "(0 batch) loss: 0.116604\n",
      "(1 batch) loss: 0.111842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2 batch) loss: 0.183424\n",
      "(3 batch) loss: 0.165531\n",
      "(4 batch) loss: 0.164185\n",
      "(5 batch) loss: 0.117948\n",
      "(6 batch) loss: 0.128476\n",
      "(7 batch) loss: 0.146122\n",
      "(8 batch) loss: 0.115939\n",
      "(9 batch) loss: 0.088024\n",
      "(10 batch) loss: 0.074454\n",
      "(Epoch 94 / 100) train_acc: 0.961538; val_acc: 0.630000\n",
      "(0 batch) loss: 0.201209\n",
      "(1 batch) loss: 0.112855\n",
      "(2 batch) loss: 0.086731\n",
      "(3 batch) loss: 0.092480\n",
      "(4 batch) loss: 0.112610\n",
      "(5 batch) loss: 0.118108\n",
      "(6 batch) loss: 0.127120\n",
      "(7 batch) loss: 0.087514\n",
      "(8 batch) loss: 0.097384\n",
      "(9 batch) loss: 0.137061\n",
      "(10 batch) loss: 0.104594\n",
      "(Epoch 95 / 100) train_acc: 0.978365; val_acc: 0.550000\n",
      "(0 batch) loss: 0.088828\n",
      "(1 batch) loss: 0.111047\n",
      "(2 batch) loss: 0.151016\n",
      "(3 batch) loss: 0.083606\n",
      "(4 batch) loss: 0.111520\n",
      "(5 batch) loss: 0.092865\n",
      "(6 batch) loss: 0.101622\n",
      "(7 batch) loss: 0.126045\n",
      "(8 batch) loss: 0.104132\n",
      "(9 batch) loss: 0.116604\n",
      "(10 batch) loss: 0.209511\n",
      "(Epoch 96 / 100) train_acc: 0.970673; val_acc: 0.550000\n",
      "(0 batch) loss: 0.162550\n",
      "(1 batch) loss: 0.091008\n",
      "(2 batch) loss: 0.084577\n",
      "(3 batch) loss: 0.102728\n",
      "(4 batch) loss: 0.130773\n",
      "(5 batch) loss: 0.138995\n",
      "(6 batch) loss: 0.127815\n",
      "(7 batch) loss: 0.134277\n",
      "(8 batch) loss: 0.106365\n",
      "(9 batch) loss: 0.119599\n",
      "(10 batch) loss: 0.142147\n",
      "(Epoch 97 / 100) train_acc: 0.973077; val_acc: 0.540000\n",
      "(0 batch) loss: 0.091041\n",
      "(1 batch) loss: 0.121569\n",
      "(2 batch) loss: 0.155238\n",
      "(3 batch) loss: 0.129390\n",
      "(4 batch) loss: 0.124874\n",
      "(5 batch) loss: 0.121811\n",
      "(6 batch) loss: 0.098803\n",
      "(7 batch) loss: 0.163193\n",
      "(8 batch) loss: 0.143415\n",
      "(9 batch) loss: 0.174970\n",
      "(10 batch) loss: 0.195928\n",
      "(Epoch 98 / 100) train_acc: 0.971154; val_acc: 0.580000\n",
      "(0 batch) loss: 0.076941\n",
      "(1 batch) loss: 0.107329\n",
      "(2 batch) loss: 0.140405\n",
      "(3 batch) loss: 0.175923\n",
      "(4 batch) loss: 0.145190\n",
      "(5 batch) loss: 0.170374\n",
      "(6 batch) loss: 0.124862\n",
      "(7 batch) loss: 0.103795\n",
      "(8 batch) loss: 0.160443\n",
      "(9 batch) loss: 0.113958\n",
      "(10 batch) loss: 0.150124\n",
      "(Epoch 99 / 100) train_acc: 0.957212; val_acc: 0.540000\n",
      "(0 batch) loss: 0.129038\n",
      "(1 batch) loss: 0.128872\n",
      "(2 batch) loss: 0.094999\n",
      "(3 batch) loss: 0.155328\n",
      "(4 batch) loss: 0.129650\n",
      "(5 batch) loss: 0.115633\n",
      "(6 batch) loss: 0.097353\n",
      "(7 batch) loss: 0.095302\n",
      "(8 batch) loss: 0.201170\n",
      "(9 batch) loss: 0.156788\n",
      "(10 batch) loss: 0.102041\n",
      "(Epoch 100 / 100) train_acc: 0.942308; val_acc: 0.570000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epoches):\n",
    "    for i, data in enumerate(dataloaders['train'], 0):\n",
    "        X_train, y_train = data\n",
    "        # Wrap them in Variable\n",
    "        X_train, y_train = Variable(X_train), Variable(y_train)\n",
    "        # forward + backward + optimize\n",
    "        out = model(X_train.cuda())\n",
    "        # print (out)\n",
    "        loss = loss_fn(out, y_train.long().cuda())\n",
    "        print('(%d batch) loss: %f' % (i, loss))\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_acc = model.check_accuracy(dataloaders['train'])\n",
    "    val_acc = model.check_accuracy(dataloaders['val'])\n",
    "    print('(Epoch %d / %d) train_acc: %f; val_acc: %f' % (epoch+1, num_epoches, train_acc, val_acc))\n",
    "    if (val_acc > best_acc):\n",
    "        best_acc = val_acc\n",
    "        torch.save(model, 'best_SHALLOW_CONV.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65\n"
     ]
    }
   ],
   "source": [
    "best_model = torch.load('best_SHALLOW_CONV.pt')\n",
    "print (best_model.check_accuracy(dataloaders['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
