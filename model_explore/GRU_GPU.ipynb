{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew/Desktop/Project/model_explore/.env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data.dataset import random_split\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/markdown"
   },
   "source": [
    "# Helper Clases / Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_Data(num):\n",
    "    if (num == -1): # All data\n",
    "        X_all = []\n",
    "        y_all = []\n",
    "        for i in range(8):\n",
    "            file_path = './../project_datasets/A0' + str(i+1) + 'T_slice.mat'\n",
    "            data = h5py.File(file_path, 'r')\n",
    "            X = np.copy(data['image'])\n",
    "            y = np.copy(data['type'])\n",
    "            X = X[:, 0:23, :]\n",
    "            X_all.append(X)\n",
    "            y = y[0,0:X.shape[0]:1]\n",
    "            y_all.append(y)\n",
    "        A, N, E, T = np.shape(X_all)\n",
    "        X_all = np.reshape(X_all, (A*N, E, T))\n",
    "        y_all = np.reshape(y_all, (-1))\n",
    "        y_all = y_all - 769\n",
    "        ## Remove NAN\n",
    "        index_Nan = []\n",
    "        for i in range(A*N):\n",
    "            for j in range(E):\n",
    "                if (any(np.isnan(X_all[i,j])) == True):\n",
    "                    index_Nan.append(i)\n",
    "        index_Nan = list(set(index_Nan))\n",
    "        X_all = np.delete(X_all, index_Nan, axis=0)\n",
    "        y_all = np.delete(y_all, index_Nan)\n",
    "        return (X_all, y_all)\n",
    "    else:\n",
    "        file_path = './../project_datasets/A0' + str(num) + 'T_slice.mat'\n",
    "        data = h5py.File(file_path, 'r')\n",
    "        X = np.copy(data['image'])\n",
    "        y = np.copy(data['type'])\n",
    "        X = X[:, 0:23, :]\n",
    "        y = y[0,0:X.shape[0]:1]\n",
    "        y = y - 769\n",
    "         ## Remove NAN\n",
    "        N, E, T = np.shape(X)\n",
    "        index_Nan = []\n",
    "        for i in range(N):\n",
    "            for j in range(E):\n",
    "                if (any(np.isnan(X[i,j])) == True):\n",
    "                    index_Nan.append(i)\n",
    "        index_Nan = list(set(index_Nan))\n",
    "        X = np.delete(X, index_Nan, axis=0)\n",
    "        y = np.delete(y, index_Nan)\n",
    "        return (X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2280, 23, 1000)\n"
     ]
    }
   ],
   "source": [
    "X, y = Load_Data(-1) # -1 to load all datas\n",
    "N, E, T = np.shape(X)\n",
    "print (np.shape(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_train = 200\n",
    "bs_val = 100\n",
    "bs_test = 100\n",
    "data = data_utils.TensorDataset(torch.Tensor(X), torch.Tensor(y))\n",
    "dset = {}\n",
    "dataloaders = {}\n",
    "dset['train'], dset['val'], dset['test'] = random_split(data, [N-bs_val-bs_test, bs_val, bs_test])\n",
    "dataloaders['train'] = data_utils.DataLoader(dset['train'], batch_size=bs_train, shuffle=True, num_workers=1)\n",
    "dataloaders['val'] = data_utils.DataLoader(dset['val'], batch_size=bs_val, shuffle=True, num_workers=1)\n",
    "dataloaders['test'] = data_utils.DataLoader(dset['test'], batch_size=bs_test, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myGRU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layer, num_class):\n",
    "        super(myGRU, self).__init__()\n",
    "        self.num_layer = num_layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.conv_temp = nn.Conv2d(1,40,tuple([1,25]))\n",
    "        self.conv_elec = nn.Conv3d(1,23,tuple([40, 23, 1]))\n",
    "        self.gru1 = nn.GRU(input_dim, hidden_dim, num_layer)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.gru2 = nn.GRU(hidden_dim, hidden_dim, num_layer)\n",
    "        self.classifier = nn.Linear(hidden_dim, num_class)\n",
    "    def forward(self, x):\n",
    "        x = x.permute(2,0,1)\n",
    "        out_gru1, _ = self.gru1(x)\n",
    "        out_act1 = self.act1(out_gru1)\n",
    "        out_gru2, _ = self.gru2(out_act1)\n",
    "        out_gru2 = out_gru2[-1, :, :] # taking the last time seq\n",
    "        out = self.classifier(out_gru2)\n",
    "        return out\n",
    "    def check_accuracy(self, dataloader):\n",
    "        total_correct = 0\n",
    "        total_label = 0\n",
    "        for i_batch, sample_batched in enumerate(dataloader):\n",
    "            X_sample, y_sample = sample_batched\n",
    "            X_sample, y_sample = Variable(X_sample), Variable(y_sample)\n",
    "            out = self.forward(X_sample.cuda())\n",
    "            _, pred = torch.max(out, 1)\n",
    "            num_correct = np.sum(pred.data.cpu().numpy() == y_sample.data.cpu().numpy())\n",
    "            total_correct += num_correct\n",
    "            total_label += len(pred)\n",
    "        return  total_correct / total_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.cuda.FloatTensor\n",
    "hidden_dim = 10\n",
    "num_classes = 4\n",
    "num_epoches = 100\n",
    "model = myGRU(E, hidden_dim, 2, num_classes)\n",
    "model.type(dtype)\n",
    "loss_fn = nn.CrossEntropyLoss().type(dtype)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0 batch) loss: 1.392268\n",
      "(1 batch) loss: 1.394019\n",
      "(2 batch) loss: 1.394917\n",
      "(3 batch) loss: 1.397715\n",
      "(4 batch) loss: 1.403159\n",
      "(5 batch) loss: 1.381242\n",
      "(6 batch) loss: 1.384384\n",
      "(7 batch) loss: 1.385979\n",
      "(8 batch) loss: 1.394333\n",
      "(9 batch) loss: 1.386945\n",
      "(10 batch) loss: 1.394715\n",
      "(Epoch 1 / 100) train_acc: 0.253846; val_acc: 0.250000\n",
      "(0 batch) loss: 1.384087\n",
      "(1 batch) loss: 1.392471\n",
      "(2 batch) loss: 1.394963\n",
      "(3 batch) loss: 1.383678\n",
      "(4 batch) loss: 1.385215\n",
      "(5 batch) loss: 1.388480\n",
      "(6 batch) loss: 1.384465\n",
      "(7 batch) loss: 1.385950\n",
      "(8 batch) loss: 1.385149\n",
      "(9 batch) loss: 1.385232\n",
      "(10 batch) loss: 1.390082\n",
      "(Epoch 2 / 100) train_acc: 0.281731; val_acc: 0.210000\n",
      "(0 batch) loss: 1.386912\n",
      "(1 batch) loss: 1.393416\n",
      "(2 batch) loss: 1.384865\n",
      "(3 batch) loss: 1.381992\n",
      "(4 batch) loss: 1.381143\n",
      "(5 batch) loss: 1.387919\n",
      "(6 batch) loss: 1.384235\n",
      "(7 batch) loss: 1.388036\n",
      "(8 batch) loss: 1.380148\n",
      "(9 batch) loss: 1.386385\n",
      "(10 batch) loss: 1.387213\n",
      "(Epoch 3 / 100) train_acc: 0.271635; val_acc: 0.220000\n",
      "(0 batch) loss: 1.383583\n",
      "(1 batch) loss: 1.387689\n",
      "(2 batch) loss: 1.385622\n",
      "(3 batch) loss: 1.387586\n",
      "(4 batch) loss: 1.385143\n",
      "(5 batch) loss: 1.384745\n",
      "(6 batch) loss: 1.381826\n",
      "(7 batch) loss: 1.384402\n",
      "(8 batch) loss: 1.385734\n",
      "(9 batch) loss: 1.381707\n",
      "(10 batch) loss: 1.380794\n",
      "(Epoch 4 / 100) train_acc: 0.275481; val_acc: 0.160000\n",
      "(0 batch) loss: 1.382151\n",
      "(1 batch) loss: 1.386619\n",
      "(2 batch) loss: 1.381902\n",
      "(3 batch) loss: 1.380892\n",
      "(4 batch) loss: 1.384942\n",
      "(5 batch) loss: 1.383979\n",
      "(6 batch) loss: 1.385594\n",
      "(7 batch) loss: 1.385115\n",
      "(8 batch) loss: 1.383067\n",
      "(9 batch) loss: 1.386076\n",
      "(10 batch) loss: 1.383943\n",
      "(Epoch 5 / 100) train_acc: 0.276442; val_acc: 0.250000\n",
      "(0 batch) loss: 1.383580\n",
      "(1 batch) loss: 1.387482\n",
      "(2 batch) loss: 1.384288\n",
      "(3 batch) loss: 1.379601\n",
      "(4 batch) loss: 1.382285\n",
      "(5 batch) loss: 1.384866\n",
      "(6 batch) loss: 1.381137\n",
      "(7 batch) loss: 1.385393\n",
      "(8 batch) loss: 1.387511\n",
      "(9 batch) loss: 1.381991\n",
      "(10 batch) loss: 1.381308\n",
      "(Epoch 6 / 100) train_acc: 0.273077; val_acc: 0.210000\n",
      "(0 batch) loss: 1.377094\n",
      "(1 batch) loss: 1.382621\n",
      "(2 batch) loss: 1.377168\n",
      "(3 batch) loss: 1.380661\n",
      "(4 batch) loss: 1.385807\n",
      "(5 batch) loss: 1.386643\n",
      "(6 batch) loss: 1.386955\n",
      "(7 batch) loss: 1.382840\n",
      "(8 batch) loss: 1.382463\n",
      "(9 batch) loss: 1.384761\n",
      "(10 batch) loss: 1.378375\n",
      "(Epoch 7 / 100) train_acc: 0.282692; val_acc: 0.200000\n",
      "(0 batch) loss: 1.379135\n",
      "(1 batch) loss: 1.383348\n",
      "(2 batch) loss: 1.381008\n",
      "(3 batch) loss: 1.381024\n",
      "(4 batch) loss: 1.385217\n",
      "(5 batch) loss: 1.379580\n",
      "(6 batch) loss: 1.385952\n",
      "(7 batch) loss: 1.389245\n",
      "(8 batch) loss: 1.382994\n",
      "(9 batch) loss: 1.375942\n",
      "(10 batch) loss: 1.372886\n",
      "(Epoch 8 / 100) train_acc: 0.285096; val_acc: 0.200000\n",
      "(0 batch) loss: 1.379306\n",
      "(1 batch) loss: 1.387158\n",
      "(2 batch) loss: 1.379497\n",
      "(3 batch) loss: 1.377816\n",
      "(4 batch) loss: 1.383019\n",
      "(5 batch) loss: 1.374723\n",
      "(6 batch) loss: 1.383402\n",
      "(7 batch) loss: 1.380914\n",
      "(8 batch) loss: 1.386363\n",
      "(9 batch) loss: 1.380739\n",
      "(10 batch) loss: 1.388512\n",
      "(Epoch 9 / 100) train_acc: 0.278846; val_acc: 0.190000\n",
      "(0 batch) loss: 1.381464\n",
      "(1 batch) loss: 1.386778\n",
      "(2 batch) loss: 1.389841\n",
      "(3 batch) loss: 1.384620\n",
      "(4 batch) loss: 1.378494\n",
      "(5 batch) loss: 1.379068\n",
      "(6 batch) loss: 1.379657\n",
      "(7 batch) loss: 1.372815\n",
      "(8 batch) loss: 1.377691\n",
      "(9 batch) loss: 1.378342\n",
      "(10 batch) loss: 1.371226\n",
      "(Epoch 10 / 100) train_acc: 0.283654; val_acc: 0.200000\n",
      "(0 batch) loss: 1.379165\n",
      "(1 batch) loss: 1.378535\n",
      "(2 batch) loss: 1.385213\n",
      "(3 batch) loss: 1.372194\n",
      "(4 batch) loss: 1.377072\n",
      "(5 batch) loss: 1.372970\n",
      "(6 batch) loss: 1.381289\n",
      "(7 batch) loss: 1.386576\n",
      "(8 batch) loss: 1.383447\n",
      "(9 batch) loss: 1.383503\n",
      "(10 batch) loss: 1.372046\n",
      "(Epoch 11 / 100) train_acc: 0.289904; val_acc: 0.190000\n",
      "(0 batch) loss: 1.378882\n",
      "(1 batch) loss: 1.363119\n",
      "(2 batch) loss: 1.373110\n",
      "(3 batch) loss: 1.384208\n",
      "(4 batch) loss: 1.384171\n",
      "(5 batch) loss: 1.388076\n",
      "(6 batch) loss: 1.381458\n",
      "(7 batch) loss: 1.375516\n",
      "(8 batch) loss: 1.373798\n",
      "(9 batch) loss: 1.375391\n",
      "(10 batch) loss: 1.403244\n",
      "(Epoch 12 / 100) train_acc: 0.287019; val_acc: 0.190000\n",
      "(0 batch) loss: 1.376975\n",
      "(1 batch) loss: 1.360001\n",
      "(2 batch) loss: 1.375131\n",
      "(3 batch) loss: 1.395360\n",
      "(4 batch) loss: 1.375658\n",
      "(5 batch) loss: 1.379266\n",
      "(6 batch) loss: 1.374185\n",
      "(7 batch) loss: 1.381888\n",
      "(8 batch) loss: 1.374779\n",
      "(9 batch) loss: 1.382107\n",
      "(10 batch) loss: 1.389793\n",
      "(Epoch 13 / 100) train_acc: 0.285577; val_acc: 0.190000\n",
      "(0 batch) loss: 1.373998\n",
      "(1 batch) loss: 1.376562\n",
      "(2 batch) loss: 1.385112\n",
      "(3 batch) loss: 1.376271\n",
      "(4 batch) loss: 1.370634\n",
      "(5 batch) loss: 1.377216\n",
      "(6 batch) loss: 1.383501\n",
      "(7 batch) loss: 1.370208\n",
      "(8 batch) loss: 1.380766\n",
      "(9 batch) loss: 1.372065\n",
      "(10 batch) loss: 1.380040\n",
      "(Epoch 14 / 100) train_acc: 0.298558; val_acc: 0.200000\n",
      "(0 batch) loss: 1.374079\n",
      "(1 batch) loss: 1.381359\n",
      "(2 batch) loss: 1.371353\n",
      "(3 batch) loss: 1.371814\n",
      "(4 batch) loss: 1.374895\n",
      "(5 batch) loss: 1.380030\n",
      "(6 batch) loss: 1.381313\n",
      "(7 batch) loss: 1.359750\n",
      "(8 batch) loss: 1.373182\n",
      "(9 batch) loss: 1.377256\n",
      "(10 batch) loss: 1.405662\n",
      "(Epoch 15 / 100) train_acc: 0.297596; val_acc: 0.190000\n",
      "(0 batch) loss: 1.374545\n",
      "(1 batch) loss: 1.366227\n",
      "(2 batch) loss: 1.365710\n",
      "(3 batch) loss: 1.380893\n",
      "(4 batch) loss: 1.371463\n",
      "(5 batch) loss: 1.383504\n",
      "(6 batch) loss: 1.374233\n",
      "(7 batch) loss: 1.383285\n",
      "(8 batch) loss: 1.376148\n",
      "(9 batch) loss: 1.374703\n",
      "(10 batch) loss: 1.363684\n",
      "(Epoch 16 / 100) train_acc: 0.298558; val_acc: 0.210000\n",
      "(0 batch) loss: 1.368927\n",
      "(1 batch) loss: 1.376270\n",
      "(2 batch) loss: 1.371297\n",
      "(3 batch) loss: 1.368876\n",
      "(4 batch) loss: 1.372240\n",
      "(5 batch) loss: 1.365934\n",
      "(6 batch) loss: 1.378832\n",
      "(7 batch) loss: 1.375418\n",
      "(8 batch) loss: 1.374395\n",
      "(9 batch) loss: 1.376759\n",
      "(10 batch) loss: 1.382121\n",
      "(Epoch 17 / 100) train_acc: 0.300962; val_acc: 0.230000\n",
      "(0 batch) loss: 1.373395\n",
      "(1 batch) loss: 1.385930\n",
      "(2 batch) loss: 1.371234\n",
      "(3 batch) loss: 1.380363\n",
      "(4 batch) loss: 1.363907\n",
      "(5 batch) loss: 1.353852\n",
      "(6 batch) loss: 1.367428\n",
      "(7 batch) loss: 1.370591\n",
      "(8 batch) loss: 1.374763\n",
      "(9 batch) loss: 1.379066\n",
      "(10 batch) loss: 1.382327\n",
      "(Epoch 18 / 100) train_acc: 0.299519; val_acc: 0.240000\n",
      "(0 batch) loss: 1.375989\n",
      "(1 batch) loss: 1.365945\n",
      "(2 batch) loss: 1.359444\n",
      "(3 batch) loss: 1.374882\n",
      "(4 batch) loss: 1.383557\n",
      "(5 batch) loss: 1.375162\n",
      "(6 batch) loss: 1.361597\n",
      "(7 batch) loss: 1.380461\n",
      "(8 batch) loss: 1.371940\n",
      "(9 batch) loss: 1.356228\n",
      "(10 batch) loss: 1.378998\n",
      "(Epoch 19 / 100) train_acc: 0.307692; val_acc: 0.240000\n",
      "(0 batch) loss: 1.385063\n",
      "(1 batch) loss: 1.385249\n",
      "(2 batch) loss: 1.362372\n",
      "(3 batch) loss: 1.360505\n",
      "(4 batch) loss: 1.378181\n",
      "(5 batch) loss: 1.362544\n",
      "(6 batch) loss: 1.351648\n",
      "(7 batch) loss: 1.379972\n",
      "(8 batch) loss: 1.362851\n",
      "(9 batch) loss: 1.367421\n",
      "(10 batch) loss: 1.365984\n",
      "(Epoch 20 / 100) train_acc: 0.315865; val_acc: 0.240000\n",
      "(0 batch) loss: 1.374279\n",
      "(1 batch) loss: 1.375805\n",
      "(2 batch) loss: 1.373088\n",
      "(3 batch) loss: 1.370159\n",
      "(4 batch) loss: 1.357587\n",
      "(5 batch) loss: 1.351601\n",
      "(6 batch) loss: 1.369271\n",
      "(7 batch) loss: 1.364413\n",
      "(8 batch) loss: 1.362970\n",
      "(9 batch) loss: 1.367015\n",
      "(10 batch) loss: 1.396129\n",
      "(Epoch 21 / 100) train_acc: 0.309135; val_acc: 0.250000\n",
      "(0 batch) loss: 1.351931\n",
      "(1 batch) loss: 1.368196\n",
      "(2 batch) loss: 1.361705\n",
      "(3 batch) loss: 1.367913\n",
      "(4 batch) loss: 1.374896\n",
      "(5 batch) loss: 1.375683\n",
      "(6 batch) loss: 1.347966\n",
      "(7 batch) loss: 1.369924\n",
      "(8 batch) loss: 1.378191\n",
      "(9 batch) loss: 1.367244\n",
      "(10 batch) loss: 1.365693\n",
      "(Epoch 22 / 100) train_acc: 0.313942; val_acc: 0.230000\n",
      "(0 batch) loss: 1.372271\n",
      "(1 batch) loss: 1.377991\n",
      "(2 batch) loss: 1.361812\n",
      "(3 batch) loss: 1.370496\n",
      "(4 batch) loss: 1.360181\n",
      "(5 batch) loss: 1.352517\n",
      "(6 batch) loss: 1.359610\n",
      "(7 batch) loss: 1.382169\n",
      "(8 batch) loss: 1.370912\n",
      "(9 batch) loss: 1.340801\n",
      "(10 batch) loss: 1.357351\n",
      "(Epoch 23 / 100) train_acc: 0.308173; val_acc: 0.250000\n",
      "(0 batch) loss: 1.346843\n",
      "(1 batch) loss: 1.350306\n",
      "(2 batch) loss: 1.356528\n",
      "(3 batch) loss: 1.355049\n",
      "(4 batch) loss: 1.372879\n",
      "(5 batch) loss: 1.381099\n",
      "(6 batch) loss: 1.377791\n",
      "(7 batch) loss: 1.359790\n",
      "(8 batch) loss: 1.359131\n",
      "(9 batch) loss: 1.375332\n",
      "(10 batch) loss: 1.367968\n",
      "(Epoch 24 / 100) train_acc: 0.320673; val_acc: 0.270000\n",
      "(0 batch) loss: 1.354219\n",
      "(1 batch) loss: 1.378405\n",
      "(2 batch) loss: 1.348567\n",
      "(3 batch) loss: 1.365085\n",
      "(4 batch) loss: 1.346193\n",
      "(5 batch) loss: 1.354126\n",
      "(6 batch) loss: 1.354728\n",
      "(7 batch) loss: 1.382721\n",
      "(8 batch) loss: 1.362715\n",
      "(9 batch) loss: 1.364497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10 batch) loss: 1.369026\n",
      "(Epoch 25 / 100) train_acc: 0.314904; val_acc: 0.270000\n",
      "(0 batch) loss: 1.363875\n",
      "(1 batch) loss: 1.347612\n",
      "(2 batch) loss: 1.352383\n",
      "(3 batch) loss: 1.349591\n",
      "(4 batch) loss: 1.372367\n",
      "(5 batch) loss: 1.379668\n",
      "(6 batch) loss: 1.330815\n",
      "(7 batch) loss: 1.353197\n",
      "(8 batch) loss: 1.363769\n",
      "(9 batch) loss: 1.374349\n",
      "(10 batch) loss: 1.396287\n",
      "(Epoch 26 / 100) train_acc: 0.321154; val_acc: 0.280000\n",
      "(0 batch) loss: 1.349379\n",
      "(1 batch) loss: 1.344474\n",
      "(2 batch) loss: 1.357122\n",
      "(3 batch) loss: 1.352660\n",
      "(4 batch) loss: 1.348694\n",
      "(5 batch) loss: 1.359403\n",
      "(6 batch) loss: 1.379706\n",
      "(7 batch) loss: 1.359775\n",
      "(8 batch) loss: 1.369636\n",
      "(9 batch) loss: 1.344795\n",
      "(10 batch) loss: 1.383516\n",
      "(Epoch 27 / 100) train_acc: 0.323077; val_acc: 0.240000\n",
      "(0 batch) loss: 1.357555\n",
      "(1 batch) loss: 1.368705\n",
      "(2 batch) loss: 1.372128\n",
      "(3 batch) loss: 1.337598\n",
      "(4 batch) loss: 1.344031\n",
      "(5 batch) loss: 1.350093\n",
      "(6 batch) loss: 1.364514\n",
      "(7 batch) loss: 1.337596\n",
      "(8 batch) loss: 1.385490\n",
      "(9 batch) loss: 1.357123\n",
      "(10 batch) loss: 1.325783\n",
      "(Epoch 28 / 100) train_acc: 0.328365; val_acc: 0.270000\n",
      "(0 batch) loss: 1.360175\n",
      "(1 batch) loss: 1.345444\n",
      "(2 batch) loss: 1.341895\n",
      "(3 batch) loss: 1.368616\n",
      "(4 batch) loss: 1.345636\n",
      "(5 batch) loss: 1.359537\n",
      "(6 batch) loss: 1.347028\n",
      "(7 batch) loss: 1.373839\n",
      "(8 batch) loss: 1.354044\n",
      "(9 batch) loss: 1.369802\n",
      "(10 batch) loss: 1.341986\n",
      "(Epoch 29 / 100) train_acc: 0.330288; val_acc: 0.260000\n",
      "(0 batch) loss: 1.363130\n",
      "(1 batch) loss: 1.355670\n",
      "(2 batch) loss: 1.331801\n",
      "(3 batch) loss: 1.374862\n",
      "(4 batch) loss: 1.330210\n",
      "(5 batch) loss: 1.346653\n",
      "(6 batch) loss: 1.354157\n",
      "(7 batch) loss: 1.380751\n",
      "(8 batch) loss: 1.354463\n",
      "(9 batch) loss: 1.336085\n",
      "(10 batch) loss: 1.353238\n",
      "(Epoch 30 / 100) train_acc: 0.332212; val_acc: 0.270000\n",
      "(0 batch) loss: 1.342606\n",
      "(1 batch) loss: 1.341641\n",
      "(2 batch) loss: 1.355228\n",
      "(3 batch) loss: 1.358779\n",
      "(4 batch) loss: 1.370891\n",
      "(5 batch) loss: 1.342398\n",
      "(6 batch) loss: 1.337174\n",
      "(7 batch) loss: 1.340674\n",
      "(8 batch) loss: 1.387484\n",
      "(9 batch) loss: 1.336284\n",
      "(10 batch) loss: 1.351684\n",
      "(Epoch 31 / 100) train_acc: 0.332212; val_acc: 0.270000\n",
      "(0 batch) loss: 1.369468\n",
      "(1 batch) loss: 1.345251\n",
      "(2 batch) loss: 1.335147\n",
      "(3 batch) loss: 1.336523\n",
      "(4 batch) loss: 1.353754\n",
      "(5 batch) loss: 1.345594\n",
      "(6 batch) loss: 1.348296\n",
      "(7 batch) loss: 1.354064\n",
      "(8 batch) loss: 1.356594\n",
      "(9 batch) loss: 1.352815\n",
      "(10 batch) loss: 1.344260\n",
      "(Epoch 32 / 100) train_acc: 0.329808; val_acc: 0.240000\n",
      "(0 batch) loss: 1.340503\n",
      "(1 batch) loss: 1.336362\n",
      "(2 batch) loss: 1.329909\n",
      "(3 batch) loss: 1.365894\n",
      "(4 batch) loss: 1.339238\n",
      "(5 batch) loss: 1.351487\n",
      "(6 batch) loss: 1.356442\n",
      "(7 batch) loss: 1.345819\n",
      "(8 batch) loss: 1.380945\n",
      "(9 batch) loss: 1.336628\n",
      "(10 batch) loss: 1.364115\n",
      "(Epoch 33 / 100) train_acc: 0.336058; val_acc: 0.260000\n",
      "(0 batch) loss: 1.333285\n",
      "(1 batch) loss: 1.357709\n",
      "(2 batch) loss: 1.364604\n",
      "(3 batch) loss: 1.365695\n",
      "(4 batch) loss: 1.361306\n",
      "(5 batch) loss: 1.342457\n",
      "(6 batch) loss: 1.343126\n",
      "(7 batch) loss: 1.319127\n",
      "(8 batch) loss: 1.357226\n",
      "(9 batch) loss: 1.314016\n",
      "(10 batch) loss: 1.320283\n",
      "(Epoch 34 / 100) train_acc: 0.341827; val_acc: 0.280000\n",
      "(0 batch) loss: 1.308823\n",
      "(1 batch) loss: 1.377780\n",
      "(2 batch) loss: 1.351740\n",
      "(3 batch) loss: 1.338109\n",
      "(4 batch) loss: 1.383070\n",
      "(5 batch) loss: 1.325066\n",
      "(6 batch) loss: 1.329233\n",
      "(7 batch) loss: 1.353323\n",
      "(8 batch) loss: 1.344609\n",
      "(9 batch) loss: 1.310835\n",
      "(10 batch) loss: 1.360255\n",
      "(Epoch 35 / 100) train_acc: 0.338942; val_acc: 0.290000\n",
      "(0 batch) loss: 1.322135\n",
      "(1 batch) loss: 1.308206\n",
      "(2 batch) loss: 1.334016\n",
      "(3 batch) loss: 1.362570\n",
      "(4 batch) loss: 1.337973\n",
      "(5 batch) loss: 1.326207\n",
      "(6 batch) loss: 1.326217\n",
      "(7 batch) loss: 1.346742\n",
      "(8 batch) loss: 1.350536\n",
      "(9 batch) loss: 1.378869\n",
      "(10 batch) loss: 1.351889\n",
      "(Epoch 36 / 100) train_acc: 0.351442; val_acc: 0.230000\n",
      "(0 batch) loss: 1.337121\n",
      "(1 batch) loss: 1.325478\n",
      "(2 batch) loss: 1.351324\n",
      "(3 batch) loss: 1.373427\n",
      "(4 batch) loss: 1.328456\n",
      "(5 batch) loss: 1.341218\n",
      "(6 batch) loss: 1.342833\n",
      "(7 batch) loss: 1.333191\n",
      "(8 batch) loss: 1.306964\n",
      "(9 batch) loss: 1.352787\n",
      "(10 batch) loss: 1.340930\n",
      "(Epoch 37 / 100) train_acc: 0.348077; val_acc: 0.280000\n",
      "(0 batch) loss: 1.315746\n",
      "(1 batch) loss: 1.359197\n",
      "(2 batch) loss: 1.363706\n",
      "(3 batch) loss: 1.325098\n",
      "(4 batch) loss: 1.299657\n",
      "(5 batch) loss: 1.312381\n",
      "(6 batch) loss: 1.327816\n",
      "(7 batch) loss: 1.370090\n",
      "(8 batch) loss: 1.356466\n",
      "(9 batch) loss: 1.332914\n",
      "(10 batch) loss: 1.324907\n",
      "(Epoch 38 / 100) train_acc: 0.354808; val_acc: 0.250000\n",
      "(0 batch) loss: 1.330708\n",
      "(1 batch) loss: 1.341964\n",
      "(2 batch) loss: 1.352292\n",
      "(3 batch) loss: 1.345186\n",
      "(4 batch) loss: 1.324405\n",
      "(5 batch) loss: 1.332538\n",
      "(6 batch) loss: 1.367677\n",
      "(7 batch) loss: 1.300013\n",
      "(8 batch) loss: 1.368113\n",
      "(9 batch) loss: 1.277515\n",
      "(10 batch) loss: 1.312987\n",
      "(Epoch 39 / 100) train_acc: 0.353365; val_acc: 0.250000\n",
      "(0 batch) loss: 1.308398\n",
      "(1 batch) loss: 1.325744\n",
      "(2 batch) loss: 1.307608\n",
      "(3 batch) loss: 1.366886\n",
      "(4 batch) loss: 1.337267\n",
      "(5 batch) loss: 1.289184\n",
      "(6 batch) loss: 1.329314\n",
      "(7 batch) loss: 1.350430\n",
      "(8 batch) loss: 1.346772\n",
      "(9 batch) loss: 1.353909\n",
      "(10 batch) loss: 1.341956\n",
      "(Epoch 40 / 100) train_acc: 0.356731; val_acc: 0.260000\n",
      "(0 batch) loss: 1.324408\n",
      "(1 batch) loss: 1.329987\n",
      "(2 batch) loss: 1.336028\n",
      "(3 batch) loss: 1.326843\n",
      "(4 batch) loss: 1.317097\n",
      "(5 batch) loss: 1.337934\n",
      "(6 batch) loss: 1.347693\n",
      "(7 batch) loss: 1.310639\n",
      "(8 batch) loss: 1.321344\n",
      "(9 batch) loss: 1.341185\n",
      "(10 batch) loss: 1.299267\n",
      "(Epoch 41 / 100) train_acc: 0.360577; val_acc: 0.250000\n",
      "(0 batch) loss: 1.322590\n",
      "(1 batch) loss: 1.328883\n",
      "(2 batch) loss: 1.326483\n",
      "(3 batch) loss: 1.316871\n",
      "(4 batch) loss: 1.347038\n",
      "(5 batch) loss: 1.330341\n",
      "(6 batch) loss: 1.318558\n",
      "(7 batch) loss: 1.298069\n",
      "(8 batch) loss: 1.371486\n",
      "(9 batch) loss: 1.337896\n",
      "(10 batch) loss: 1.254606\n",
      "(Epoch 42 / 100) train_acc: 0.360577; val_acc: 0.240000\n",
      "(0 batch) loss: 1.356954\n",
      "(1 batch) loss: 1.328274\n",
      "(2 batch) loss: 1.342294\n",
      "(3 batch) loss: 1.318541\n",
      "(4 batch) loss: 1.326137\n",
      "(5 batch) loss: 1.337962\n",
      "(6 batch) loss: 1.300870\n",
      "(7 batch) loss: 1.344159\n",
      "(8 batch) loss: 1.305528\n",
      "(9 batch) loss: 1.307380\n",
      "(10 batch) loss: 1.284231\n",
      "(Epoch 43 / 100) train_acc: 0.363462; val_acc: 0.250000\n",
      "(0 batch) loss: 1.328473\n",
      "(1 batch) loss: 1.325963\n",
      "(2 batch) loss: 1.336025\n",
      "(3 batch) loss: 1.320233\n",
      "(4 batch) loss: 1.357943\n",
      "(5 batch) loss: 1.273901\n",
      "(6 batch) loss: 1.294629\n",
      "(7 batch) loss: 1.305852\n",
      "(8 batch) loss: 1.350342\n",
      "(9 batch) loss: 1.316933\n",
      "(10 batch) loss: 1.310758\n",
      "(Epoch 44 / 100) train_acc: 0.363462; val_acc: 0.250000\n",
      "(0 batch) loss: 1.283440\n",
      "(1 batch) loss: 1.318790\n",
      "(2 batch) loss: 1.351917\n",
      "(3 batch) loss: 1.301052\n",
      "(4 batch) loss: 1.333024\n",
      "(5 batch) loss: 1.342845\n",
      "(6 batch) loss: 1.338617\n",
      "(7 batch) loss: 1.279192\n",
      "(8 batch) loss: 1.297425\n",
      "(9 batch) loss: 1.361424\n",
      "(10 batch) loss: 1.252507\n",
      "(Epoch 45 / 100) train_acc: 0.370673; val_acc: 0.230000\n",
      "(0 batch) loss: 1.298947\n",
      "(1 batch) loss: 1.337273\n",
      "(2 batch) loss: 1.322424\n",
      "(3 batch) loss: 1.292410\n",
      "(4 batch) loss: 1.302321\n",
      "(5 batch) loss: 1.285387\n",
      "(6 batch) loss: 1.294500\n",
      "(7 batch) loss: 1.368468\n",
      "(8 batch) loss: 1.337016\n",
      "(9 batch) loss: 1.305918\n",
      "(10 batch) loss: 1.320236\n",
      "(Epoch 46 / 100) train_acc: 0.376923; val_acc: 0.250000\n",
      "(0 batch) loss: 1.332278\n",
      "(1 batch) loss: 1.330551\n",
      "(2 batch) loss: 1.320910\n",
      "(3 batch) loss: 1.300104\n",
      "(4 batch) loss: 1.327744\n",
      "(5 batch) loss: 1.313158\n",
      "(6 batch) loss: 1.295882\n",
      "(7 batch) loss: 1.324930\n",
      "(8 batch) loss: 1.306945\n",
      "(9 batch) loss: 1.273237\n",
      "(10 batch) loss: 1.335107\n",
      "(Epoch 47 / 100) train_acc: 0.372115; val_acc: 0.240000\n",
      "(0 batch) loss: 1.295490\n",
      "(1 batch) loss: 1.314645\n",
      "(2 batch) loss: 1.295914\n",
      "(3 batch) loss: 1.329556\n",
      "(4 batch) loss: 1.355920\n",
      "(5 batch) loss: 1.353614\n",
      "(6 batch) loss: 1.285023\n",
      "(7 batch) loss: 1.310613\n",
      "(8 batch) loss: 1.234079\n",
      "(9 batch) loss: 1.343703\n",
      "(10 batch) loss: 1.315667\n",
      "(Epoch 48 / 100) train_acc: 0.373558; val_acc: 0.220000\n",
      "(0 batch) loss: 1.315951\n",
      "(1 batch) loss: 1.313565\n",
      "(2 batch) loss: 1.281774\n",
      "(3 batch) loss: 1.263778\n",
      "(4 batch) loss: 1.305583\n",
      "(5 batch) loss: 1.368861\n",
      "(6 batch) loss: 1.302249\n",
      "(7 batch) loss: 1.324161\n",
      "(8 batch) loss: 1.287687\n",
      "(9 batch) loss: 1.307088\n",
      "(10 batch) loss: 1.364125\n",
      "(Epoch 49 / 100) train_acc: 0.375962; val_acc: 0.260000\n",
      "(0 batch) loss: 1.341663\n",
      "(1 batch) loss: 1.322188\n",
      "(2 batch) loss: 1.335989\n",
      "(3 batch) loss: 1.287944\n",
      "(4 batch) loss: 1.330845\n",
      "(5 batch) loss: 1.280072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6 batch) loss: 1.311922\n",
      "(7 batch) loss: 1.282791\n",
      "(8 batch) loss: 1.270195\n",
      "(9 batch) loss: 1.314723\n",
      "(10 batch) loss: 1.256629\n",
      "(Epoch 50 / 100) train_acc: 0.384615; val_acc: 0.210000\n",
      "(0 batch) loss: 1.327783\n",
      "(1 batch) loss: 1.324927\n",
      "(2 batch) loss: 1.281069\n",
      "(3 batch) loss: 1.310171\n",
      "(4 batch) loss: 1.265589\n",
      "(5 batch) loss: 1.335870\n",
      "(6 batch) loss: 1.245579\n",
      "(7 batch) loss: 1.310366\n",
      "(8 batch) loss: 1.333459\n",
      "(9 batch) loss: 1.266120\n",
      "(10 batch) loss: 1.346670\n",
      "(Epoch 51 / 100) train_acc: 0.385577; val_acc: 0.250000\n",
      "(0 batch) loss: 1.294472\n",
      "(1 batch) loss: 1.281593\n",
      "(2 batch) loss: 1.280810\n",
      "(3 batch) loss: 1.288242\n",
      "(4 batch) loss: 1.307063\n",
      "(5 batch) loss: 1.321187\n",
      "(6 batch) loss: 1.298756\n",
      "(7 batch) loss: 1.316409\n",
      "(8 batch) loss: 1.307777\n",
      "(9 batch) loss: 1.310876\n",
      "(10 batch) loss: 1.308212\n",
      "(Epoch 52 / 100) train_acc: 0.391827; val_acc: 0.230000\n",
      "(0 batch) loss: 1.289772\n",
      "(1 batch) loss: 1.253024\n",
      "(2 batch) loss: 1.320425\n",
      "(3 batch) loss: 1.292449\n",
      "(4 batch) loss: 1.287926\n",
      "(5 batch) loss: 1.298240\n",
      "(6 batch) loss: 1.313467\n",
      "(7 batch) loss: 1.294981\n",
      "(8 batch) loss: 1.310829\n",
      "(9 batch) loss: 1.337421\n",
      "(10 batch) loss: 1.289977\n",
      "(Epoch 53 / 100) train_acc: 0.387500; val_acc: 0.230000\n",
      "(0 batch) loss: 1.265579\n",
      "(1 batch) loss: 1.328652\n",
      "(2 batch) loss: 1.268554\n",
      "(3 batch) loss: 1.285160\n",
      "(4 batch) loss: 1.276592\n",
      "(5 batch) loss: 1.271101\n",
      "(6 batch) loss: 1.356370\n",
      "(7 batch) loss: 1.320063\n",
      "(8 batch) loss: 1.283999\n",
      "(9 batch) loss: 1.297879\n",
      "(10 batch) loss: 1.347880\n",
      "(Epoch 54 / 100) train_acc: 0.398558; val_acc: 0.230000\n",
      "(0 batch) loss: 1.277552\n",
      "(1 batch) loss: 1.314745\n",
      "(2 batch) loss: 1.258855\n",
      "(3 batch) loss: 1.301470\n",
      "(4 batch) loss: 1.342204\n",
      "(5 batch) loss: 1.305930\n",
      "(6 batch) loss: 1.291647\n",
      "(7 batch) loss: 1.274819\n",
      "(8 batch) loss: 1.264502\n",
      "(9 batch) loss: 1.274955\n",
      "(10 batch) loss: 1.296075\n",
      "(Epoch 55 / 100) train_acc: 0.392308; val_acc: 0.240000\n",
      "(0 batch) loss: 1.309456\n",
      "(1 batch) loss: 1.262734\n",
      "(2 batch) loss: 1.302423\n",
      "(3 batch) loss: 1.301378\n",
      "(4 batch) loss: 1.318251\n",
      "(5 batch) loss: 1.267251\n",
      "(6 batch) loss: 1.285216\n",
      "(7 batch) loss: 1.265634\n",
      "(8 batch) loss: 1.271229\n",
      "(9 batch) loss: 1.326653\n",
      "(10 batch) loss: 1.252209\n",
      "(Epoch 56 / 100) train_acc: 0.397596; val_acc: 0.220000\n",
      "(0 batch) loss: 1.267563\n",
      "(1 batch) loss: 1.265199\n",
      "(2 batch) loss: 1.325200\n",
      "(3 batch) loss: 1.242400\n",
      "(4 batch) loss: 1.330518\n",
      "(5 batch) loss: 1.311820\n",
      "(6 batch) loss: 1.249501\n",
      "(7 batch) loss: 1.265001\n",
      "(8 batch) loss: 1.290641\n",
      "(9 batch) loss: 1.333174\n",
      "(10 batch) loss: 1.249863\n",
      "(Epoch 57 / 100) train_acc: 0.394712; val_acc: 0.240000\n",
      "(0 batch) loss: 1.255208\n",
      "(1 batch) loss: 1.259271\n",
      "(2 batch) loss: 1.322187\n",
      "(3 batch) loss: 1.236025\n",
      "(4 batch) loss: 1.232531\n",
      "(5 batch) loss: 1.263826\n",
      "(6 batch) loss: 1.343757\n",
      "(7 batch) loss: 1.333046\n",
      "(8 batch) loss: 1.295227\n",
      "(9 batch) loss: 1.287175\n",
      "(10 batch) loss: 1.241635\n",
      "(Epoch 58 / 100) train_acc: 0.404808; val_acc: 0.220000\n",
      "(0 batch) loss: 1.294841\n",
      "(1 batch) loss: 1.300313\n",
      "(2 batch) loss: 1.303135\n",
      "(3 batch) loss: 1.262039\n",
      "(4 batch) loss: 1.273238\n",
      "(5 batch) loss: 1.256975\n",
      "(6 batch) loss: 1.304478\n",
      "(7 batch) loss: 1.267864\n",
      "(8 batch) loss: 1.257219\n",
      "(9 batch) loss: 1.268642\n",
      "(10 batch) loss: 1.298369\n",
      "(Epoch 59 / 100) train_acc: 0.404808; val_acc: 0.220000\n",
      "(0 batch) loss: 1.255295\n",
      "(1 batch) loss: 1.303352\n",
      "(2 batch) loss: 1.286436\n",
      "(3 batch) loss: 1.272552\n",
      "(4 batch) loss: 1.262783\n",
      "(5 batch) loss: 1.276356\n",
      "(6 batch) loss: 1.297669\n",
      "(7 batch) loss: 1.243580\n",
      "(8 batch) loss: 1.312968\n",
      "(9 batch) loss: 1.284414\n",
      "(10 batch) loss: 1.176718\n",
      "(Epoch 60 / 100) train_acc: 0.412019; val_acc: 0.220000\n",
      "(0 batch) loss: 1.288707\n",
      "(1 batch) loss: 1.286895\n",
      "(2 batch) loss: 1.237295\n",
      "(3 batch) loss: 1.241473\n",
      "(4 batch) loss: 1.282807\n",
      "(5 batch) loss: 1.296139\n",
      "(6 batch) loss: 1.254344\n",
      "(7 batch) loss: 1.289401\n",
      "(8 batch) loss: 1.253584\n",
      "(9 batch) loss: 1.292940\n",
      "(10 batch) loss: 1.256366\n",
      "(Epoch 61 / 100) train_acc: 0.400962; val_acc: 0.250000\n",
      "(0 batch) loss: 1.283088\n",
      "(1 batch) loss: 1.301837\n",
      "(2 batch) loss: 1.230530\n",
      "(3 batch) loss: 1.293764\n",
      "(4 batch) loss: 1.273144\n",
      "(5 batch) loss: 1.279513\n",
      "(6 batch) loss: 1.271508\n",
      "(7 batch) loss: 1.265291\n",
      "(8 batch) loss: 1.280007\n",
      "(9 batch) loss: 1.238755\n",
      "(10 batch) loss: 1.229678\n",
      "(Epoch 62 / 100) train_acc: 0.409135; val_acc: 0.250000\n",
      "(0 batch) loss: 1.282371\n",
      "(1 batch) loss: 1.263686\n",
      "(2 batch) loss: 1.262867\n",
      "(3 batch) loss: 1.242489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-282:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/andrew/Desktop/Project/model_explore/.env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4 batch) loss: 1.263684\n",
      "(5 batch) loss: 1.235944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x7faff7ae4b70>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/andrew/Desktop/Project/model_explore/.env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 333, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/andrew/Desktop/Project/model_explore/.env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 319, in _shutdown_workers\n",
      "    self.data_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 345, in get\n",
      "    return ForkingPickler.loads(res)\n",
      "  File \"/home/andrew/Desktop/Project/model_explore/.env/lib/python3.5/site-packages/torch/multiprocessing/reductions.py\", line 70, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 487, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 614, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-3caf52b8cd60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# zero the parameter gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Project/model_explore/.env/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Project/model_explore/.env/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epoches):\n",
    "    for i, data in enumerate(dataloaders['train'], 0):\n",
    "        X_train, y_train = data\n",
    "        # Wrap them in Variable\n",
    "        X_train, y_train = Variable(X_train), Variable(y_train)\n",
    "        # forward + backward + optimize\n",
    "        out = model(X_train.cuda())\n",
    "        # print (out)\n",
    "        loss = loss_fn(out, y_train.long().cuda())\n",
    "        print('(%d batch) loss: %f' % (i, loss))\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_acc = model.check_accuracy(dataloaders['train'])\n",
    "    val_acc = model.check_accuracy(dataloaders['val'])\n",
    "    print('(Epoch %d / %d) train_acc: %f; val_acc: %f' % (epoch+1, num_epoches, train_acc, val_acc))\n",
    "    if (val_acc > best_acc):\n",
    "        best_acc = val_acc\n",
    "        torch.save(model, 'best_GRU.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew/Desktop/Project/model_explore/.env/lib/python3.5/site-packages/ipykernel_launcher.py:17: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    }
   ],
   "source": [
    "best_model = torch.load('best_GRU.pt')\n",
    "print (best_model.check_accuracy(dataloaders['val']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
